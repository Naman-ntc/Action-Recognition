{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0a4c1c7910>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random, numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the datasets, i.e loading frames for few actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.382997</td>\n",
       "      <td>-0.419442</td>\n",
       "      <td>3.449989</td>\n",
       "      <td>-0.366909</td>\n",
       "      <td>-0.092619</td>\n",
       "      <td>3.443680</td>\n",
       "      <td>-0.353380</td>\n",
       "      <td>0.229542</td>\n",
       "      <td>3.427116</td>\n",
       "      <td>-0.391862</td>\n",
       "      <td>...</td>\n",
       "      <td>3.636719</td>\n",
       "      <td>-0.435790</td>\n",
       "      <td>-0.536338</td>\n",
       "      <td>3.280097</td>\n",
       "      <td>-0.364369</td>\n",
       "      <td>-0.491436</td>\n",
       "      <td>3.269750</td>\n",
       "      <td>1</td>\n",
       "      <td>72057594037944340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.383146</td>\n",
       "      <td>-0.419292</td>\n",
       "      <td>3.450006</td>\n",
       "      <td>-0.367569</td>\n",
       "      <td>-0.092003</td>\n",
       "      <td>3.443895</td>\n",
       "      <td>-0.353885</td>\n",
       "      <td>0.230300</td>\n",
       "      <td>3.427162</td>\n",
       "      <td>-0.391820</td>\n",
       "      <td>...</td>\n",
       "      <td>3.633053</td>\n",
       "      <td>-0.436031</td>\n",
       "      <td>-0.536649</td>\n",
       "      <td>3.281972</td>\n",
       "      <td>-0.358806</td>\n",
       "      <td>-0.471054</td>\n",
       "      <td>3.269975</td>\n",
       "      <td>1</td>\n",
       "      <td>72057594037944340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.385776</td>\n",
       "      <td>-0.421191</td>\n",
       "      <td>3.449611</td>\n",
       "      <td>-0.369506</td>\n",
       "      <td>-0.092775</td>\n",
       "      <td>3.443796</td>\n",
       "      <td>-0.354571</td>\n",
       "      <td>0.230189</td>\n",
       "      <td>3.426965</td>\n",
       "      <td>-0.403822</td>\n",
       "      <td>...</td>\n",
       "      <td>3.632370</td>\n",
       "      <td>-0.436489</td>\n",
       "      <td>-0.536484</td>\n",
       "      <td>3.286322</td>\n",
       "      <td>-0.358079</td>\n",
       "      <td>-0.470344</td>\n",
       "      <td>3.270202</td>\n",
       "      <td>1</td>\n",
       "      <td>72057594037944340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.385807</td>\n",
       "      <td>-0.421205</td>\n",
       "      <td>3.449582</td>\n",
       "      <td>-0.369576</td>\n",
       "      <td>-0.092714</td>\n",
       "      <td>3.443878</td>\n",
       "      <td>-0.354524</td>\n",
       "      <td>0.230369</td>\n",
       "      <td>3.427140</td>\n",
       "      <td>-0.403580</td>\n",
       "      <td>...</td>\n",
       "      <td>3.499778</td>\n",
       "      <td>-0.441701</td>\n",
       "      <td>-0.533234</td>\n",
       "      <td>3.278971</td>\n",
       "      <td>-0.360298</td>\n",
       "      <td>-0.476572</td>\n",
       "      <td>3.268953</td>\n",
       "      <td>1</td>\n",
       "      <td>72057594037944340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.357840</td>\n",
       "      <td>-0.420304</td>\n",
       "      <td>3.438846</td>\n",
       "      <td>-0.364956</td>\n",
       "      <td>-0.092426</td>\n",
       "      <td>3.442334</td>\n",
       "      <td>-0.354907</td>\n",
       "      <td>0.230391</td>\n",
       "      <td>3.427352</td>\n",
       "      <td>-0.405945</td>\n",
       "      <td>...</td>\n",
       "      <td>3.400878</td>\n",
       "      <td>-0.430001</td>\n",
       "      <td>-0.536492</td>\n",
       "      <td>3.278641</td>\n",
       "      <td>-0.358697</td>\n",
       "      <td>-0.471415</td>\n",
       "      <td>3.270685</td>\n",
       "      <td>1</td>\n",
       "      <td>72057594037944340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.382997 -0.419442  3.449989 -0.366909 -0.092619  3.443680 -0.353380   \n",
       "1 -0.383146 -0.419292  3.450006 -0.367569 -0.092003  3.443895 -0.353885   \n",
       "2 -0.385776 -0.421191  3.449611 -0.369506 -0.092775  3.443796 -0.354571   \n",
       "3 -0.385807 -0.421205  3.449582 -0.369576 -0.092714  3.443878 -0.354524   \n",
       "4 -0.357840 -0.420304  3.438846 -0.364956 -0.092426  3.442334 -0.354907   \n",
       "\n",
       "          7         8         9    ...           68        69        70  \\\n",
       "0  0.229542  3.427116 -0.391862    ...     3.636719 -0.435790 -0.536338   \n",
       "1  0.230300  3.427162 -0.391820    ...     3.633053 -0.436031 -0.536649   \n",
       "2  0.230189  3.426965 -0.403822    ...     3.632370 -0.436489 -0.536484   \n",
       "3  0.230369  3.427140 -0.403580    ...     3.499778 -0.441701 -0.533234   \n",
       "4  0.230391  3.427352 -0.405945    ...     3.400878 -0.430001 -0.536492   \n",
       "\n",
       "         71        72        73        74  label                 id  video_id  \n",
       "0  3.280097 -0.364369 -0.491436  3.269750      1  72057594037944340         0  \n",
       "1  3.281972 -0.358806 -0.471054  3.269975      1  72057594037944340         0  \n",
       "2  3.286322 -0.358079 -0.470344  3.270202      1  72057594037944340         0  \n",
       "3  3.278971 -0.360298 -0.476572  3.268953      1  72057594037944340         0  \n",
       "4  3.278641 -0.358697 -0.471415  3.270685      1  72057594037944340         0  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading and prepping data\n",
    "#initially only one action\n",
    "dframe = pd.read_csv('./csv_data/action_1.csv')\n",
    "dframe2 = pd.read_csv('./csv_data/action_2.csv')\n",
    "dframe3 = pd.read_csv('./csv_data/action_3.csv')\n",
    "dframe4 = pd.read_csv('./csv_data/action_4.csv')\n",
    "dframe5 = pd.read_csv('./csv_data/action_5.csv')\n",
    "dframe6 = pd.read_csv('./csv_data/action_6.csv')\n",
    "dframe7 = pd.read_csv('./csv_data/action_7.csv')\n",
    "\n",
    "#to look at data\n",
    "dframe.iloc[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some utility functions to split the datasets and loading the datasets in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ (array([[ 0.       ,  0.       ,  0.       , ...,  0.0186279, -0.0719937,\n",
       "        -0.180239 ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.0243399, -0.0517625,\n",
       "        -0.180031 ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.0276977, -0.0491529,\n",
       "        -0.179409 ],\n",
       "       ..., \n",
       "       [ 0.       ,  0.       ,  0.       , ..., -0.0563203, -0.0113986,\n",
       "        -0.174284 ],\n",
       "       [ 0.       ,  0.       ,  0.       , ..., -0.0561737, -0.0162162,\n",
       "        -0.171437 ],\n",
       "       [ 0.       ,  0.       ,  0.       , ..., -0.0559275, -0.0062211,\n",
       "        -0.172233 ]]), 0),\n",
       "       (array([[ 0.        ,  0.        ,  0.        , ...,  0.01849598,\n",
       "         0.0721854 , -0.16189   ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.10443107,\n",
       "         0.05172237, -0.133567  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.08676113,\n",
       "         0.05116256, -0.145192  ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.10355284,\n",
       "         0.52594266, -0.431462  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.09376174,\n",
       "         0.53507   , -0.434229  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.08766792,\n",
       "         0.541823  , -0.422078  ]]), 0),\n",
       "       (array([[ 0.        ,  0.        ,  0.        , ...,  0.10299776,\n",
       "        -0.001346  , -0.226027  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.10025404,\n",
       "        -0.0125163 , -0.230909  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.09391637,\n",
       "        -0.0240066 , -0.245108  ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.0862923 ,\n",
       "         0.0006496 , -0.251447  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.09493048,\n",
       "        -0.0351646 , -0.236531  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.11232334,\n",
       "        -0.0043682 , -0.221527  ]]), 0)], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making test and train split\n",
    "#the recentering has been done so that the pelvic joint is always at the origin\n",
    "#labels are to be zero indexed\n",
    "def train_test_split(dframe_list):\n",
    "    train_split = np.empty(0, dtype=object)\n",
    "    test_split = np.empty(0, dtype=object)\n",
    "    for dframe in dframe_list:\n",
    "        label = dframe.iloc[0,75]-1\n",
    "#         print(label)\n",
    "        num_samples = len(dframe.iloc[:,:])\n",
    "        video_ids = np.unique(dframe.iloc[:,-1].values)\n",
    "        train_video_ids = video_ids[:-15]\n",
    "        test_video_ids = video_ids[-15:]\n",
    "        train_split1 = np.empty(len(train_video_ids), dtype=object)\n",
    "        test_split1 = np.empty(len(test_video_ids), dtype=object)\n",
    "        for idx,i in enumerate(train_video_ids):\n",
    "            train_split1[idx] = dframe.loc[dframe['video_id'] == i].values[:,0:75]\n",
    "            for fidx, f in enumerate(train_split1[idx]):\n",
    "                f = np.reshape(f, (25,3))\n",
    "                f = f-f[0,:]\n",
    "                f = np.reshape(f, (1,75))\n",
    "                train_split1[idx][fidx] = f\n",
    "#             mean_vec = np.mean(train_split1[idx], axis=0)\n",
    "#             std_vec = np.std(train_split1[idx], axis=0)\n",
    "            train_split1[idx] = (train_split1[idx], label)\n",
    "\n",
    "        for idx,i in enumerate(test_video_ids):\n",
    "            test_split1[idx] = dframe.loc[dframe['video_id'] == i].values[:,0:75]\n",
    "            for fidx, f in enumerate(test_split1[idx]):\n",
    "                f = np.reshape(f, (25,3))\n",
    "                f = f-f[0,:]\n",
    "                f = np.reshape(f, (1,75))\n",
    "                test_split1[idx][fidx] = f\n",
    "#             mean_vec = np.mean(test_split1[idx], axis=0)\n",
    "#             std_vec = np.std(test_split1[idx], axis=0)\n",
    "            test_split1[idx] = (test_split1[idx], label)\n",
    "        train_split = np.concatenate((train_split, train_split1))\n",
    "        test_split = np.concatenate((test_split, test_split1))\n",
    "    return train_split, test_split\n",
    "\n",
    "train_split, test_split = train_test_split([dframe, dframe2, dframe3, dframe4, dframe5, dframe6, dframe7])\n",
    "\n",
    "# #looking at split\n",
    "train_split[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.        ,  0.        ,  0.        , ...,  0.2024111 ,\n",
       "         -0.01506584, -0.004548  ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.2055523 ,\n",
       "         -0.01783494,  0.019847  ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.20663304,\n",
       "          0.02533501,  0.020561  ],\n",
       "        ..., \n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.36056865,\n",
       "          0.14389151,  0.058361  ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.3562009 ,\n",
       "          0.16215682,  0.040082  ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.405074  ,\n",
       "          0.15019034,  0.055531  ]]), 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEQ_LEN = None\n",
    "def Data_gen( train_split, SEQ_LEN):\n",
    "    while(True):\n",
    "        X = train_split\n",
    "        databatch = random.sample(list(X), 1)[0]\n",
    "#         print(databatch)\n",
    "        databatch, label = databatch[0], databatch[1]\n",
    "        if SEQ_LEN is not None:\n",
    "            if len(databatch) > SEQ_LEN:\n",
    "                databatch = databatch[0:SEQ_LEN]\n",
    "            elif len(databatch) < SEQ_LEN:\n",
    "                databatch = np.concatenate((databatch, np.zeros((SEQ_LEN - len(databatch), 75))))\n",
    "            else:\n",
    "                pass\n",
    "            yield databatch,label\n",
    "        else:\n",
    "            yield databatch,label\n",
    "\n",
    "ACTd = Data_gen(train_split, SEQ_LEN)\n",
    "\n",
    "#to look at batch created by Actd\n",
    "next(ACTd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Classifier model defination and intialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action LSTM\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, joints_dim, hidden_dim, label_size, batch_size, num_layers, kernel_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(joints_dim, hidden_dim, num_layers=self.num_layers)\n",
    "        self.conv1 = nn.Conv1d(1, 1, kernel_size, stride=1, padding=1)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()),\n",
    "                autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()))\n",
    "    \n",
    "    def forward(self, joints3d_vec):\n",
    "        x = joints3d_vec\n",
    "#         print('x : ',x.size())\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "#         print('lstm_out[-1] : ', lstm_out[-1].size())\n",
    "        t = lstm_out[-1].view(1,1,-1)\n",
    "        y1 = self.conv1(t)\n",
    "#         print('y1 : ', y1.size())\n",
    "        y1 = y1.view(1,-1)\n",
    "#         print('y1 : ', y1.size())\n",
    "        y  = self.hidden2label(y1)\n",
    "        log_probs = F.log_softmax(y, dim=1)\n",
    "        return log_probs\n",
    "#instanstiating a model\n",
    "model0 = LSTMClassifier(75, 512, 7, 1, 2, 3)\n",
    "#to do stuff in CUDA\n",
    "model0 = model0.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, test_split):\n",
    "    pred_labels = np.empty(len(test_split))\n",
    "    orig_labels = np.array([t[1] for t in test_split])\n",
    "    for i in range(len(test_split)):\n",
    "        d_in = autograd.Variable(torch.from_numpy(test_split[i][0]).float().cuda())\n",
    "        d_in = d_in.view(d_in.size()[0], 1, -1)\n",
    "        y_pred = model(d_in)\n",
    "        pred_labels[i] = y_pred.data.cpu().max(1)[1].numpy()[0];\n",
    "    n_samples = len(pred_labels)\n",
    "    res=(orig_labels==pred_labels)\n",
    "    correct_count = (res==True).sum()\n",
    "    return (correct_count*100/n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training ...\n",
      "epoch 0 starting ...\n",
      "epoch: 0 iterations: 0 loss :0.290866\n",
      "epoch: 0 iterations: 100 loss :0.234462\n",
      "epoch: 0 iterations: 200 loss :0.00158863\n",
      "epoch: 0 iterations: 300 loss :0.027255\n",
      "epoch: 0 iterations: 400 loss :0.0100944\n",
      "epoch: 0 iterations: 500 loss :0.0554237\n",
      "epoch: 0 iterations: 600 loss :0.00692503\n",
      "epoch: 0 iterations: 700 loss :0.00505206\n",
      "epoch: 0 iterations: 800 loss :0.0108041\n",
      "epoch: 0 iterations: 900 loss :0.0282719\n",
      "epoch: 0 <====train track===> avg_loss: 0.004225606746345312, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 1 starting ...\n",
      "epoch: 1 iterations: 0 loss :0.00858604\n",
      "epoch: 1 iterations: 100 loss :0.00859443\n",
      "epoch: 1 iterations: 200 loss :2.49549\n",
      "epoch: 1 iterations: 300 loss :0.590845\n",
      "epoch: 1 iterations: 400 loss :1.53027\n",
      "epoch: 1 iterations: 500 loss :0.000518188\n",
      "epoch: 1 iterations: 600 loss :0.0165257\n",
      "epoch: 1 iterations: 700 loss :0.0914848\n",
      "epoch: 1 iterations: 800 loss :0.00760304\n",
      "epoch: 1 iterations: 900 loss :0.00553966\n",
      "epoch: 1 <====train track===> avg_loss: 0.0046936824940461645, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 2 starting ...\n",
      "epoch: 2 iterations: 0 loss :0.0208629\n",
      "epoch: 2 iterations: 100 loss :0.0794726\n",
      "epoch: 2 iterations: 200 loss :0.230976\n",
      "epoch: 2 iterations: 300 loss :0.074834\n",
      "epoch: 2 iterations: 400 loss :0.196258\n",
      "epoch: 2 iterations: 500 loss :6.41757\n",
      "epoch: 2 iterations: 600 loss :0.000824469\n",
      "epoch: 2 iterations: 700 loss :0.205172\n",
      "epoch: 2 iterations: 800 loss :0.86217\n",
      "epoch: 2 iterations: 900 loss :0.00622287\n",
      "epoch: 2 <====train track===> avg_loss: 0.004349500191403026, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 3 starting ...\n",
      "epoch: 3 iterations: 0 loss :0.00193528\n",
      "epoch: 3 iterations: 100 loss :0.00540901\n",
      "epoch: 3 iterations: 200 loss :1.80307\n",
      "epoch: 3 iterations: 300 loss :0.0374959\n",
      "epoch: 3 iterations: 400 loss :0.0212796\n",
      "epoch: 3 iterations: 500 loss :0.00470068\n",
      "epoch: 3 iterations: 600 loss :0.74928\n",
      "epoch: 3 iterations: 700 loss :0.00560794\n",
      "epoch: 3 iterations: 800 loss :0.000471957\n",
      "epoch: 3 iterations: 900 loss :0.0216039\n",
      "epoch: 3 <====train track===> avg_loss: 0.004526980680873463, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 4 starting ...\n",
      "epoch: 4 iterations: 0 loss :0.411618\n",
      "epoch: 4 iterations: 100 loss :0.2083\n",
      "epoch: 4 iterations: 200 loss :0.0101775\n",
      "epoch: 4 iterations: 300 loss :0.140398\n",
      "epoch: 4 iterations: 400 loss :0.800277\n",
      "epoch: 4 iterations: 500 loss :1.24961\n",
      "epoch: 4 iterations: 600 loss :0.00535992\n",
      "epoch: 4 iterations: 700 loss :0.491487\n",
      "epoch: 4 iterations: 800 loss :0.261132\n",
      "epoch: 4 iterations: 900 loss :0.0282547\n",
      "epoch: 4 <====train track===> avg_loss: 0.004097941157130637, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 5 starting ...\n",
      "epoch: 5 iterations: 0 loss :0.0238871\n",
      "epoch: 5 iterations: 100 loss :0.0081542\n",
      "epoch: 5 iterations: 200 loss :0.0553867\n",
      "epoch: 5 iterations: 300 loss :1.04683\n",
      "epoch: 5 iterations: 400 loss :0.27778\n",
      "epoch: 5 iterations: 500 loss :0.0659687\n",
      "epoch: 5 iterations: 600 loss :1.3729\n",
      "epoch: 5 iterations: 700 loss :0.00308062\n",
      "epoch: 5 iterations: 800 loss :0.468453\n",
      "epoch: 5 iterations: 900 loss :0.0114227\n",
      "epoch: 5 <====train track===> avg_loss: 0.004009029037360752, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 6 starting ...\n",
      "epoch: 6 iterations: 0 loss :0.0816764\n",
      "epoch: 6 iterations: 100 loss :0.664555\n",
      "epoch: 6 iterations: 200 loss :0.0417211\n",
      "epoch: 6 iterations: 300 loss :0.00550113\n",
      "epoch: 6 iterations: 400 loss :0.0310288\n",
      "epoch: 6 iterations: 500 loss :0.00435749\n",
      "epoch: 6 iterations: 600 loss :0.0286328\n",
      "epoch: 6 iterations: 700 loss :0.00461952\n",
      "epoch: 6 iterations: 800 loss :0.0141908\n",
      "epoch: 6 iterations: 900 loss :0.0209767\n",
      "epoch: 6 <====train track===> avg_loss: 0.004258947616430893, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 7 starting ...\n",
      "epoch: 7 iterations: 0 loss :0.028785\n",
      "epoch: 7 iterations: 100 loss :0.306054\n",
      "epoch: 7 iterations: 200 loss :0.185176\n",
      "epoch: 7 iterations: 300 loss :0.00907357\n",
      "epoch: 7 iterations: 400 loss :0.860754\n",
      "epoch: 7 iterations: 500 loss :0.413854\n",
      "epoch: 7 iterations: 600 loss :0.00903825\n",
      "epoch: 7 iterations: 700 loss :0.0924069\n",
      "epoch: 7 iterations: 800 loss :1.14311\n",
      "epoch: 7 iterations: 900 loss :0.0201672\n",
      "epoch: 7 <====train track===> avg_loss: 0.004268251171871827, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 8 starting ...\n",
      "epoch: 8 iterations: 0 loss :0.262288\n",
      "epoch: 8 iterations: 100 loss :0.0181312\n",
      "epoch: 8 iterations: 200 loss :0.191363\n",
      "epoch: 8 iterations: 300 loss :0.658839\n",
      "epoch: 8 iterations: 400 loss :0.129779\n",
      "epoch: 8 iterations: 500 loss :0.00232688\n",
      "epoch: 8 iterations: 600 loss :0.0340847\n",
      "epoch: 8 iterations: 700 loss :0.0528954\n",
      "epoch: 8 iterations: 800 loss :0.508328\n",
      "epoch: 8 iterations: 900 loss :0.0176452\n",
      "epoch: 8 <====train track===> avg_loss: 0.0045571457272470545, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 9 starting ...\n",
      "epoch: 9 iterations: 0 loss :0.118003\n",
      "epoch: 9 iterations: 100 loss :0.0648425\n",
      "epoch: 9 iterations: 200 loss :0.137385\n",
      "epoch: 9 iterations: 300 loss :0.616486\n",
      "epoch: 9 iterations: 400 loss :0.00599704\n",
      "epoch: 9 iterations: 500 loss :0.0136961\n",
      "epoch: 9 iterations: 600 loss :0.0249069\n",
      "epoch: 9 iterations: 700 loss :0.0206197\n",
      "epoch: 9 iterations: 800 loss :0.00143175\n",
      "epoch: 9 iterations: 900 loss :0.197943\n",
      "epoch: 9 <====train track===> avg_loss: 0.00446726575124619, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 10 starting ...\n",
      "epoch: 10 iterations: 0 loss :0.0225361\n",
      "epoch: 10 iterations: 100 loss :0.0331605\n",
      "epoch: 10 iterations: 200 loss :0.0211679\n",
      "epoch: 10 iterations: 300 loss :0.154157\n",
      "epoch: 10 iterations: 400 loss :0.000630299\n",
      "epoch: 10 iterations: 500 loss :0.138709\n",
      "epoch: 10 iterations: 600 loss :0.69379\n",
      "epoch: 10 iterations: 700 loss :0.344922\n",
      "epoch: 10 iterations: 800 loss :0.137868\n",
      "epoch: 10 iterations: 900 loss :0.0549072\n",
      "epoch: 10 <====train track===> avg_loss: 0.004970342421045194, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 11 starting ...\n",
      "epoch: 11 iterations: 0 loss :0.0420638\n",
      "epoch: 11 iterations: 100 loss :0.0363215\n",
      "epoch: 11 iterations: 200 loss :1.39808\n",
      "epoch: 11 iterations: 300 loss :0.934485\n",
      "epoch: 11 iterations: 400 loss :0.0999867\n",
      "epoch: 11 iterations: 500 loss :1.12883\n",
      "epoch: 11 iterations: 600 loss :0.99058\n",
      "epoch: 11 iterations: 700 loss :0.00128639\n",
      "epoch: 11 iterations: 800 loss :0.00063292\n",
      "epoch: 11 iterations: 900 loss :0.00693024\n",
      "epoch: 11 <====train track===> avg_loss: 0.003884554475825825, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 12 starting ...\n",
      "epoch: 12 iterations: 0 loss :0.0758695\n",
      "epoch: 12 iterations: 100 loss :0.0182612\n",
      "epoch: 12 iterations: 200 loss :0.0867773\n",
      "epoch: 12 iterations: 300 loss :0.00115816\n",
      "epoch: 12 iterations: 400 loss :0.294848\n",
      "epoch: 12 iterations: 500 loss :1.44614\n",
      "epoch: 12 iterations: 600 loss :0.217872\n",
      "epoch: 12 iterations: 700 loss :0.00897516\n",
      "epoch: 12 iterations: 800 loss :0.0800829\n",
      "epoch: 12 iterations: 900 loss :1.61126\n",
      "epoch: 12 <====train track===> avg_loss: 0.004287003980468927, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 13 starting ...\n",
      "epoch: 13 iterations: 0 loss :0.0506983\n",
      "epoch: 13 iterations: 100 loss :1.82929\n",
      "epoch: 13 iterations: 200 loss :0.0614818\n",
      "epoch: 13 iterations: 300 loss :0.191895\n",
      "epoch: 13 iterations: 400 loss :0.00337436\n",
      "epoch: 13 iterations: 500 loss :0.0047097\n",
      "epoch: 13 iterations: 600 loss :0.0543908\n",
      "epoch: 13 iterations: 700 loss :0.700537\n",
      "epoch: 13 iterations: 800 loss :0.00186186\n",
      "epoch: 13 iterations: 900 loss :0.181895\n",
      "epoch: 13 <====train track===> avg_loss: 0.0037074226721787035, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 14 starting ...\n",
      "epoch: 14 iterations: 0 loss :0.00110494\n",
      "epoch: 14 iterations: 100 loss :0.00551121\n",
      "epoch: 14 iterations: 200 loss :0.127177\n",
      "epoch: 14 iterations: 300 loss :0.376129\n",
      "epoch: 14 iterations: 400 loss :0.00292517\n",
      "epoch: 14 iterations: 500 loss :0.156068\n",
      "epoch: 14 iterations: 600 loss :0.00592808\n",
      "epoch: 14 iterations: 700 loss :0.0496793\n",
      "epoch: 14 iterations: 800 loss :0.0783953\n",
      "epoch: 14 iterations: 900 loss :0.271041\n",
      "epoch: 14 <====train track===> avg_loss: 0.004493654129033969, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 15 starting ...\n",
      "epoch: 15 iterations: 0 loss :0.0472434\n",
      "epoch: 15 iterations: 100 loss :1.05331\n",
      "epoch: 15 iterations: 200 loss :0.12736\n",
      "epoch: 15 iterations: 300 loss :0.0139273\n",
      "epoch: 15 iterations: 400 loss :0.287069\n",
      "epoch: 15 iterations: 500 loss :0.00311307\n",
      "epoch: 15 iterations: 600 loss :0.00785583\n",
      "epoch: 15 iterations: 700 loss :0.00251216\n",
      "epoch: 15 iterations: 800 loss :0.0019461\n",
      "epoch: 15 iterations: 900 loss :0.104003\n",
      "epoch: 15 <====train track===> avg_loss: 0.004336078480057908, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 16 starting ...\n",
      "epoch: 16 iterations: 0 loss :0.00143187\n",
      "epoch: 16 iterations: 100 loss :0.0413008\n",
      "epoch: 16 iterations: 200 loss :0.00849987\n",
      "epoch: 16 iterations: 300 loss :0.414147\n",
      "epoch: 16 iterations: 400 loss :0.0709152\n",
      "epoch: 16 iterations: 500 loss :0.00141997\n",
      "epoch: 16 iterations: 600 loss :0.188176\n",
      "epoch: 16 iterations: 700 loss :0.00708733\n",
      "epoch: 16 iterations: 800 loss :0.0022956\n",
      "epoch: 16 iterations: 900 loss :0.383237\n",
      "epoch: 16 <====train track===> avg_loss: 0.004208341592532004, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 17 starting ...\n",
      "epoch: 17 iterations: 0 loss :2.5579\n",
      "epoch: 17 iterations: 100 loss :0.131396\n",
      "epoch: 17 iterations: 200 loss :0.00181248\n",
      "epoch: 17 iterations: 300 loss :0.058585\n",
      "epoch: 17 iterations: 400 loss :0.00991432\n",
      "epoch: 17 iterations: 500 loss :0.0657926\n",
      "epoch: 17 iterations: 600 loss :2.70422\n",
      "epoch: 17 iterations: 700 loss :0.205453\n",
      "epoch: 17 iterations: 800 loss :0.000988233\n",
      "epoch: 17 iterations: 900 loss :0.0034675\n",
      "epoch: 17 <====train track===> avg_loss: 0.004082290035932403, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 18 starting ...\n",
      "epoch: 18 iterations: 0 loss :0.379693\n",
      "epoch: 18 iterations: 100 loss :0.50712\n",
      "epoch: 18 iterations: 200 loss :0.338053\n",
      "epoch: 18 iterations: 300 loss :0.00394673\n",
      "epoch: 18 iterations: 400 loss :0.579848\n",
      "epoch: 18 iterations: 500 loss :0.963511\n",
      "epoch: 18 iterations: 600 loss :0.0652197\n",
      "epoch: 18 iterations: 700 loss :0.00374355\n",
      "epoch: 18 iterations: 800 loss :0.00918839\n",
      "epoch: 18 iterations: 900 loss :0.00342283\n",
      "epoch: 18 <====train track===> avg_loss: 0.004256890828891767, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 19 starting ...\n",
      "epoch: 19 iterations: 0 loss :0.603141\n",
      "epoch: 19 iterations: 100 loss :0.243235\n",
      "epoch: 19 iterations: 200 loss :0.0881445\n",
      "epoch: 19 iterations: 300 loss :0.00191434\n",
      "epoch: 19 iterations: 400 loss :4.17039\n",
      "epoch: 19 iterations: 500 loss :0.747113\n",
      "epoch: 19 iterations: 600 loss :1.49366\n",
      "epoch: 19 iterations: 700 loss :0.0338817\n",
      "epoch: 19 iterations: 800 loss :0.595901\n",
      "epoch: 19 iterations: 900 loss :0.449223\n",
      "epoch: 19 <====train track===> avg_loss: 0.004155593968411641, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 20 starting ...\n",
      "epoch: 20 iterations: 0 loss :1.24789\n",
      "epoch: 20 iterations: 100 loss :0.297236\n",
      "epoch: 20 iterations: 200 loss :0.10078\n",
      "epoch: 20 iterations: 300 loss :0.00103111\n",
      "epoch: 20 iterations: 400 loss :0.000864251\n",
      "epoch: 20 iterations: 500 loss :0.395937\n",
      "epoch: 20 iterations: 600 loss :2.511\n",
      "epoch: 20 iterations: 700 loss :0.054252\n",
      "epoch: 20 iterations: 800 loss :0.00734971\n",
      "epoch: 20 iterations: 900 loss :0.00435321\n",
      "epoch: 20 <====train track===> avg_loss: 0.0035150671231340205, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 21 starting ...\n",
      "epoch: 21 iterations: 0 loss :0.0877939\n",
      "epoch: 21 iterations: 100 loss :0.15769\n",
      "epoch: 21 iterations: 200 loss :0.14131\n",
      "epoch: 21 iterations: 300 loss :0.0260666\n",
      "epoch: 21 iterations: 400 loss :0.0315219\n",
      "epoch: 21 iterations: 500 loss :0.00691237\n",
      "epoch: 21 iterations: 600 loss :0.0283726\n",
      "epoch: 21 iterations: 700 loss :2.59959\n",
      "epoch: 21 iterations: 800 loss :0.0443393\n",
      "epoch: 21 iterations: 900 loss :0.0538953\n",
      "epoch: 21 <====train track===> avg_loss: 0.003830653454307008, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 22 starting ...\n",
      "epoch: 22 iterations: 0 loss :0.0137092\n",
      "epoch: 22 iterations: 100 loss :0.0128603\n",
      "epoch: 22 iterations: 200 loss :1.21479\n",
      "epoch: 22 iterations: 300 loss :0.857752\n",
      "epoch: 22 iterations: 400 loss :0.113202\n",
      "epoch: 22 iterations: 500 loss :0.0294221\n",
      "epoch: 22 iterations: 600 loss :0.276912\n",
      "epoch: 22 iterations: 700 loss :0.00152269\n",
      "epoch: 22 iterations: 800 loss :0.0125915\n",
      "epoch: 22 iterations: 900 loss :0.00400491\n",
      "epoch: 22 <====train track===> avg_loss: 0.0038883736377798147, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 23 starting ...\n",
      "epoch: 23 iterations: 0 loss :0.00809413\n",
      "epoch: 23 iterations: 100 loss :0.00199155\n",
      "epoch: 23 iterations: 200 loss :0.00191933\n",
      "epoch: 23 iterations: 300 loss :0.00769319\n",
      "epoch: 23 iterations: 400 loss :0.00152769\n",
      "epoch: 23 iterations: 500 loss :0.274969\n",
      "epoch: 23 iterations: 600 loss :0.0279335\n",
      "epoch: 23 iterations: 700 loss :0.114085\n",
      "epoch: 23 iterations: 800 loss :2.78039\n",
      "epoch: 23 iterations: 900 loss :0.00110255\n",
      "epoch: 23 <====train track===> avg_loss: 0.0038224895243930686, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 24 starting ...\n",
      "epoch: 24 iterations: 0 loss :0.0408809\n",
      "epoch: 24 iterations: 100 loss :2.45255\n",
      "epoch: 24 iterations: 200 loss :0.0100623\n",
      "epoch: 24 iterations: 300 loss :0.00116983\n",
      "epoch: 24 iterations: 400 loss :2.20311\n",
      "epoch: 24 iterations: 500 loss :0.0305433\n",
      "epoch: 24 iterations: 600 loss :0.0584483\n",
      "epoch: 24 iterations: 700 loss :0.417154\n",
      "epoch: 24 iterations: 800 loss :0.002251\n",
      "epoch: 24 iterations: 900 loss :0.0122012\n",
      "epoch: 24 <====train track===> avg_loss: 0.0038645207095702643, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 25 starting ...\n",
      "epoch: 25 iterations: 0 loss :0.0125969\n",
      "epoch: 25 iterations: 100 loss :1.00147\n",
      "epoch: 25 iterations: 200 loss :0.000996212\n",
      "epoch: 25 iterations: 300 loss :0.233275\n",
      "epoch: 25 iterations: 400 loss :0.0688562\n",
      "epoch: 25 iterations: 500 loss :0.00570266\n",
      "epoch: 25 iterations: 600 loss :4.16802\n",
      "epoch: 25 iterations: 700 loss :0.664555\n",
      "epoch: 25 iterations: 800 loss :3.22823\n",
      "epoch: 25 iterations: 900 loss :0.0160908\n",
      "epoch: 25 <====train track===> avg_loss: 0.0034322869038810886, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 26 starting ...\n",
      "epoch: 26 iterations: 0 loss :0.019209\n",
      "epoch: 26 iterations: 100 loss :2.57629\n",
      "epoch: 26 iterations: 200 loss :0.00404528\n",
      "epoch: 26 iterations: 300 loss :0.0737574\n",
      "epoch: 26 iterations: 400 loss :0.0625865\n",
      "epoch: 26 iterations: 500 loss :0.0753545\n",
      "epoch: 26 iterations: 600 loss :0.00966453\n",
      "epoch: 26 iterations: 700 loss :0.0114327\n",
      "epoch: 26 iterations: 800 loss :0.0375946\n",
      "epoch: 26 iterations: 900 loss :1.04427\n",
      "epoch: 26 <====train track===> avg_loss: 0.0034495904394701055, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 27 starting ...\n",
      "epoch: 27 iterations: 0 loss :2.45235\n",
      "epoch: 27 iterations: 100 loss :0.242561\n",
      "epoch: 27 iterations: 200 loss :0.456754\n",
      "epoch: 27 iterations: 300 loss :0.270296\n",
      "epoch: 27 iterations: 400 loss :0.00546165\n",
      "epoch: 27 iterations: 500 loss :0.503845\n",
      "epoch: 27 iterations: 600 loss :0.0157562\n",
      "epoch: 27 iterations: 700 loss :0.0188978\n",
      "epoch: 27 iterations: 800 loss :0.586333\n",
      "epoch: 27 iterations: 900 loss :1.76939\n",
      "epoch: 27 <====train track===> avg_loss: 0.004566359053616009, accuracy: 80.95238095238095% \n",
      "\n",
      "epoch 28 starting ...\n",
      "epoch: 28 iterations: 0 loss :0.616435\n",
      "epoch: 28 iterations: 100 loss :0.181396\n",
      "epoch: 28 iterations: 200 loss :0.000629823\n",
      "epoch: 28 iterations: 300 loss :0.122429\n",
      "epoch: 28 iterations: 400 loss :0.0291264\n",
      "epoch: 28 iterations: 500 loss :0.00511777\n",
      "epoch: 28 iterations: 600 loss :0.0224665\n",
      "epoch: 28 iterations: 700 loss :0.0244773\n",
      "epoch: 28 iterations: 800 loss :0.0285325\n",
      "epoch: 28 iterations: 900 loss :0.101661\n",
      "epoch: 28 <====train track===> avg_loss: 0.0041149035280328965, accuracy: 81.9047619047619% \n",
      "\n",
      "epoch 29 starting ...\n",
      "epoch: 29 iterations: 0 loss :0.0162016\n",
      "epoch: 29 iterations: 100 loss :0.00786281\n",
      "epoch: 29 iterations: 200 loss :1.93012\n",
      "epoch: 29 iterations: 300 loss :0.0504269\n",
      "epoch: 29 iterations: 400 loss :0.0120956\n",
      "epoch: 29 iterations: 500 loss :0.00192219\n",
      "epoch: 29 iterations: 600 loss :0.121635\n",
      "epoch: 29 iterations: 700 loss :0.00322228\n",
      "epoch: 29 iterations: 800 loss :0.524258\n",
      "epoch: 29 iterations: 900 loss :0.0297936\n",
      "epoch: 29 <====train track===> avg_loss: 0.0037995415754536093, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 30 starting ...\n",
      "epoch: 30 iterations: 0 loss :0.00120198\n",
      "epoch: 30 iterations: 100 loss :0.0090607\n",
      "epoch: 30 iterations: 200 loss :0.0078647\n",
      "epoch: 30 iterations: 300 loss :0.0194005\n",
      "epoch: 30 iterations: 400 loss :0.0184184\n",
      "epoch: 30 iterations: 500 loss :0.0124924\n",
      "epoch: 30 iterations: 600 loss :0.0377912\n",
      "epoch: 30 iterations: 700 loss :3.79028\n",
      "epoch: 30 iterations: 800 loss :0.0357351\n",
      "epoch: 30 iterations: 900 loss :0.128101\n",
      "epoch: 30 <====train track===> avg_loss: 0.0038996285296529843, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 31 starting ...\n",
      "epoch: 31 iterations: 0 loss :0.185165\n",
      "epoch: 31 iterations: 100 loss :0.122193\n",
      "epoch: 31 iterations: 200 loss :0.00927674\n",
      "epoch: 31 iterations: 300 loss :0.36395\n",
      "epoch: 31 iterations: 400 loss :0.212235\n",
      "epoch: 31 iterations: 500 loss :0.0428409\n",
      "epoch: 31 iterations: 600 loss :0.124316\n",
      "epoch: 31 iterations: 700 loss :0.677756\n",
      "epoch: 31 iterations: 800 loss :0.0165009\n",
      "epoch: 31 iterations: 900 loss :0.0268059\n",
      "epoch: 31 <====train track===> avg_loss: 0.003380592651987295, accuracy: 80.95238095238095% \n",
      "\n",
      "epoch 32 starting ...\n",
      "epoch: 32 iterations: 0 loss :0.00530609\n",
      "epoch: 32 iterations: 100 loss :0.863058\n",
      "epoch: 32 iterations: 200 loss :1.9383\n",
      "epoch: 32 iterations: 300 loss :1.03181\n",
      "epoch: 32 iterations: 400 loss :0.0129317\n",
      "epoch: 32 iterations: 500 loss :0.0362614\n",
      "epoch: 32 iterations: 600 loss :0.0743983\n",
      "epoch: 32 iterations: 700 loss :0.0137983\n",
      "epoch: 32 iterations: 800 loss :0.0212096\n",
      "epoch: 32 iterations: 900 loss :0.194917\n",
      "epoch: 32 <====train track===> avg_loss: 0.003997754498079337, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 33 starting ...\n",
      "epoch: 33 iterations: 0 loss :0.00496405\n",
      "epoch: 33 iterations: 100 loss :0.00188602\n",
      "epoch: 33 iterations: 200 loss :0.262424\n",
      "epoch: 33 iterations: 300 loss :0.0104852\n",
      "epoch: 33 iterations: 400 loss :0.00365886\n",
      "epoch: 33 iterations: 500 loss :0.00131425\n",
      "epoch: 33 iterations: 600 loss :0.010853\n",
      "epoch: 33 iterations: 700 loss :0.540141\n",
      "epoch: 33 iterations: 800 loss :0.22132\n",
      "epoch: 33 iterations: 900 loss :0.000711069\n",
      "epoch: 33 <====train track===> avg_loss: 0.003789688051258358, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 34 starting ...\n",
      "epoch: 34 iterations: 0 loss :0.172784\n",
      "epoch: 34 iterations: 100 loss :0.0443248\n",
      "epoch: 34 iterations: 200 loss :0.368682\n",
      "epoch: 34 iterations: 300 loss :0.177151\n",
      "epoch: 34 iterations: 400 loss :4.79784\n",
      "epoch: 34 iterations: 500 loss :1.49462\n",
      "epoch: 34 iterations: 600 loss :0.00486535\n",
      "epoch: 34 iterations: 700 loss :0.0115803\n",
      "epoch: 34 iterations: 800 loss :1.98757\n",
      "epoch: 34 iterations: 900 loss :0.014972\n",
      "epoch: 34 <====train track===> avg_loss: 0.0039465812803058, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 35 starting ...\n",
      "epoch: 35 iterations: 0 loss :0.0604221\n",
      "epoch: 35 iterations: 100 loss :0.0598194\n",
      "epoch: 35 iterations: 200 loss :1.47251\n",
      "epoch: 35 iterations: 300 loss :0.00315466\n",
      "epoch: 35 iterations: 400 loss :0.755794\n",
      "epoch: 35 iterations: 500 loss :0.0273942\n",
      "epoch: 35 iterations: 600 loss :0.111835\n",
      "epoch: 35 iterations: 700 loss :0.066636\n",
      "epoch: 35 iterations: 800 loss :0.091451\n",
      "epoch: 35 iterations: 900 loss :0.54678\n",
      "epoch: 35 <====train track===> avg_loss: 0.003604506120648066, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 36 starting ...\n",
      "epoch: 36 iterations: 0 loss :0.315215\n",
      "epoch: 36 iterations: 100 loss :0.0130311\n",
      "epoch: 36 iterations: 200 loss :0.00377739\n",
      "epoch: 36 iterations: 300 loss :0.00276493\n",
      "epoch: 36 iterations: 400 loss :0.0103564\n",
      "epoch: 36 iterations: 500 loss :0.22097\n",
      "epoch: 36 iterations: 600 loss :0.108896\n",
      "epoch: 36 iterations: 700 loss :0.00692681\n",
      "epoch: 36 iterations: 800 loss :1.19337\n",
      "epoch: 36 iterations: 900 loss :0.365659\n",
      "epoch: 36 <====train track===> avg_loss: 0.003773031366369819, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 37 starting ...\n",
      "epoch: 37 iterations: 0 loss :0.0609443\n",
      "epoch: 37 iterations: 100 loss :0.627454\n",
      "epoch: 37 iterations: 200 loss :0.00132497\n",
      "epoch: 37 iterations: 300 loss :0.0448732\n",
      "epoch: 37 iterations: 400 loss :0.101296\n",
      "epoch: 37 iterations: 500 loss :0.248932\n",
      "epoch: 37 iterations: 600 loss :0.0455632\n",
      "epoch: 37 iterations: 700 loss :0.552609\n",
      "epoch: 37 iterations: 800 loss :0.16214\n",
      "epoch: 37 iterations: 900 loss :0.0157265\n",
      "epoch: 37 <====train track===> avg_loss: 0.003730988694963508, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 38 starting ...\n",
      "epoch: 38 iterations: 0 loss :1.11028\n",
      "epoch: 38 iterations: 100 loss :0.00270644\n",
      "epoch: 38 iterations: 200 loss :0.118814\n",
      "epoch: 38 iterations: 300 loss :0.015181\n",
      "epoch: 38 iterations: 400 loss :0.309383\n",
      "epoch: 38 iterations: 500 loss :0.0121787\n",
      "epoch: 38 iterations: 600 loss :0.103153\n",
      "epoch: 38 iterations: 700 loss :0.0013583\n",
      "epoch: 38 iterations: 800 loss :0.0194926\n",
      "epoch: 38 iterations: 900 loss :1.99652\n",
      "epoch: 38 <====train track===> avg_loss: 0.0036378021137646456, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 39 starting ...\n",
      "epoch: 39 iterations: 0 loss :0.00654257\n",
      "epoch: 39 iterations: 100 loss :0.084781\n",
      "epoch: 39 iterations: 200 loss :0.298158\n",
      "epoch: 39 iterations: 300 loss :0.0544131\n",
      "epoch: 39 iterations: 400 loss :0.994668\n",
      "epoch: 39 iterations: 500 loss :1.57138\n",
      "epoch: 39 iterations: 600 loss :0.00734534\n",
      "epoch: 39 iterations: 700 loss :0.530123\n",
      "epoch: 39 iterations: 800 loss :0.654761\n",
      "epoch: 39 iterations: 900 loss :0.143019\n",
      "epoch: 39 <====train track===> avg_loss: 0.003734119442401271, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 40 starting ...\n",
      "epoch: 40 iterations: 0 loss :0.212301\n",
      "epoch: 40 iterations: 100 loss :0.0608953\n",
      "epoch: 40 iterations: 200 loss :0.000884618\n",
      "epoch: 40 iterations: 300 loss :1.49991\n",
      "epoch: 40 iterations: 400 loss :0.028689\n",
      "epoch: 40 iterations: 500 loss :0.260809\n",
      "epoch: 40 iterations: 600 loss :0.211068\n",
      "epoch: 40 iterations: 700 loss :1.18525\n",
      "epoch: 40 iterations: 800 loss :0.02397\n",
      "epoch: 40 iterations: 900 loss :0.0385869\n",
      "epoch: 40 <====train track===> avg_loss: 0.0038002994421415127, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 41 starting ...\n",
      "epoch: 41 iterations: 0 loss :0.979377\n",
      "epoch: 41 iterations: 100 loss :1.2778\n",
      "epoch: 41 iterations: 200 loss :0.0151246\n",
      "epoch: 41 iterations: 300 loss :0.0361807\n",
      "epoch: 41 iterations: 400 loss :0.00916843\n",
      "epoch: 41 iterations: 500 loss :0.00381362\n",
      "epoch: 41 iterations: 600 loss :0.0015703\n",
      "epoch: 41 iterations: 700 loss :0.0570942\n",
      "epoch: 41 iterations: 800 loss :0.031308\n",
      "epoch: 41 iterations: 900 loss :0.0102645\n",
      "epoch: 41 <====train track===> avg_loss: 0.0035751312596148826, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 42 starting ...\n",
      "epoch: 42 iterations: 0 loss :0.00899477\n",
      "epoch: 42 iterations: 100 loss :0.00720249\n",
      "epoch: 42 iterations: 200 loss :0.00449006\n",
      "epoch: 42 iterations: 300 loss :0.384296\n",
      "epoch: 42 iterations: 400 loss :0.915716\n",
      "epoch: 42 iterations: 500 loss :0.347244\n",
      "epoch: 42 iterations: 600 loss :0.00937323\n",
      "epoch: 42 iterations: 700 loss :0.0146399\n",
      "epoch: 42 iterations: 800 loss :0.00343032\n",
      "epoch: 42 iterations: 900 loss :0.000878306\n",
      "epoch: 42 <====train track===> avg_loss: 0.0038813288322766025, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 43 starting ...\n",
      "epoch: 43 iterations: 0 loss :1.22237\n",
      "epoch: 43 iterations: 100 loss :0.00188768\n",
      "epoch: 43 iterations: 200 loss :0.00241441\n",
      "epoch: 43 iterations: 300 loss :0.0145241\n",
      "epoch: 43 iterations: 400 loss :0.000855557\n",
      "epoch: 43 iterations: 500 loss :2.25209\n",
      "epoch: 43 iterations: 600 loss :0.0036034\n",
      "epoch: 43 iterations: 700 loss :0.000317761\n",
      "epoch: 43 iterations: 800 loss :0.102973\n",
      "epoch: 43 iterations: 900 loss :2.23461\n",
      "epoch: 43 <====train track===> avg_loss: 0.003946577051667687, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 44 starting ...\n",
      "epoch: 44 iterations: 0 loss :1.34312\n",
      "epoch: 44 iterations: 100 loss :0.371341\n",
      "epoch: 44 iterations: 200 loss :0.0563703\n",
      "epoch: 44 iterations: 300 loss :1.25335\n",
      "epoch: 44 iterations: 400 loss :0.00231106\n",
      "epoch: 44 iterations: 500 loss :0.0423593\n",
      "epoch: 44 iterations: 600 loss :0.0161865\n",
      "epoch: 44 iterations: 700 loss :0.0895872\n",
      "epoch: 44 iterations: 800 loss :0.00735812\n",
      "epoch: 44 iterations: 900 loss :0.396033\n",
      "epoch: 44 <====train track===> avg_loss: 0.004039243228210748, accuracy: 87.61904761904762% \n",
      "\n",
      "epoch 45 starting ...\n",
      "epoch: 45 iterations: 0 loss :0.000992521\n",
      "epoch: 45 iterations: 100 loss :0.0414983\n",
      "epoch: 45 iterations: 200 loss :0.00753454\n",
      "epoch: 45 iterations: 300 loss :1.18458\n",
      "epoch: 45 iterations: 400 loss :0.0136792\n",
      "epoch: 45 iterations: 500 loss :0.00420519\n",
      "epoch: 45 iterations: 600 loss :1.02593\n",
      "epoch: 45 iterations: 700 loss :0.143767\n",
      "epoch: 45 iterations: 800 loss :3.3516\n",
      "epoch: 45 iterations: 900 loss :0.4267\n",
      "epoch: 45 <====train track===> avg_loss: 0.003938889112847592, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 46 starting ...\n",
      "epoch: 46 iterations: 0 loss :0.30266\n",
      "epoch: 46 iterations: 100 loss :0.466589\n",
      "epoch: 46 iterations: 200 loss :0.0295328\n",
      "epoch: 46 iterations: 300 loss :0.691664\n",
      "epoch: 46 iterations: 400 loss :0.0249178\n",
      "epoch: 46 iterations: 500 loss :0.011023\n",
      "epoch: 46 iterations: 600 loss :0.927182\n",
      "epoch: 46 iterations: 700 loss :0.640923\n",
      "epoch: 46 iterations: 800 loss :0.0379841\n",
      "epoch: 46 iterations: 900 loss :0.0726631\n",
      "epoch: 46 <====train track===> avg_loss: 0.0036081348037847296, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 47 starting ...\n",
      "epoch: 47 iterations: 0 loss :0.00574817\n",
      "epoch: 47 iterations: 100 loss :0.002603\n",
      "epoch: 47 iterations: 200 loss :0.00713361\n",
      "epoch: 47 iterations: 300 loss :0.00122936\n",
      "epoch: 47 iterations: 400 loss :0.00928087\n",
      "epoch: 47 iterations: 500 loss :0.0817919\n",
      "epoch: 47 iterations: 600 loss :0.0830813\n",
      "epoch: 47 iterations: 700 loss :0.0260023\n",
      "epoch: 47 iterations: 800 loss :0.173089\n",
      "epoch: 47 iterations: 900 loss :1.63315\n",
      "epoch: 47 <====train track===> avg_loss: 0.0035936954052911475, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 48 starting ...\n",
      "epoch: 48 iterations: 0 loss :0.263817\n",
      "epoch: 48 iterations: 100 loss :0.00358118\n",
      "epoch: 48 iterations: 200 loss :0.00571795\n",
      "epoch: 48 iterations: 300 loss :0.441767\n",
      "epoch: 48 iterations: 400 loss :0.0205913\n",
      "epoch: 48 iterations: 500 loss :0.00442253\n",
      "epoch: 48 iterations: 600 loss :0.00229774\n",
      "epoch: 48 iterations: 700 loss :0.0295349\n",
      "epoch: 48 iterations: 800 loss :0.233186\n",
      "epoch: 48 iterations: 900 loss :0.607331\n",
      "epoch: 48 <====train track===> avg_loss: 0.0037300475539646105, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 49 starting ...\n",
      "epoch: 49 iterations: 0 loss :1.63617\n",
      "epoch: 49 iterations: 100 loss :0.0204559\n",
      "epoch: 49 iterations: 200 loss :0.00345705\n",
      "epoch: 49 iterations: 300 loss :0.00532459\n",
      "epoch: 49 iterations: 400 loss :0.156234\n",
      "epoch: 49 iterations: 500 loss :0.0180886\n",
      "epoch: 49 iterations: 600 loss :0.00288142\n",
      "epoch: 49 iterations: 700 loss :0.0049446\n",
      "epoch: 49 iterations: 800 loss :0.00666858\n",
      "epoch: 49 iterations: 900 loss :0.0024552\n",
      "epoch: 49 <====train track===> avg_loss: 0.0038030712475295384, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 50 starting ...\n",
      "epoch: 50 iterations: 0 loss :0.0127883\n",
      "epoch: 50 iterations: 100 loss :0.00472097\n",
      "epoch: 50 iterations: 200 loss :0.148514\n",
      "epoch: 50 iterations: 300 loss :0.00494092\n",
      "epoch: 50 iterations: 400 loss :1.03746\n",
      "epoch: 50 iterations: 500 loss :0.0177186\n",
      "epoch: 50 iterations: 600 loss :0.0131277\n",
      "epoch: 50 iterations: 700 loss :0.00588067\n",
      "epoch: 50 iterations: 800 loss :0.195007\n",
      "epoch: 50 iterations: 900 loss :0.0134925\n",
      "epoch: 50 <====train track===> avg_loss: 0.004076772276670594, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 51 starting ...\n",
      "epoch: 51 iterations: 0 loss :0.0199001\n",
      "epoch: 51 iterations: 100 loss :0.0301091\n",
      "epoch: 51 iterations: 200 loss :0.000849244\n",
      "epoch: 51 iterations: 300 loss :0.00924886\n",
      "epoch: 51 iterations: 400 loss :0.00156007\n",
      "epoch: 51 iterations: 500 loss :0.0402593\n",
      "epoch: 51 iterations: 600 loss :0.0859496\n",
      "epoch: 51 iterations: 700 loss :0.00883834\n",
      "epoch: 51 iterations: 800 loss :0.0435474\n",
      "epoch: 51 iterations: 900 loss :0.141314\n",
      "epoch: 51 <====train track===> avg_loss: 0.0037071437922906515, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 52 starting ...\n",
      "epoch: 52 iterations: 0 loss :0.000547021\n",
      "epoch: 52 iterations: 100 loss :0.00480355\n",
      "epoch: 52 iterations: 200 loss :1.14193\n",
      "epoch: 52 iterations: 300 loss :0.0995905\n",
      "epoch: 52 iterations: 400 loss :0.300193\n",
      "epoch: 52 iterations: 500 loss :0.0225226\n",
      "epoch: 52 iterations: 600 loss :0.537747\n",
      "epoch: 52 iterations: 700 loss :0.256982\n",
      "epoch: 52 iterations: 800 loss :0.00442632\n",
      "epoch: 52 iterations: 900 loss :0.00816295\n",
      "epoch: 52 <====train track===> avg_loss: 0.003498796116991263, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 53 starting ...\n",
      "epoch: 53 iterations: 0 loss :0.00164981\n",
      "epoch: 53 iterations: 100 loss :0.053385\n",
      "epoch: 53 iterations: 200 loss :0.0276568\n",
      "epoch: 53 iterations: 300 loss :0.305965\n",
      "epoch: 53 iterations: 400 loss :0.00126627\n",
      "epoch: 53 iterations: 500 loss :4.10211\n",
      "epoch: 53 iterations: 600 loss :0.00829891\n",
      "epoch: 53 iterations: 700 loss :0.0269704\n",
      "epoch: 53 iterations: 800 loss :0.000740016\n",
      "epoch: 53 iterations: 900 loss :0.0137076\n",
      "epoch: 53 <====train track===> avg_loss: 0.0036887170547914103, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 54 starting ...\n",
      "epoch: 54 iterations: 0 loss :1.29313\n",
      "epoch: 54 iterations: 100 loss :0.0200063\n",
      "epoch: 54 iterations: 200 loss :0.00555401\n",
      "epoch: 54 iterations: 300 loss :0.0200604\n",
      "epoch: 54 iterations: 400 loss :0.00278181\n",
      "epoch: 54 iterations: 500 loss :1.77487\n",
      "epoch: 54 iterations: 600 loss :0.130289\n",
      "epoch: 54 iterations: 700 loss :0.000794453\n",
      "epoch: 54 iterations: 800 loss :0.00850047\n",
      "epoch: 54 iterations: 900 loss :0.36978\n",
      "epoch: 54 <====train track===> avg_loss: 0.0033859257480459526, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 55 starting ...\n",
      "epoch: 55 iterations: 0 loss :0.0015346\n",
      "epoch: 55 iterations: 100 loss :0.281471\n",
      "epoch: 55 iterations: 200 loss :0.000858415\n",
      "epoch: 55 iterations: 300 loss :0.00855377\n",
      "epoch: 55 iterations: 400 loss :1.06522\n",
      "epoch: 55 iterations: 500 loss :0.00407116\n",
      "epoch: 55 iterations: 600 loss :0.0173316\n",
      "epoch: 55 iterations: 700 loss :0.166623\n",
      "epoch: 55 iterations: 800 loss :0.00835081\n",
      "epoch: 55 iterations: 900 loss :0.0993307\n",
      "epoch: 55 <====train track===> avg_loss: 0.003513376729462793, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 56 starting ...\n",
      "epoch: 56 iterations: 0 loss :0.0168979\n",
      "epoch: 56 iterations: 100 loss :0.0867403\n",
      "epoch: 56 iterations: 200 loss :0.193456\n",
      "epoch: 56 iterations: 300 loss :0.0195297\n",
      "epoch: 56 iterations: 400 loss :0.0230598\n",
      "epoch: 56 iterations: 500 loss :0.0142557\n",
      "epoch: 56 iterations: 600 loss :0.0393645\n",
      "epoch: 56 iterations: 700 loss :0.0430609\n",
      "epoch: 56 iterations: 800 loss :0.00831014\n",
      "epoch: 56 iterations: 900 loss :0.0110991\n",
      "epoch: 56 <====train track===> avg_loss: 0.003456472875833906, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 57 starting ...\n",
      "epoch: 57 iterations: 0 loss :0.0136461\n",
      "epoch: 57 iterations: 100 loss :0.0599082\n",
      "epoch: 57 iterations: 200 loss :0.00757133\n",
      "epoch: 57 iterations: 300 loss :0.188805\n",
      "epoch: 57 iterations: 400 loss :0.000808627\n",
      "epoch: 57 iterations: 500 loss :0.0034675\n",
      "epoch: 57 iterations: 600 loss :0.00345657\n",
      "epoch: 57 iterations: 700 loss :0.000711188\n",
      "epoch: 57 iterations: 800 loss :0.00422181\n",
      "epoch: 57 iterations: 900 loss :0.161346\n",
      "epoch: 57 <====train track===> avg_loss: 0.003698052453107385, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 58 starting ...\n",
      "epoch: 58 iterations: 0 loss :0.157707\n",
      "epoch: 58 iterations: 100 loss :0.0478744\n",
      "epoch: 58 iterations: 200 loss :0.313687\n",
      "epoch: 58 iterations: 300 loss :0.0572649\n",
      "epoch: 58 iterations: 400 loss :0.00186175\n",
      "epoch: 58 iterations: 500 loss :2.53917\n",
      "epoch: 58 iterations: 600 loss :0.103\n",
      "epoch: 58 iterations: 700 loss :0.0203776\n",
      "epoch: 58 iterations: 800 loss :0.0192161\n",
      "epoch: 58 iterations: 900 loss :0.0229174\n",
      "epoch: 58 <====train track===> avg_loss: 0.003821415935784445, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 59 starting ...\n",
      "epoch: 59 iterations: 0 loss :2.33104\n",
      "epoch: 59 iterations: 100 loss :0.0441632\n",
      "epoch: 59 iterations: 200 loss :0.443216\n",
      "epoch: 59 iterations: 300 loss :0.0230209\n",
      "epoch: 59 iterations: 400 loss :0.00553563\n",
      "epoch: 59 iterations: 500 loss :0.000833402\n",
      "epoch: 59 iterations: 600 loss :0.605853\n",
      "epoch: 59 iterations: 700 loss :0.0578945\n",
      "epoch: 59 iterations: 800 loss :0.365108\n",
      "epoch: 59 iterations: 900 loss :0.31828\n",
      "epoch: 59 <====train track===> avg_loss: 0.0037683237283983434, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 60 starting ...\n",
      "epoch: 60 iterations: 0 loss :0.113486\n",
      "epoch: 60 iterations: 100 loss :0.0304312\n",
      "epoch: 60 iterations: 200 loss :0.075901\n",
      "epoch: 60 iterations: 300 loss :0.498967\n",
      "epoch: 60 iterations: 400 loss :0.00726037\n",
      "epoch: 60 iterations: 500 loss :0.0067054\n",
      "epoch: 60 iterations: 600 loss :0.00176239\n",
      "epoch: 60 iterations: 700 loss :0.0504641\n",
      "epoch: 60 iterations: 800 loss :0.00774204\n",
      "epoch: 60 iterations: 900 loss :0.125093\n",
      "epoch: 60 <====train track===> avg_loss: 0.003386952527110512, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 61 starting ...\n",
      "epoch: 61 iterations: 0 loss :0.000454442\n",
      "epoch: 61 iterations: 100 loss :0.550133\n",
      "epoch: 61 iterations: 200 loss :0.0312484\n",
      "epoch: 61 iterations: 300 loss :2.16322\n",
      "epoch: 61 iterations: 400 loss :0.0218113\n",
      "epoch: 61 iterations: 500 loss :0.0377226\n",
      "epoch: 61 iterations: 600 loss :0.00392441\n",
      "epoch: 61 iterations: 700 loss :0.306185\n",
      "epoch: 61 iterations: 800 loss :0.0217991\n",
      "epoch: 61 iterations: 900 loss :0.00748721\n",
      "epoch: 61 <====train track===> avg_loss: 0.003424033403720757, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 62 starting ...\n",
      "epoch: 62 iterations: 0 loss :0.0351012\n",
      "epoch: 62 iterations: 100 loss :0.00631871\n",
      "epoch: 62 iterations: 200 loss :0.00494495\n",
      "epoch: 62 iterations: 300 loss :0.0367251\n",
      "epoch: 62 iterations: 400 loss :0.0114853\n",
      "epoch: 62 iterations: 500 loss :0.314661\n",
      "epoch: 62 iterations: 600 loss :0.076551\n",
      "epoch: 62 iterations: 700 loss :0.0125987\n",
      "epoch: 62 iterations: 800 loss :0.0154595\n",
      "epoch: 62 iterations: 900 loss :0.3742\n",
      "epoch: 62 <====train track===> avg_loss: 0.003981883334156885, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 63 starting ...\n",
      "epoch: 63 iterations: 0 loss :0.0047657\n",
      "epoch: 63 iterations: 100 loss :0.0251337\n",
      "epoch: 63 iterations: 200 loss :0.0131779\n",
      "epoch: 63 iterations: 300 loss :1.58943\n",
      "epoch: 63 iterations: 400 loss :0.0460347\n",
      "epoch: 63 iterations: 500 loss :0.0715224\n",
      "epoch: 63 iterations: 600 loss :0.00840271\n",
      "epoch: 63 iterations: 700 loss :0.00197811\n",
      "epoch: 63 iterations: 800 loss :0.139267\n",
      "epoch: 63 iterations: 900 loss :0.0202146\n",
      "epoch: 63 <====train track===> avg_loss: 0.0038382357847508135, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 64 starting ...\n",
      "epoch: 64 iterations: 0 loss :0.000745734\n",
      "epoch: 64 iterations: 100 loss :0.0132672\n",
      "epoch: 64 iterations: 200 loss :0.00746142\n",
      "epoch: 64 iterations: 300 loss :0.00914244\n",
      "epoch: 64 iterations: 400 loss :0.00680948\n",
      "epoch: 64 iterations: 500 loss :0.526078\n",
      "epoch: 64 iterations: 600 loss :0.0034713\n",
      "epoch: 64 iterations: 700 loss :0.00152198\n",
      "epoch: 64 iterations: 800 loss :1.80487\n",
      "epoch: 64 iterations: 900 loss :0.135886\n",
      "epoch: 64 <====train track===> avg_loss: 0.003271053654796439, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 65 starting ...\n",
      "epoch: 65 iterations: 0 loss :0.0171985\n",
      "epoch: 65 iterations: 100 loss :0.222559\n",
      "epoch: 65 iterations: 200 loss :0.169657\n",
      "epoch: 65 iterations: 300 loss :0.263032\n",
      "epoch: 65 iterations: 400 loss :0.217133\n",
      "epoch: 65 iterations: 500 loss :0.22345\n",
      "epoch: 65 iterations: 600 loss :0.209903\n",
      "epoch: 65 iterations: 700 loss :0.0109763\n",
      "epoch: 65 iterations: 800 loss :0.0222343\n",
      "epoch: 65 iterations: 900 loss :0.114959\n",
      "epoch: 65 <====train track===> avg_loss: 0.0033581474484569358, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 66 starting ...\n",
      "epoch: 66 iterations: 0 loss :0.00452874\n",
      "epoch: 66 iterations: 100 loss :0.334989\n",
      "epoch: 66 iterations: 200 loss :0.0294105\n",
      "epoch: 66 iterations: 300 loss :1.76872\n",
      "epoch: 66 iterations: 400 loss :0.00381991\n",
      "epoch: 66 iterations: 500 loss :0.0161924\n",
      "epoch: 66 iterations: 600 loss :0.0777396\n",
      "epoch: 66 iterations: 700 loss :0.0116361\n",
      "epoch: 66 iterations: 800 loss :0.0508167\n",
      "epoch: 66 iterations: 900 loss :3.49612\n",
      "epoch: 66 <====train track===> avg_loss: 0.002881024997610654, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 67 starting ...\n",
      "epoch: 67 iterations: 0 loss :0.0285731\n",
      "epoch: 67 iterations: 100 loss :0.120076\n",
      "epoch: 67 iterations: 200 loss :0.0029632\n",
      "epoch: 67 iterations: 300 loss :0.354214\n",
      "epoch: 67 iterations: 400 loss :0.0353578\n",
      "epoch: 67 iterations: 500 loss :0.32674\n",
      "epoch: 67 iterations: 600 loss :0.112176\n",
      "epoch: 67 iterations: 700 loss :0.00877997\n",
      "epoch: 67 iterations: 800 loss :0.235198\n",
      "epoch: 67 iterations: 900 loss :1.98903\n",
      "epoch: 67 <====train track===> avg_loss: 0.0038215261681742584, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 68 starting ...\n",
      "epoch: 68 iterations: 0 loss :0.00426514\n",
      "epoch: 68 iterations: 100 loss :1.14804\n",
      "epoch: 68 iterations: 200 loss :0.0324157\n",
      "epoch: 68 iterations: 300 loss :1.74214\n",
      "epoch: 68 iterations: 400 loss :0.00448329\n",
      "epoch: 68 iterations: 500 loss :0.0846878\n",
      "epoch: 68 iterations: 600 loss :0.0831122\n",
      "epoch: 68 iterations: 700 loss :0.00936768\n",
      "epoch: 68 iterations: 800 loss :0.356127\n",
      "epoch: 68 iterations: 900 loss :0.0985234\n",
      "epoch: 68 <====train track===> avg_loss: 0.003938981252712068, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 69 starting ...\n",
      "epoch: 69 iterations: 0 loss :0.00858864\n",
      "epoch: 69 iterations: 100 loss :0.0672258\n",
      "epoch: 69 iterations: 200 loss :0.221843\n",
      "epoch: 69 iterations: 300 loss :0.0111861\n",
      "epoch: 69 iterations: 400 loss :0.00676307\n",
      "epoch: 69 iterations: 500 loss :1.07102\n",
      "epoch: 69 iterations: 600 loss :0.00207388\n",
      "epoch: 69 iterations: 700 loss :0.00097025\n",
      "epoch: 69 iterations: 800 loss :0.0114767\n",
      "epoch: 69 iterations: 900 loss :0.00146044\n",
      "epoch: 69 <====train track===> avg_loss: 0.0037623930684177465, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 70 starting ...\n",
      "epoch: 70 iterations: 0 loss :0.000904389\n",
      "epoch: 70 iterations: 100 loss :0.385935\n",
      "epoch: 70 iterations: 200 loss :0.000415001\n",
      "epoch: 70 iterations: 300 loss :0.0396277\n",
      "epoch: 70 iterations: 400 loss :0.186364\n",
      "epoch: 70 iterations: 500 loss :0.0608144\n",
      "epoch: 70 iterations: 600 loss :0.241418\n",
      "epoch: 70 iterations: 700 loss :0.000938571\n",
      "epoch: 70 iterations: 800 loss :0.0201014\n",
      "epoch: 70 iterations: 900 loss :0.00452637\n",
      "epoch: 70 <====train track===> avg_loss: 0.0036286745217321693, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 71 starting ...\n",
      "epoch: 71 iterations: 0 loss :0.000435138\n",
      "epoch: 71 iterations: 100 loss :0.0397384\n",
      "epoch: 71 iterations: 200 loss :0.233774\n",
      "epoch: 71 iterations: 300 loss :1.2685\n",
      "epoch: 71 iterations: 400 loss :0.00114959\n",
      "epoch: 71 iterations: 500 loss :0.247956\n",
      "epoch: 71 iterations: 600 loss :0.0526512\n",
      "epoch: 71 iterations: 700 loss :0.00140175\n",
      "epoch: 71 iterations: 800 loss :5.79039\n",
      "epoch: 71 iterations: 900 loss :0.0101849\n",
      "epoch: 71 <====train track===> avg_loss: 0.0035391354249612182, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 72 starting ...\n",
      "epoch: 72 iterations: 0 loss :0.272025\n",
      "epoch: 72 iterations: 100 loss :0.00269277\n",
      "epoch: 72 iterations: 200 loss :0.0240729\n",
      "epoch: 72 iterations: 300 loss :0.00713172\n",
      "epoch: 72 iterations: 400 loss :0.0738766\n",
      "epoch: 72 iterations: 500 loss :0.0140289\n",
      "epoch: 72 iterations: 600 loss :0.894783\n",
      "epoch: 72 iterations: 700 loss :0.00706247\n",
      "epoch: 72 iterations: 800 loss :0.0409251\n",
      "epoch: 72 iterations: 900 loss :0.0116199\n",
      "epoch: 72 <====train track===> avg_loss: 0.003619246631276124, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 73 starting ...\n",
      "epoch: 73 iterations: 0 loss :0.0700861\n",
      "epoch: 73 iterations: 100 loss :0.0197359\n",
      "epoch: 73 iterations: 200 loss :0.0964746\n",
      "epoch: 73 iterations: 300 loss :0.937882\n",
      "epoch: 73 iterations: 400 loss :0.164486\n",
      "epoch: 73 iterations: 500 loss :0.0319369\n",
      "epoch: 73 iterations: 600 loss :0.0350728\n",
      "epoch: 73 iterations: 700 loss :0.311382\n",
      "epoch: 73 iterations: 800 loss :0.0229531\n",
      "epoch: 73 iterations: 900 loss :0.165468\n",
      "epoch: 73 <====train track===> avg_loss: 0.0035911022426088363, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 74 starting ...\n",
      "epoch: 74 iterations: 0 loss :0.0449747\n",
      "epoch: 74 iterations: 100 loss :0.385751\n",
      "epoch: 74 iterations: 200 loss :0.00223922\n",
      "epoch: 74 iterations: 300 loss :0.0207745\n",
      "epoch: 74 iterations: 400 loss :0.0177473\n",
      "epoch: 74 iterations: 500 loss :0.0387168\n",
      "epoch: 74 iterations: 600 loss :0.0193282\n",
      "epoch: 74 iterations: 700 loss :0.0622752\n",
      "epoch: 74 iterations: 800 loss :0.026341\n",
      "epoch: 74 iterations: 900 loss :0.00147353\n",
      "epoch: 74 <====train track===> avg_loss: 0.0035725934455567336, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 75 starting ...\n",
      "epoch: 75 iterations: 0 loss :0.241043\n",
      "epoch: 75 iterations: 100 loss :0.00613141\n",
      "epoch: 75 iterations: 200 loss :0.130142\n",
      "epoch: 75 iterations: 300 loss :0.00481244\n",
      "epoch: 75 iterations: 400 loss :1.79072\n",
      "epoch: 75 iterations: 500 loss :0.00921745\n",
      "epoch: 75 iterations: 600 loss :0.215523\n",
      "epoch: 75 iterations: 700 loss :0.040449\n",
      "epoch: 75 iterations: 800 loss :0.12835\n",
      "epoch: 75 iterations: 900 loss :2.15231\n",
      "epoch: 75 <====train track===> avg_loss: 0.0038655472281487416, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 76 starting ...\n",
      "epoch: 76 iterations: 0 loss :0.0121007\n",
      "epoch: 76 iterations: 100 loss :0.00378309\n",
      "epoch: 76 iterations: 200 loss :0.00633162\n",
      "epoch: 76 iterations: 300 loss :0.797565\n",
      "epoch: 76 iterations: 400 loss :0.00356313\n",
      "epoch: 76 iterations: 500 loss :0.0405103\n",
      "epoch: 76 iterations: 600 loss :0.324938\n",
      "epoch: 76 iterations: 700 loss :0.00176655\n",
      "epoch: 76 iterations: 800 loss :3.0515\n",
      "epoch: 76 iterations: 900 loss :0.00518371\n",
      "epoch: 76 <====train track===> avg_loss: 0.003666068140166767, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 77 starting ...\n",
      "epoch: 77 iterations: 0 loss :0.0617016\n",
      "epoch: 77 iterations: 100 loss :0.000982636\n",
      "epoch: 77 iterations: 200 loss :0.00184973\n",
      "epoch: 77 iterations: 300 loss :0.0431025\n",
      "epoch: 77 iterations: 400 loss :0.0680149\n",
      "epoch: 77 iterations: 500 loss :0.0660986\n",
      "epoch: 77 iterations: 600 loss :0.355251\n",
      "epoch: 77 iterations: 700 loss :0.00848581\n",
      "epoch: 77 iterations: 800 loss :0.057231\n",
      "epoch: 77 iterations: 900 loss :0.0488556\n",
      "epoch: 77 <====train track===> avg_loss: 0.0036248977264298956, accuracy: 87.61904761904762% \n",
      "\n",
      "epoch 78 starting ...\n",
      "epoch: 78 iterations: 0 loss :0.0832923\n",
      "epoch: 78 iterations: 100 loss :0.00350302\n",
      "epoch: 78 iterations: 200 loss :0.0226305\n",
      "epoch: 78 iterations: 300 loss :2.01737\n",
      "epoch: 78 iterations: 400 loss :2.50692\n",
      "epoch: 78 iterations: 500 loss :0.118612\n",
      "epoch: 78 iterations: 600 loss :0.146582\n",
      "epoch: 78 iterations: 700 loss :0.134281\n",
      "epoch: 78 iterations: 800 loss :0.0832985\n",
      "epoch: 78 iterations: 900 loss :0.00608437\n",
      "epoch: 78 <====train track===> avg_loss: 0.0037553322402954442, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 79 starting ...\n",
      "epoch: 79 iterations: 0 loss :1.07591\n",
      "epoch: 79 iterations: 100 loss :0.00127008\n",
      "epoch: 79 iterations: 200 loss :0.00327931\n",
      "epoch: 79 iterations: 300 loss :0.199073\n",
      "epoch: 79 iterations: 400 loss :0.006807\n",
      "epoch: 79 iterations: 500 loss :0.838719\n",
      "epoch: 79 iterations: 600 loss :0.0402026\n",
      "epoch: 79 iterations: 700 loss :0.00440793\n",
      "epoch: 79 iterations: 800 loss :0.0060839\n",
      "epoch: 79 iterations: 900 loss :0.0667134\n",
      "epoch: 79 <====train track===> avg_loss: 0.003621754130871522, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 80 starting ...\n",
      "epoch: 80 iterations: 0 loss :1.28732\n",
      "epoch: 80 iterations: 100 loss :0.316391\n",
      "epoch: 80 iterations: 200 loss :0.0034839\n",
      "epoch: 80 iterations: 300 loss :0.0106194\n",
      "epoch: 80 iterations: 400 loss :0.00488386\n",
      "epoch: 80 iterations: 500 loss :0.0221037\n",
      "epoch: 80 iterations: 600 loss :0.229814\n",
      "epoch: 80 iterations: 700 loss :0.837384\n",
      "epoch: 80 iterations: 800 loss :0.0724473\n",
      "epoch: 80 iterations: 900 loss :3.28987\n",
      "epoch: 80 <====train track===> avg_loss: 0.0031182259162423113, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 81 starting ...\n",
      "epoch: 81 iterations: 0 loss :0.0116032\n",
      "epoch: 81 iterations: 100 loss :1.05241\n",
      "epoch: 81 iterations: 200 loss :0.0882641\n",
      "epoch: 81 iterations: 300 loss :0.730598\n",
      "epoch: 81 iterations: 400 loss :0.0011821\n",
      "epoch: 81 iterations: 500 loss :0.00197692\n",
      "epoch: 81 iterations: 600 loss :0.000290709\n",
      "epoch: 81 iterations: 700 loss :0.421038\n",
      "epoch: 81 iterations: 800 loss :0.00343246\n",
      "epoch: 81 iterations: 900 loss :0.0856939\n",
      "epoch: 81 <====train track===> avg_loss: 0.0031460965036148912, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 82 starting ...\n",
      "epoch: 82 iterations: 0 loss :0.00588245\n",
      "epoch: 82 iterations: 100 loss :0.00288095\n",
      "epoch: 82 iterations: 200 loss :0.00245341\n",
      "epoch: 82 iterations: 300 loss :0.0048166\n",
      "epoch: 82 iterations: 400 loss :0.0153623\n",
      "epoch: 82 iterations: 500 loss :0.1143\n",
      "epoch: 82 iterations: 600 loss :1.23609\n",
      "epoch: 82 iterations: 700 loss :0.0495866\n",
      "epoch: 82 iterations: 800 loss :0.041099\n",
      "epoch: 82 iterations: 900 loss :0.0131851\n",
      "epoch: 82 <====train track===> avg_loss: 0.0038444830632142794, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 83 starting ...\n",
      "epoch: 83 iterations: 0 loss :0.00326493\n",
      "epoch: 83 iterations: 100 loss :0.0176226\n",
      "epoch: 83 iterations: 200 loss :0.469241\n",
      "epoch: 83 iterations: 300 loss :0.000736323\n",
      "epoch: 83 iterations: 400 loss :0.21007\n",
      "epoch: 83 iterations: 500 loss :0.687501\n",
      "epoch: 83 iterations: 600 loss :0.106985\n",
      "epoch: 83 iterations: 700 loss :0.0187307\n",
      "epoch: 83 iterations: 800 loss :0.363892\n",
      "epoch: 83 iterations: 900 loss :0.0526083\n",
      "epoch: 83 <====train track===> avg_loss: 0.003556366176001008, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 84 starting ...\n",
      "epoch: 84 iterations: 0 loss :0.00376706\n",
      "epoch: 84 iterations: 100 loss :0.0018878\n",
      "epoch: 84 iterations: 200 loss :0.00265294\n",
      "epoch: 84 iterations: 300 loss :0.000983351\n",
      "epoch: 84 iterations: 400 loss :0.00263938\n",
      "epoch: 84 iterations: 500 loss :0.75779\n",
      "epoch: 84 iterations: 600 loss :0.539659\n",
      "epoch: 84 iterations: 700 loss :0.00391966\n",
      "epoch: 84 iterations: 800 loss :0.480777\n",
      "epoch: 84 iterations: 900 loss :0.0011328\n",
      "epoch: 84 <====train track===> avg_loss: 0.003126804124800113, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 85 starting ...\n",
      "epoch: 85 iterations: 0 loss :0.0013558\n",
      "epoch: 85 iterations: 100 loss :0.186999\n",
      "epoch: 85 iterations: 200 loss :0.00201773\n",
      "epoch: 85 iterations: 300 loss :0.0430319\n",
      "epoch: 85 iterations: 400 loss :0.0124685\n",
      "epoch: 85 iterations: 500 loss :0.00379069\n",
      "epoch: 85 iterations: 600 loss :0.387104\n",
      "epoch: 85 iterations: 700 loss :0.00616968\n",
      "epoch: 85 iterations: 800 loss :0.202816\n",
      "epoch: 85 iterations: 900 loss :0.1317\n",
      "epoch: 85 <====train track===> avg_loss: 0.0036309251325625123, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 86 starting ...\n",
      "epoch: 86 iterations: 0 loss :0.217721\n",
      "epoch: 86 iterations: 100 loss :0.00166123\n",
      "epoch: 86 iterations: 200 loss :0.0166621\n",
      "epoch: 86 iterations: 300 loss :0.379674\n",
      "epoch: 86 iterations: 400 loss :1.43196\n",
      "epoch: 86 iterations: 500 loss :0.0077683\n",
      "epoch: 86 iterations: 600 loss :0.491638\n",
      "epoch: 86 iterations: 700 loss :0.00747609\n",
      "epoch: 86 iterations: 800 loss :0.0057522\n",
      "epoch: 86 iterations: 900 loss :0.00989083\n",
      "epoch: 86 <====train track===> avg_loss: 0.003582346192053916, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 87 starting ...\n",
      "epoch: 87 iterations: 0 loss :0.00679006\n",
      "epoch: 87 iterations: 100 loss :1.58512\n",
      "epoch: 87 iterations: 200 loss :0.0627488\n",
      "epoch: 87 iterations: 300 loss :0.154412\n",
      "epoch: 87 iterations: 400 loss :0.035846\n",
      "epoch: 87 iterations: 500 loss :0.00152186\n",
      "epoch: 87 iterations: 600 loss :0.0777516\n",
      "epoch: 87 iterations: 700 loss :0.0078951\n",
      "epoch: 87 iterations: 800 loss :0.303822\n",
      "epoch: 87 iterations: 900 loss :0.497736\n",
      "epoch: 87 <====train track===> avg_loss: 0.003521449335434475, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 88 starting ...\n",
      "epoch: 88 iterations: 0 loss :4.79911\n",
      "epoch: 88 iterations: 100 loss :0.0047797\n",
      "epoch: 88 iterations: 200 loss :2.5889\n",
      "epoch: 88 iterations: 300 loss :0.0261062\n",
      "epoch: 88 iterations: 400 loss :0.00533846\n",
      "epoch: 88 iterations: 500 loss :0.0199074\n",
      "epoch: 88 iterations: 600 loss :0.00521929\n",
      "epoch: 88 iterations: 700 loss :1.56832\n",
      "epoch: 88 iterations: 800 loss :0.0615097\n",
      "epoch: 88 iterations: 900 loss :5.06748\n",
      "epoch: 88 <====train track===> avg_loss: 0.0036127401643403674, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 89 starting ...\n",
      "epoch: 89 iterations: 0 loss :0.00281652\n",
      "epoch: 89 iterations: 100 loss :0.0348362\n",
      "epoch: 89 iterations: 200 loss :0.558812\n",
      "epoch: 89 iterations: 300 loss :0.0482354\n",
      "epoch: 89 iterations: 400 loss :0.011294\n",
      "epoch: 89 iterations: 500 loss :0.0696622\n",
      "epoch: 89 iterations: 600 loss :0.00738474\n",
      "epoch: 89 iterations: 700 loss :0.0574839\n",
      "epoch: 89 iterations: 800 loss :0.000760504\n",
      "epoch: 89 iterations: 900 loss :0.00874133\n",
      "epoch: 89 <====train track===> avg_loss: 0.002950764779167132, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 90 starting ...\n",
      "epoch: 90 iterations: 0 loss :0.00178345\n",
      "epoch: 90 iterations: 100 loss :0.0804357\n",
      "epoch: 90 iterations: 200 loss :0.0140203\n",
      "epoch: 90 iterations: 300 loss :0.000352559\n",
      "epoch: 90 iterations: 400 loss :0.350337\n",
      "epoch: 90 iterations: 500 loss :1.64676\n",
      "epoch: 90 iterations: 600 loss :0.0850593\n",
      "epoch: 90 iterations: 700 loss :0.44752\n",
      "epoch: 90 iterations: 800 loss :0.356804\n",
      "epoch: 90 iterations: 900 loss :3.21121\n",
      "epoch: 90 <====train track===> avg_loss: 0.003296949424490149, accuracy: 87.61904761904762% \n",
      "\n",
      "epoch 91 starting ...\n",
      "epoch: 91 iterations: 0 loss :0.0266458\n",
      "epoch: 91 iterations: 100 loss :0.000914037\n",
      "epoch: 91 iterations: 200 loss :0.676864\n",
      "epoch: 91 iterations: 300 loss :0.0213106\n",
      "epoch: 91 iterations: 400 loss :1.5209\n",
      "epoch: 91 iterations: 500 loss :0.0328181\n",
      "epoch: 91 iterations: 600 loss :0.000934641\n",
      "epoch: 91 iterations: 700 loss :0.0063777\n",
      "epoch: 91 iterations: 800 loss :0.00996991\n",
      "epoch: 91 iterations: 900 loss :0.326909\n",
      "epoch: 91 <====train track===> avg_loss: 0.003279497524953109, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 92 starting ...\n",
      "epoch: 92 iterations: 0 loss :0.889865\n",
      "epoch: 92 iterations: 100 loss :0.000469098\n",
      "epoch: 92 iterations: 200 loss :0.000809818\n",
      "epoch: 92 iterations: 300 loss :1.3465\n",
      "epoch: 92 iterations: 400 loss :0.0021192\n",
      "epoch: 92 iterations: 500 loss :4.14613\n",
      "epoch: 92 iterations: 600 loss :0.055755\n",
      "epoch: 92 iterations: 700 loss :0.0376397\n",
      "epoch: 92 iterations: 800 loss :0.0140463\n",
      "epoch: 92 iterations: 900 loss :0.0122609\n",
      "epoch: 92 <====train track===> avg_loss: 0.003104371016088401, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 93 starting ...\n",
      "epoch: 93 iterations: 0 loss :0.00392358\n",
      "epoch: 93 iterations: 100 loss :0.325201\n",
      "epoch: 93 iterations: 200 loss :0.00135116\n",
      "epoch: 93 iterations: 300 loss :0.00174216\n",
      "epoch: 93 iterations: 400 loss :0.00131699\n",
      "epoch: 93 iterations: 500 loss :0.0296342\n",
      "epoch: 93 iterations: 600 loss :0.00117709\n",
      "epoch: 93 iterations: 700 loss :0.00144794\n",
      "epoch: 93 iterations: 800 loss :0.00729185\n",
      "epoch: 93 iterations: 900 loss :0.058692\n",
      "epoch: 93 <====train track===> avg_loss: 0.003473291696244884, accuracy: 89.52380952380952% \n",
      "\n",
      "epoch 94 starting ...\n",
      "epoch: 94 iterations: 0 loss :0.0735012\n",
      "epoch: 94 iterations: 100 loss :0.0407954\n",
      "epoch: 94 iterations: 200 loss :0.117514\n",
      "epoch: 94 iterations: 300 loss :0.211731\n",
      "epoch: 94 iterations: 400 loss :0.00732534\n",
      "epoch: 94 iterations: 500 loss :0.00339836\n",
      "epoch: 94 iterations: 600 loss :0.0154611\n",
      "epoch: 94 iterations: 700 loss :0.048714\n",
      "epoch: 94 iterations: 800 loss :0.361674\n",
      "epoch: 94 iterations: 900 loss :0.00124699\n",
      "epoch: 94 <====train track===> avg_loss: 0.0037137827871181477, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 95 starting ...\n",
      "epoch: 95 iterations: 0 loss :0.0214783\n",
      "epoch: 95 iterations: 100 loss :0.0056972\n",
      "epoch: 95 iterations: 200 loss :0.0346895\n",
      "epoch: 95 iterations: 300 loss :1.27149\n",
      "epoch: 95 iterations: 400 loss :0.0648905\n",
      "epoch: 95 iterations: 500 loss :0.00788398\n",
      "epoch: 95 iterations: 600 loss :0.00200928\n",
      "epoch: 95 iterations: 700 loss :0.00182498\n",
      "epoch: 95 iterations: 800 loss :0.0579122\n",
      "epoch: 95 iterations: 900 loss :0.0026099\n",
      "epoch: 95 <====train track===> avg_loss: 0.0032578665646384756, accuracy: 87.61904761904762% \n",
      "\n",
      "epoch 96 starting ...\n",
      "epoch: 96 iterations: 0 loss :1.57336\n",
      "epoch: 96 iterations: 100 loss :0.0163087\n",
      "epoch: 96 iterations: 200 loss :0.385342\n",
      "epoch: 96 iterations: 300 loss :0.126423\n",
      "epoch: 96 iterations: 400 loss :0.237592\n",
      "epoch: 96 iterations: 500 loss :0.193245\n",
      "epoch: 96 iterations: 600 loss :0.00148377\n",
      "epoch: 96 iterations: 700 loss :0.283731\n",
      "epoch: 96 iterations: 800 loss :0.00980537\n",
      "epoch: 96 iterations: 900 loss :1.52859\n",
      "epoch: 96 <====train track===> avg_loss: 0.002840649316997539, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 97 starting ...\n",
      "epoch: 97 iterations: 0 loss :0.16231\n",
      "epoch: 97 iterations: 100 loss :0.00293313\n",
      "epoch: 97 iterations: 200 loss :0.0572852\n",
      "epoch: 97 iterations: 300 loss :0.217929\n",
      "epoch: 97 iterations: 400 loss :0.013603\n",
      "epoch: 97 iterations: 500 loss :0.0011559\n",
      "epoch: 97 iterations: 600 loss :0.00802046\n",
      "epoch: 97 iterations: 700 loss :0.369673\n",
      "epoch: 97 iterations: 800 loss :0.00411818\n",
      "epoch: 97 iterations: 900 loss :1.02687\n",
      "epoch: 97 <====train track===> avg_loss: 0.003512272356130479, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 98 starting ...\n",
      "epoch: 98 iterations: 0 loss :0.0155486\n",
      "epoch: 98 iterations: 100 loss :0.0496709\n",
      "epoch: 98 iterations: 200 loss :0.00522901\n",
      "epoch: 98 iterations: 300 loss :0.556571\n",
      "epoch: 98 iterations: 400 loss :0.00330093\n",
      "epoch: 98 iterations: 500 loss :0.0233408\n",
      "epoch: 98 iterations: 600 loss :0.707746\n",
      "epoch: 98 iterations: 700 loss :0.00449729\n",
      "epoch: 98 iterations: 800 loss :0.000529983\n",
      "epoch: 98 iterations: 900 loss :0.357293\n",
      "epoch: 98 <====train track===> avg_loss: 0.0028659593368348083, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 99 starting ...\n",
      "epoch: 99 iterations: 0 loss :0.00180225\n",
      "epoch: 99 iterations: 100 loss :0.010136\n",
      "epoch: 99 iterations: 200 loss :0.014139\n",
      "epoch: 99 iterations: 300 loss :0.014135\n",
      "epoch: 99 iterations: 400 loss :0.0623955\n",
      "epoch: 99 iterations: 500 loss :0.00142163\n",
      "epoch: 99 iterations: 600 loss :0.0247593\n",
      "epoch: 99 iterations: 700 loss :0.222523\n",
      "epoch: 99 iterations: 800 loss :0.00369557\n",
      "epoch: 99 iterations: 900 loss :0.00490035\n",
      "epoch: 99 <====train track===> avg_loss: 0.0034272510047507476, accuracy: 88.57142857142857% \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'avg loss')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0a0e192c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEKCAYAAAAvlUMdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzsvXl8XGd97/9+Ztcs2iVb8r4lxlnsbE5CWBMoCZQkLKHhFn5wy1La8iuFXwvhLm3hQlta2hRuoS0tUC5LA4Qt8EtYExJIgh1ndZzEsbzLkm2tI2n25bl/nPOcOTOa5cxII4+U5/16+WXpzDlnHm3P93y3z1dIKdFoNBqNZrFxnesFaDQajWZlog2MRqPRaJqCNjAajUajaQrawGg0Go2mKWgDo9FoNJqmoA2MRqPRaJqCNjAajUajaQrawGg0Go2mKWgDo9FoNJqm4DnXCziX9Pb2yo0bN57rZWg0Gs2y4tFHHx2XUvbVOu8FbWA2btzIvn37zvUyNBqNZlkhhDju5DwdItNoNBpNU9AGRqPRaDRNoakGRghxvRDioBBiSAhxW5nX/UKIb5qv7xFCbLS99lHz+EEhxGtsx48JIfYLIZ4QQuyzHe8WQvxMCHHI/L+rmV+bRqPRaKrTNAMjhHADnwNuAHYAbxVC7Cg57V3AlJRyK3A78Cnz2h3ArcAFwPXA5837KV4ppdwlpbzcduw24BdSym3AL8zPNRqNRnOOaKYHsxsYklIekVKmgTuAm0rOuQn4ivnxncB1QghhHr9DSpmSUh4Fhsz7VcN+r68ANy/C16DRaDSaBmmmgVkDnLR9PmweK3uOlDILRIGeGtdK4KdCiEeFEO+1nbNKSjlq3msU6C+3KCHEe4UQ+4QQ+8bGxhr6wjQajUZTm2YaGFHmWOn4zErnVLv2GinlpRihtz8SQrysnkVJKb8gpbxcSnl5X1/NMm6NRqPRNEgzDcwwsM72+VpgpNI5QggP0AFMVrtWSqn+Pwt8j0Lo7IwQYsC81wBwdhG/lpbh3ufOcGo6ca6XodFoNDVppoF5BNgmhNgkhPBhJO3vKjnnLuAd5sdvBu6VUkrz+K1mldkmYBuwVwgREkJEAIQQIeC3gKfL3OsdwA+a9HWdM6SUvO+rj/HVhx31OGk0Gs05pWmd/FLKrBDi/cBPADfwJSnlASHEx4F9Usq7gC8CXxVCDGF4Lrea1x4QQnwLeAbIAn8kpcwJIVYB3zPqAPAA35BS/th8y78BviWEeBdwArilWV/buSKWzpHO5Umks+d6KRqNRlOTpkrFSCnvBu4uOfbnto+TVDAEUspPAp8sOXYE2Fnh/AngugUuuaWJJjIApHP5c7wSjUajqY3u5F9GROOGgUlltIHRaDStjzYwywjlwaSy2sBoNJrWRxuYZcRMUhmY3DleiUaj0dRGG5hlhPZgNBrNckIbmGXEjDYwGo1mGaENzDJCezAajWY5oQ3MMsIyMBmdg9FoNK2PNjDLCKsPRnswGo1mGaANzDJCh8g0Gs1yQhuYZUQhya9DZBqNpvXRBmYZoT0YjUaznNAGZhkRTRgil9rAaDSa5YA2MMsEKaUVIktn8xhTDTQajaZ10QZmmZDM5Enn8kQChgC29mI0Gk2row3MMkHlX/ojfkAbGI1G0/poA7NMUAamzzQwuhdGo9G0OtrALBOUknJ/JADoUmWNRtP6aAOzTFDDxnSITKPRLBe0gVkmWDmYdtPA6KmWGo2mxdEGZplQmoPRITKNRtPqaAOzTFAGpjesQ2QajWZ5oA3MMiGayBDxewj63ICuItNoNK2PNjAL5AdPnOJvf/xc099nJpGhvc2L32MYGO3BaDSaVkcbmAVyz/7TfGPviaa/TzSRoaPNi99j/Mh0Dkaj0bQ62sAskOlEmul4pukb/kxSGRjTg9FVZBqNpsXRBmaBTJv9KeNz6aa+j/JgfJYHow2MRqNpbbSBWSCquuvsTLLp79Pe5tEhMo1Gs2zQBmaBKA9mbDbV1PexcjBe40emq8g0Gk2row3MAkhmciQyhidxtokGJpXNkczkjRCZW4fINBrN8kAbmAWgwmPQXAOj3qejzYvH7cLjEjpEptFoWh5tYBaACo9Bc0NkapJle5sXAJ/HpavINBpNy6MNzAKYjhcqx8Zmm5fkjyaygOHBAPg9Lh0i02g0LY82MAtg2qYP1swQWakH4/e4dZJfo9G0PNrALAA1o+W8VeGmhsjsORgAv9elczAajabl0QZmAUwnjBDZtn7DwOTzctHurSZYQhkDs4JCZH/74+f4028/ea6XodFomoA2MAtgOp7B6xZs7A2RzUum4ovTzT8ZS3PFJ37Odx4dBsoZGPeKMTD7T0V55NjkuV6GRqNpAtrALICpeIaONh/9kQAAY3OLEyY7Oh4jlc1bIprRRIagz43X7IHxeVZOiCyVzTMVa67MjkajOTc01cAIIa4XQhwUQgwJIW4r87pfCPFN8/U9QoiNttc+ah4/KIR4Tcl1biHE40KIH9mO/YcQ4qgQ4gnz365mfm0A0USazqDXmjJ5dqZgYGKpLE+fijZ039FoAoBHj09xbDxmdfEr/CuoTDmVzTOTzJLNrYyvR6PRFGiagRFCuIHPATcAO4C3CiF2lJz2LmBKSrkVuB34lHntDuBW4ALgeuDz5v0UHwCeLfO2fyal3GX+e2JRv6AyTMczdLZ56VcGxpbo/8IDR3jj5x8imanf0xiZNgyMEPDdx4aZKWNg0itkQ1bVcDPJ7DleiUajWWya6cHsBoaklEeklGngDuCmknNuAr5ifnwncJ0QQpjH75BSpqSUR4Eh834IIdYCrwP+vYlrd8R0PFPkwdgryZ4aniadyxc1YzplZDpJ2O/hJVt7+e7jp5hOZGgP2A2MewV5MIYBXqz8lUajaR2aaWDWACdtnw+bx8qeI6XMAlGgp8a1/wh8GCi3w35SCPGUEOJ2IYR/wV9BDaKJDJ1BHyG/h5DPzVlbs+UzozNAYxvnaDTBYGeAN166huGpBE+cmLZ6YKC+MuWZZKbpQpwLQRnKaW1gNJoVRzMNjChzrLSOt9I5ZY8LIX4bOCulfLTM6x8FtgNXAN3AR8ouSoj3CiH2CSH2jY2NVVy8E6bjaTrNjb+/PWBt5BNzKc6Y+ZhGEtgj00kGOtp4zQWrCfncpHP5ohCZz+28TPkTP3qGt39xT91rWCpUqG8qVr+np9FoWptmGphhYJ3t87XASKVzhBAeoAOYrHLtNcCNQohjGCG3a4UQXwOQUo5KgxTwZcyQWilSyi9IKS+XUl7e19fX8BeXzuaJpXN0Bo2Nvy9S6OZ/dnTWOm9yAR5M0OfhhosGAIpzMF7nBubsbIrnTs+2rIeQyugQmUazUmmmgXkE2CaE2CSE8GEk7e8qOecu4B3mx28G7pVSSvP4rWaV2SZgG7BXSvlRKeVaKeVG8373SinfBiCEGDD/F8DNwNNN/NoKvSlBH2AYGOXBPDNaqB6r14NJZnKMz6UZ7GgD4I2XGpHB4iS/c6mYeNrYwJ8abqyirdkoD8auTK3RaFYGTTMwZk7l/cBPMCq+viWlPCCE+LgQ4kbztC8CPUKIIeBDwG3mtQeAbwHPAD8G/khKWSvp8HUhxH5gP9ALfGKxvyY7yiOwQmR2AzMyQ2/YSAFN1ZnkPx018jgDnYaBuWpTD3/4ii3ccNFq6xx/HX0wCdPAPHlyuq51LAVSSssT0x6MRrPy8DTz5lLKu4G7S479ue3jJHBLhWs/CXyyyr1/CfzS9vm1C1ttfSihS3uIbC6VJZ7O8uzoLBev7eCRo5NM1unBjJg9MIMdRvOmyyX48PXbi87xe9xkcpJcXuJ2lUtXFVAD0Z4cbj0Dk81LpJmVq9cQazSa1kd38jeIKj/ubDNCZKqb/+RkgqGxOXYMtNMV8tX9ZD4ybXgwg6YHU456xiYrD+aJk1GkXDyttMXAnkdq1RyRRqNpHG1gGsQKkQULITKAB4fGyeUlOwYNA1OvBzNqNlmuNj2YchTGJtcOk8XTWXweF+NzKUajzZtZ0wgpWxNqI/1CGo2mtdEGpkEKSf5CiAzg/ueN0ucdA+10B731ezDRJD0hHwGvu+I5yoNxUkkWT+e4ZF0n0Hp5GLsagQ6RaTQrD21gGmQ6nsHtEkT8RhpLeTC/OTJByOdmfXeQrqCv7v6OkelE1fAYGDkYqB0iy+WNJPplG7rwugVPtFgeRjVZet1Ch8g0mhWINjANMp0wmiyNqmjoCvrwuASpbJ7tA+24XKKhHMxoNMFAlfAYGFVkUDtEpnTQOtq87Bhob1kPpj8S0FVkGs0KRBuYBpmOZ6zwGBjVXqo0ecdAOwDdIR/xdK4uwcuR6aQDD8b4sSVr6JGpHpigz83OdZ3sH46SW8ShaAtFeTD97X6SmXxDwqAajaZ10QamQZSSsp3+dtPADBoGpstswnT6dD6TzDCXyjLYWd2D8Xmc5WBUBVmbz8POtZ3E0jmOjM05WstSoDyw1e3G16sT/RrNykIbmAaZTqTpNA2IQuVhXmR5MIYBcpqHGTVLlAc6nOVgaoXI4hlDAt/wYDoAeKKFwmQqh7TKNDA6TKbRrCy0gWmQch5MX8SPS8D5qyIAlgFyunGqOTA1Q2QO+2DilgfjZnNvmLDf01INl8oDU56fNjAazcqiqZ38K5loSQ4G4O1XbeTitZ20+QwPoztkGBinvTBWF3+NEJnfYYgsqQyM143LJbh4bUdLeTBq/SpEFtUhMo1mRaE9mAbI5PLMprJWF79ix2A7b9293vq83hzM6HQSt0tYqgCVKITInCf5AS5d38Wzo7PMJltjI1chvkKIrDXWpdFoFgdtYBpgpkSHrBLqdac5mJHpBKvbAzX1xSwPpkbVVTxTbGBevKWHXF7yyLFJR+tpNikrB6NDZBrNSkQbmAYoFbqshNftIhLwOM/BOOiBAechskTaSPK3+YxI6KUbuvB5XDw0NOFoPc1G5ZDaA14CXpduttRoVhjawDSAJXRZUkVWju469MhGppOWTH81nIbIVJly0JSdCXjdXL6hiwcPt4aBUev3e9x0BX26TFmjWWFoA9MApbNgqtEVdNbNn89LTkeTNRP8UL6K7N1feYSP/fBA0XkqRKaKDsAIkz07OlO3CGczUOv3eVx0Bn06B6PRrDC0gWmAggdT28B0O5SLOT4ZJ53LW5Msq1FOTfnpUzM8f2a26LxEOocQhZAawNVbegFDM+1co9bv87joCnp1iEyjWWFoA9MAVg6mrXaIrDPorZjkT2Zy3P6z57nxn37NtX//SwC29odr3tPlEvjcLivEJKVkMpZmLpktOi+ezhH0ui29NICdazsI+z08dHi87L1zeTnPUDWLdDaPxyVwu4TxfdIGRqNZUWgD0wDReBohIBKo3UbUHaycg3no8Dif+cUhBPDBV53HD9//Eq7Z2utoDT6Py9LymktlSefyzJYxMCrBr/C4Xeze1F0x0f+DJ07xmn98gOdOzzhax0JIZfOWd9WpczAazYpDN1o2wHQiQ0ebF1eNcmKArpCPRMYQvCyd8XJ8Ig7Al955BT2mUKZT/B6XFWJSBmymxMAkMzmrRNnOi7f0cO9zZ03l5uKQ3OMnppES7n5qlO2r2+taU72ksjn85vekK+hlOpFBSlnkcWk0muWL9mAaoJxMTCVUN3+58M+JyTghn9s6px78HpeVJJ8wDcxcqtgDiKeztJUZXPZiMw/zcJlqsgMjUQDufvp03Wuql3Q2b+WTuoI+cnnJbCpb4yqNRrNc0AamAaYTGToclChDoZu/XJjs5GScdd3Bhp7Y/V63lYOZnDPunczkydimRBohsvkGZvvqCN0hHw+WhMlyeclzp2fpaPMydHaOQ03OxaSyeasiTpV8T9c5oE2j0bQu2sA0wGdv3cW/vu0yR+d2VenmPzEZZ313sKE1lAuRAUV5mES6fIjM5RJcvbmHhw6PI2VhPsyxiRjxdI73vmwzQsA9TfZi7B6M8gh1ol+jWTloA9MAnUEfqx103INN8LJk45RSLsjA+DyFKrIJm4GxV5IlKuRgAK7e0sNoNMkxMw8E8MyIkdh/xfl9XLa+i7v3jza0NqfYPZguc7SBqtDTaDTLH21gmkyXysGUhMjG5lIkM3nW9yzAgzGryCZjKev4jE3IMlGmikyhqtUeHCqUKx8YmcHrFmzrj3DDRQM8d3qWo+OxhtbnhFQ2Z6kSWCEy7cFoNCsGbWCaTKXQz8lJw3NY13CIzG2FyIo8GFuSPJ7O0eYt/yPe2BNkTWdbkYF5ZnSGbf0RfB4X11+4GoB7nm6eF1Oa5If5hlij0SxftIFpMh63i/aAZ97GeUIZmK7GPZh0TnkwaTxmybQ9BxNPZwlW8GCEELx4Sw8PH5kgn5dIKXlmJMoF5rjnNZ1t7FzXyT37m5eHsYfI2s2eIi0Xo9GsHLSBWQK6Qz4mSzbOExPGcLG1XbWlYcrh99pDZGnrPvZZL4lM+SoyxTVbe5mOZ3hmdIazsynG59LsGCz0vrz2wtXsPxXllDlpc7GxezDKEEd1DkajWTFoA7MEdIV8ZT2Y1e2Bec2XTjFCZGaSfy7Nhp4QUAiRZXJ5MjlpKSmX48VbegD49dC4leC/YLDDel19PDwZn3/xImB4MIX1dTnUbdNoNMsD3cm/BHQHfZyeSRYdO7mACjLA1CIrlClvMIsFVIgsUUZJuZT+9gDnrQrz4NA4WTPc9qKBiPV6yG9cG0s3p/kxbZOKAbSiskazwtAezBLQGSzvwTSa4AczRJbNk0jnSGRyrO4I4HO7rCoyNQummoEBo6v/kWOTPHFymg09QSKBgkJB2G88f8ylqk/ObJRUNofPbmDatKKyRrOS0AZmCegOeYv6YJKZHKdnkgvyYJRUzIRZotwT8hEJeKw+mHi6eFxyJa7Z2ksyk+e+g2PsGCjWHguZBibWJPmWVKbYg6lnOJtGo2l9ahoYIURICOEyPz5PCHGjEMKZEJcGMHILyUze8iqGp4yk+fqexhL8UMjBqA25O+QnEvAUQmTKg/FWj4JeubkblzBkYi4YLDYwYbOyq3QMwGKRyuWLPJi+iJ+zs6kidQGNRrN8ceLBPAAEhBBrgF8A/xX4j2YuaqWhPJUnTk4DhR6YhXowubzk7IzhwXSHfIQDHquKLJExjEItD6Y94GXnuk6gOMEPEPKpENniGxgppZmDKaxvVXuAdDavZfs1mhWCEwMjpJRx4I3A/5ZSvgHY0dxlrSyu3d5PyOfmO48NA7YemIUk+c0n/9Go4Q31hn1E/F7LGMQd5mAAXmp29Zd6MG6XoM3rbkqITPXw2ENkq9sN+Z3SgogXKt99bJgTE82p4NNolgJHBkYIcTXwu8D/bx7T1Wd1EPR5eN3FA9yzf5R4OsuJyTgBr4u+OmfA2FEb86lpYzPuNnMwsyU5mHJy/aW852Wb+eq7dtPfPl9fLeT3NKWKTJVYFxmYDuP7cUYbGJKZHB/61pN8fc/xc70UjaZhnBiYPwE+CnxPSnlACLEZuK+5y1p5vOnStcTSOX789GmrRHkhg7VU/8jpaAKf20XY7zFDZMU5mFohMoBIwMtLt/VVeM0zb1LmYqCaRO0GZpVp4LSBKeTpxmZTNc7UaFqXmp6IlPJ+4H4AM9k/LqX842YvbKVxxcZu1ncH+c5jw0zMpRuWiFGojXkkmqQ75EMIQXvAa8vBKAOzMGcz5G9uiMye5O+PmCGy6PLeVLO5PB73wgo0lXrC2Nzy/l5oXtg4qSL7hhCiXQgRAp4BDgoh/qz5S1tZuFyCN166hocOT3BkLLag/AtgJcdHowlrJEDY72EulUVKWVcOphohn4dYE/pgUqYBtCf5fR4XveH5TanLiW/vO8lln/j5gvt5TpkezPicLtvWLF+cPGbtkFLOADcDdwPrgbc7ubkQ4nohxEEhxJAQ4rYyr/uFEN80X98jhNhoe+2j5vGDQojXlFznFkI8LoT4ke3YJvMeh8x71j+HuMm86dK1SGk8vS+kggwKHsyZaIqesPGlRgIe8hJi6RwJM2/iJAdTDWW0FptySX4wvJjlHCL7+p4TRBMZfvrMmQXd59S0kdwf1x6MZhnjxMB4zb6Xm4EfSCkzQM1GBSGEG/gccANG1dlbhRCl1WfvAqaklFuB24FPmdfuAG4FLgCuBz5v3k/xAeDZknt9CrhdSrkNmDLv3VKs6w5y5aZuYGElylAILaVzecuDUV34c8ks8XQOj0sUhaAaIRxojoFROZjS9a3uCHA6ujwNzNHxmFWKvtBhbcqDmYylyed1X5BmeeJk9/lX4BgQAh4QQmwAZhxctxsYklIekVKmgTuAm0rOuQn4ivnxncB1wsh83wTcIaVMSSmPAkPm/RBCrAVeB/y7uol5zbXmPTDvebODNS45b929HiHgvFWR2idXobQDHgqNkbPJjDELZoHhMTCryJphYKwqsuI1rmp37sFIKZum9NwIP3jiFELAjTsHeXBonOgC+nnU15XLSy0Aqlm21DQwUsrPSinXSClfKw2OA690cO81wEnb58PmsbLnSCmzQBToqXHtPwIfBvK213uAafMeld4LACHEe4UQ+4QQ+8bGxhx8GYvLTbsGefAj1zY8yVJhVyHuCRVCZACzqSzJKuOS66FpIbJsBQ+mPcBELG0JeVbjwaEJXvqpezkyNrfo66sXKSXff/wUV23q4fdesolMTvKzZxsPk52aSljhzQktn6NZpjhJ8ncIIf5BbcpCiL/H8GZqXlrmWKmvX+mcsseFEL8NnJVSPtrAexkHpfyClPJyKeXlfX3lS3ObiRCCwc7GJWIUxR6M0T/SbnkwRohsoRVkYCT5U9m8pba8WCgDUpqDUb0wSqGgGiPTCfLSGPV8rnlyOMqxiTg3XzLIzrUdrOls454Gw2SZXJ7TM0kuWmsoK4y3SKlyNpfnw3c+yaEzs+d6KZplgpMQ2ZeAWeAt5r8Z4MsOrhsG1tk+XwuMVDpHCOEBOoDJKtdeA9wohDiGEXK7VgjxNWAc6DTvUem9VhRlQ2R+IwejQmSNzpqxo8Jui11JpjwYf8lI53p6YZRnNXT23Hsw33/8lDlqegAhBDdcuJpfHRq31K3r4XQ0SV7CTtPAtEqp8mg0ybf2DfPtR4fP9VI0ywQnBmaLlPIvzFzKESnlx4DNDq57BNhmVnf5MJL2d5WccxfwDvPjNwP3SkPp8C7gVrPKbBOwDdgrpfyolHKtlHKjeb97pZRvM6+5z7wH5j1/4GCNyxZ7aMleRQZGkj+RyS5SiMy4x2xqcfXBVA7G556f5AdncjHKwBw+xyGyTC7PD58c4brt/XS0GUb+tRcPkM7l+UUDYTKVf1Eaca1Sqqymje47NnmOV6JZLjgxMAkhxEvUJ0KIa4CamVUzH/J+4CcYFV/fMpUAPi6EuNE87YtAjxBiCPgQcJt57QHgWxh9Nz8G/khKWesR+iPAh8x79Zj3XrHYk+PdpTmYZJZEenFyMAXJ/mZ5MMVrXG15MLWf2it5MFJKvvf4MMlMc+bYlPLroXEmYmluvqSQ9tu1tpOBjgB37z9d9/1UBdmLBtrxukXLlCorRYenT80s2fdWs7xxEqT/A+ArQogOjFzHJPBOJzeXUt6N0TtjP/bnto+TwC0Vrv0k8Mkq9/4l8Evb50cwK81eCNhDSyrJH/J5EKIQIutdgNaZIuRvjqKyysGUejAdbV58HpejEJna8I6Mx8jlJW6XkYp75NgUH/zmk3hcLl6/c3BR112OL/36KN0hH684v5DTc7kEN1w4wNf2HGculbWGtzlBeTBrOtvoCflbJgejVCLSuTz7T0W5YmN3w/f6wROnOD4R54+v27ZYy9O0IE6qyJ6QUu4ELgYuklJeIqV8svlL01RD5WDcLkMiBoxNLezzMJvKklikKrJIk4aOpSrkYIQQrG531guj1pTO5hmeKqgOP35iClgaTbN9xyb51aFx3vfyzfNKrndv6iadzXNsPFbXPU9NJegN+wl43fRGfC3nwQDsOza1oHt97r4h/vmXh8npHp8VTcXHKiHEhyocB0BK+Q9NWpPGAerJvyvow+UqFNEpwcvF7IOBZngw5Tv5wQiTOc3BuF2CXF5yeGyODT1GceOTw0az49klePK//efP0xv28/arNs57bVV7QR36wjUd816vxKnpBGu6jErD3rC/ZXIwyoPpDvl49PgksKWh+4xMJ3j+jBHWPDYRY0tfeLGWqGkxqnkwkRr/NOcQIQR+U7vLTsQcOpZI52pOs3RCuMkGpjREBrCqo7jZUkpZNuY/l8xaDav2PMyTJ6MAnG2yB/ObIxM8ODTBH7xiS1ljriri6jV0p6YTrO20G5jW8GBmTA/m5ef18ejxqYYnj97/fKH/7JkWKDHXNI+KO5BZLaZpYXwel5XgV0QCXiPJv0ghspCDEJk9/+GUdNYYl1xuZMHqdj8/jSaRUiKE4CsPHeOz9w6x979dV6RSPJvKsqazjbHZpGVgzs4ml0SJWErJP/zsefojfn73yvVlz+mL1D/fJp831AlevWMVYBiYibm09b04l8wmM7R53Vy1uZvvPX6Kw2MxtvbX733cf3CMVe3G1/XM6MyS5Mk054aFCVVpzil+j3uegQn7PUzG0uTycpFCZMY9KhmYk5NxrvyrX/DNR07Udd9UNoe/gqT9qvYAqWzeKov91r5hJmNp63NFLJUlEvCwpS9sGZinTO+lL+J31KzZKA8fnmDv0Un+8BVbKvYbed0uekK+ujyY8ViKdDbPGsuD8ZHO5ZlJLL6aQr3MJo3v92UbjOS+ESarj0wuz4ND41y7vZ+t/WHtwaxwtIFZxrzj6g3cvKtYEScS8Fgb2kKVlMEwYj63i9kKBuav73mW8bkUn/7p83WVrqay+XkJfoW9F+bYeIxnRo1NaKZk8JmqztraH+bwWAwpJU8OT+N2CV5xXl9TczA/fGqE9oCHW3eX914U/e2BukJ1qkRZGRjlBbVCs6UyMFv6QnQFvQ0l+h8/Mc1sKsvLz+tjx2C79bPVrEy0gVnG/L/XbeNVZihFEQl4mTS1qxYjRAaVh4795sgEd+8/zbXb+xmbTfG13xTG+yYzOX7/q/v4wROnyt4znc3Pq7pSqF6Y09Ekdz9dkFuZKfGeq9Z7AAAgAElEQVRg5pJZwqYHE01kGJ9L88TJac5fFWF9d5BoItO0fo2R6SQbe0M11RL6I35HPT0Kq0TZluSH1pDtn0lmiAS8CCG4bEMXjx6v38Dc//xZ3C7Bi7f2smOgnbHZFGdnFydXNpvM8JrbH2hoXZrm4ESL7ENl/r1LCLFrKRaoqQ/VbAkLHzamMBSVizfqXF7y8R8+w2BHgM/9l0u5enMP/3L/ERLpHFJK/uf3n+YnB87wwPPjZe+ZMnMw5bDLxdy9f9TyxOwhslQ2RzqXtzwYgENnZ3ny5DQ713XSb1ZwNWvk8Olo0jKE1VjV7q9rA7U8mJY0MFnr9+uyDd0cGY8xUee6fnlwjMvWd9Ee8LJjsB2AZ0cXR9vs0Nk5Dp6Z5YHnl17EVlMeJx7M5cD7MNSJ1wDvBV4B/JsQ4sPNW5qmESK2hr7FELuE8orK3953kmdGZ/jIDdtp87n54KvPY3wuxdf3HOcbe0/w7UeHEYKKUvPpbK5siTJgGYe9R6d4+tQMr7t4AKBI10sZPLuB+cWzZ5lJZtm1rsMav9ys0NJoNGGF8qqxqj3A2GzKcb/HqekEkYDH6m1SVYKt0Gw5m8zQbkrhXL6xC6Aub+HsbJIDIzO83GxI3TFgGJjFysOMThuG/NDZxRXj/PdfHeGdX967qPd8oeBkB+oBLpVSzgEIIf4CY+7Ky4BHgb9t3vI09RIO2A3M4ngw4ZKZMIl0jk//9CCXbejiRrMCaPembq7Z2sP/vneIeNqIsWdy+YoGppoHo4oXfvikoVf61t3ruPPR4aJE95yZjwn7PQx0BAj53FY4bue6TrI5Y0NvRqI/ns4yk8w6MjD9ET95CRNzKfodeDynphJW/gWMPie3S7REL8xsMmspdl9k9vUcPD3Lb12w2tH1vzK92ZefZxiYzqCPNZ1ti5aHGY0a3t+hM4urTbf/VJQHh8bJ52VRz5mmNk48mPWA/bc7A2yQUiaAc/9YpSlCTbUEFkVNGYwQmd2DOXR2lvG5NO96yaai0tk/edV5RBMZVncE+Mytu+gN+5mqMMvEyMFU/vVb1R4gncuzc20HOwaMzcweIlPimyG/ByEEW/rDjM+lCfrcbOuP0K+S44sU37ejVAYGnBiYOnthTk0nWNtVMDAul6A71Jxu/sdOTHHrFx62cna1mDVzMGD8brV53XWpRf/q0Bi9Yb/luYCht/bMSLS+hVdgxPRgjo7HLK27xSCWypHJyZYIUy43nBiYbwC/EUL8hem9PAj8pxAihCFGqWkhIk3yYOwGRm2wa0rm2lyxsZvP3LqLr73rSjqDPrqCXqYqTHVMVUnyg9ELA3DDRQMEvC68blG0mSkPRn29qhv8wjUduF2CnrAfl2hON7/6+lc5ysE4Hz8A8z0YaF6z5XcfG+Y3Ryb5q7tLp4/PJ53Nk8zki0KwEVM1wimj0SSb+0JFXsCOwXaOjMeIpxdehq08mGxecnyiPnmeaqi1tdL01OWCEy2y/wW8B5jGmDj5Pinlx6WUMSnl7zZ7gZr6KM7BNKeK7Iy5aZcLEd20a40l2dIV8hFNZMoOK0tlcxVDZPZ7v+4iY75KR5u3qIosli6EyAArD7PLlLhXRmahSf6/uvtZ/uae54qOjVoeTO3Bcf1Ws2XtdZycjDObyrKuu3jaaW/Yx1gTQmS/PjSOz+3izkeHefjwRNVzlUyM/QGmXgOTzObnedU7BtqR0gi1LZSRaNLqC3t+EcNk6nd/1IE+nqYYJ1VknwH8UsrPSCn/UUq5bwnWpWkQe4hssarIwn5vURXZmWgSt0vUVGvuChp/7NOJ+V5MrRDZrVes57+9dru12bYHvMUhMpWDKfFgLl5b0PzqC/sX5MEcOjPLv/3qiJULUiidNCdVZKqPxUkl2VceOobbJbjhooHie4QXX1H55GScYxNxPvjq81jfHeS/f29/1THV6vutkvxg/K7VEyJLZXIESn7mF5iVZIuRhxmdTnDN1l6EWNxEfyxtfF9GtAdTN05CZI8B/0MIMSSE+DshxOXNXpSmcYpDZItVReZmLpUlb1ZCnZ5J0hf215SH6TKfJqfLJPqrJfnBSNS/92UFMcVIm7eo0VKF7JQH84rz+/jI9dt51YsKfUH9ZUqE9x6dnKcIUInP3juElDASTRT105yOJulo8zoy4F63oRdXy4OJJjL8594TvP7igfkhsogRImtU+6scvx4yEu6v3tHP/7r5Qo6Mx/jnXx6ueP6sFZK0G5g6PZjM/Cmra7vaiPg9C64ky+TyjM2l2NQbYl1XkEOLOOU0bv6uqRyPxjlOQmRfkVK+FmPWyvPAp4QQh5q+Mk1D2KvIFqOTHwp6ZHFzkz0zk2SVgwR3t+nBTMbq92BKKQ2R2avIwEg6/0GJbEt/iVzMVCzNrV94mPd8ZR+ZMmE7O4fOzPKjp0bY3BtCSjgxWRgHMBpNOkrwK/oitbv5/3PvCWLpHO9+6fxhsT0hH6lsflEFR389NM6qdj9b+sK8/Lw+btw5yOfvO1zR0yoXImsPeK3jTkhm8gTKjGd40SJ09J+ZSSIlDHYEOG9VmENnFt+DUTkejXPq6eTfCmwHNgLPVT9Vc65QG4DP46pbgLISpYKXRpNh7WFmnUHjabdclVKtJH8p7QFPcQ4mlUWI6nmm/kiACVOXDeDpkSh5CXuPTfLXd1f/Ff7svUMEvW7+/PU7AKMySXFmJumoRFlhNFtW9mDS2TxffvAo12ztKSvrX2i2XJw8TD4veWhonJds7bOqAN9y+TrSuTxHx8onx2dKiirUx/XlYOZ7MAAXDnbwzMjMgiq/rLxYZxvbVkU4Oh6r+BDx2V8c4r6DZx3fWyX5dYisfpzkYJTH8nHgAHCZlPL1TV+ZpiH8Hjc+j2vREvxQ2FTUE/TpGWdd7N3VQmSZ6kn+UtrbiuP9s6YOWTWF4b6In1xeWgbu6VPGU/KbLl3Llx48WlHGRnkv73jxRi5ZbzQU2g3MqMMufsWqSKBqFdkPnxzhzEyK95TxXsAIkQF1d81X4sDIDFPxDC/d1mt7D7Ohs4IRU55K+yKHyAB2b+oilc3zlDnHpxHU5j/YEWBbf5hMrnwlWTSe4fafP8+/PXDE0X3T2TwZs6dqRCf568bJX/hR4Gop5fVSyi9JKRv/LdAsCRG/h+AihcfAGMUMRlgqns4ym8w6ahpUSf7JMgYmnas/RBZNZKw8xFyy9hji/pIE+9MjUdZ1t/E3b7qI3Ru7+ch3nuK50/NDM/90n+G9vPulm+lo89Ib9llP9ulsnvG5VF0eTH+7kUMp180vpeTffnWE81dFrAbEUqxu/kUyML8aMqRUXry1xzrWEzKNWKz8eygPptjAeElkcjXDjaBm+uTnJfkBdm8y1rHnaP3qzAq7B6NmBJVruNxzdAIpjR6gakUNCuW9dAa9jM2mHF2jKeAkB/MvQE4IsVsI8TL1bwnWpmmQSMCzaBVkUBwiUz0gTp7g23xuAl4X0yW9MPm8JJOT9XkwAS+ZnLFJgVGmXNPAtCsDY2yaB05FuXCwA6/bxT/97iW0ed189hfF6cSZZIZ79p/mlsvXWR7Yxp4QR82nYWWs6snB9LcHrG7+Ug6MzPDc6Vn+6zUbK3pjfWGlqLw4IbIHh8bZvjpiyekAdAW9CFHbgwmXhMigkA+rRmFE9vzfy+6Qj/NXRfjNkeql0tUYNSV2wn5D/FSI8qXKD5vvkczkeWq4doOn8tq3mWXwZ6K62bIenITI3g08APwE+Jj5/182d1mahRBeZANjn2qpqqGcPsF3B33zcjDpnBqXXEcOps1YgwqTzZpKytWw9MhmUswkMxybiFs5jv5IgJt2reHnz54tCr399MAZ0rk8N+0qDMHa1BuyQmT1NFkW1lG5F0Y9tSt9rnJ0h3zG5r8IpcrJTI5Hjk3xkq29Rcc9bhddQV/FMNxsMkvI5y7K66mKMidhspT5YFBJXeKqzd3sOzblyBsqx0g0yaDZl9Tmc5uVZPMT/Q8fnrBKo/c4MGhxM8G/td/wilql2fILDxzm9p89f66XURMnj5AfAK4AjkspXwlcAmi50hZmXVfQUROgU9RGHktnrVyC0w22K+SbJxejNpt6Q2RQkItRs2CqYZ+lcsDMv9iT6DftGiSdzfPjp09bx3745Ahru9qshk2ATX0hxmZTzCYzdTVZKgqjk+fH8PcenWB9d/Wfl8ccXFbPZMxK7D06STqb5yXbeue91hPyMVHFg7GXKEPhwcNJL0zSDC2VVpEprtzcQyKTc+RVlGM0mmCgs/A7ua0/PC9ENhlL89zpWW64cDXbV0f4zZHaIblYiQfTKpVk3398hHtsoyxaFSd/4UkpZRJACOGXUj4HnN/cZWkWwqdv2cntv7N40xTUVMu5VK7QZOjQg+kK+uYJXqo4dr0hMijMhHGSgwl43cYAtpkkB0y9K/X0CkbX/4aeoJXsn4yl+fXQOK/fOVgUrtpkKhMcn4hbm3y9VWQw34PJ5yV7j06ye1N3zXts6g1xpEyF18OHJ9h3zHnu4rETUwhB2fesJkkza5PqVyjhSycejOojClTwWtV69hxtLEw2Op0sMtLbVkU4Mj5XpCKhPJart/Rw1eYeHj0+VbNyTXkwWywDc+4T/VIaBQyVZJhaCSd/4cNCiE7g+8DPhBA/AEZqXKM5h4T8npqbbz1YIbKkkYMJ13H/rpBv3h+CFY+vs4oMCk/LMQceDJi9MLMp9p+KMtARKFIfEEJw0641PHR4gjMzSe55epRcXvL6i4tnxG/qMwzMkfEYo9EkQZ/b2lyd0Bv2I8R8PbKhsTmm4hlHBmZLX5jDY/NzCn/+g6f563ucdw0cn4gz2NFWtgm3J+xjooLw5YxNql9RCJE58GBqhMh6w3629YfZ48CrmH/vHBOxdFFeTFWSHZso9C89fGSCoM/NxWs7uXJTN4lMjv2nqtcsKQ+mJ+SjO+RriRDZRCxNLJ1jOp5e1ObbZuAkyf8GKeW0lPIvgf8JfBG4udkL07QObV43LmH8sZ2ZSVpP5E7oCnrn5WCUganHgykNkc2maudgwMi1nJ1N8fSpKBcMzu8xuXnXIFLCXU+M8MMnR9jSF+JFA5GiczaaHszRsZg1aKxaeXQpXjPEVdoLo/IvVzowMFv7w0zE0kXhxlQ2x5HxmJUXcsLxiRjrS7TOFPV6MJFGPJgKITKAqzb3sO/YZFntumqUU7cuVJIV8jAPH57g8o3deN0uy6jXCpMpDyZkjoUYbQEDc9w0mpmctNbXqtQ1MllKeb+U8i4p5bkfTqFZMoQQlmT/6TqbDLuCPmaSxYKX6WwDSX5zM5tJZJFSMpfKFgl7VqK/3c/xiRhHxmPWDBM7m/vCXLy2g6/+5jh7jk7OC4+B8dS9prONYxOxur9+ax1luvn3Hp1kVbu/4oZvR4VohmxezJGxGLm85MxM0vFAs+MTcTb0lH+/npCP2WS2bCmuYWBKPRhlYJx4MMrAVP6ZX7m5m1g6x9N1ysaMmHmRQZvEztb+MBG/h3+5/zDJTI6x2RSHzs5x9WajJLon7Oe8VeGalWtKVDXkczPQ0dYScjH2/p5K85ZahboMjOaFixo6diaaZFXE+QbbHfIh5fxxxwD+Kk+zpVghskSGeDqHlIXy6Wr0hf2Mz6WREi5c0172nJt2reHEZBwp4bdLwmOKjb1By1toyMC0+zljS/JLKdl7dILdm3oceUNbTTHPwzaNLaVAnM1LR02Yc6ksE7E06ysYmEJD5/xNy0jyl3owzqvIklkVIqv8M7/S7Iept1xZTbK0ezBtPjeffstOnhyO8t+/97R1z6u3FHp/VB6mWuWaCpEF/R7WdAYsY3YuOW4L+5W2ALQa2sBoHBHye5hJZjg7m3KkQ6ZQcjH2PIzlwbid//p53YY6QTSRsf7oHYXIbOG8cjIsAK/fOYBLGNLxSva/FCPJPmfIxNRRoqxYFQkU6aKdmIxzZiblKDwGxuydgNfFkM3APGeTuHeSfFZPvhu6Q2Vf7zH7fsoZmJkyITKfx4Xf43KkkaY8mGpea1/Ez5a+kKPyYTuq8KS0Eu81F6zmA9dt4zuPDfM39zxH2O/hQluRx5Wbeoinc+w/VblyTamIB71uBjrbmE1my3psUjoz8ouBXRdPGxjNiiDk93BiMkE2L+vaYFWzot2VLzTd1ffr127Kw8+WKClXQ/XC9EX8FUur+yMB/sfrdnDbDdsr3mdTb5jZZJZsXtbVZKlYZXbzq1ChSmY7NTAul2Bzb7goRPb8mVl8ppF2YmBOmE++FUNkSvOspJs/mcmRzuaLuvgVhmR/PTmY6mHRqzb38Mix8l32lTyNkekEXcHy6tYfuG4br3rRKk5NJ9i9qRuP7aHmys1m5VqVPEw8nSXoc+NyCSsEV+57/eOnT7P7r37B3gWoETjl+ETMKlbRITLNiiDi93B03Njc6mkytORiYvMNjM9dXzNoe5uHmUR2npJyNVSTo/3JtRy/95JNvKyCVAvApt7Cpry6gR6jPtXNb34f9hydpDvkq+gxlWNLf7jIgzl4etZKVp92ELo5bj75VgyRhct7MNYsmDIeY3vA4ygHU2i0rL7lvGrHKuZSWe57rrjV7qcHTnPBX/yEr+85Pu8aQ926/M/E5RLc/js7edl5fbzl8rVFr/WG/WzsCVatJIulc1bF3aD5YFGukmx4KkEuL/nTbz9ZNJzPTj4vufPR4QVP7zw+EWfXOsMbLzdrqZXQBkbjiJDfbZWa1pXkLyN4mW7Qg1F6ZKWzYKqhQmSVwmNO2dRbMASNhMjWmE2AH/nOUzx0eJy9xya4YmNXXdVoW/vCnJpOkEjnmE1mODWd4OotPfjcLkYdNGEen4jTFfSW9USgoNpcGuopSPWX82CcCV4WGi2rP1S8dGsvfRE/33lsuOj4Fx44QiaX579/72k+8aNniooaRqYTDHZW/plEAl7+z+/t5voLB+a91hv2M1VmnIQinspafWADyoMpk+hXOcaTU/GKI6h/8dxZ/vTbT/KLZ50rOZei8mgXrzUagacrlJW3CtrAaBxhT6jXFSIrMxPGSvLXUaYMhRDZXB05mA09Id506doi6ZdGWNvVhseUSWkkyf+K8/r54+u28dRwlP/yb3s4OZmwRB6dsrU/jJRwZHyO583y2/NXRVjdEXBUqnxiMsb6nvL5FzBGHwS8rnmlyrNlpPoVEYczYZyGyDxuF2+4ZA33PXfWMnTPjMyw7/gUt12/nXe+eCP//uuj/P5X91nrrObB1EI9tFTC7sGsivhxifKy/dFEhs6gl/e8dDNf33OC+5+fL3by/ceNht56FKhLUXm0rf1hQj639mA0KwPlLbhEIZTihDafG7/HVdaDqacPBgqS/SpEFvGXfxK343W7+Pu37LS0pBrF63axrjuI1y2sZHg9uFyCD736PB667Vo+9aaLeM0Fq3jdRfOfqKuxpd8wDkNn56wE//mrDQPjLMkfZ0OVkmghBD0hf8UQ2YI8GBUic/Azf+Ola8jmpTWq+mt7juP3uPidK9bxlzdewMdvuoD7Do7xir/7JZ/5+SGiiUyRTEw91DIw8bShwQaG8VvVXr6SLJrI0NHm5UOvPo9t/WE+cudTRRI60USGnz17BqBiCM0JKo+2vjtIZxmVjFZDGxiNI5SB6Yv4ixKlTugO+crmYOrpgwFzM4gXPBgVulgqNveGWN0RwLWAQW4Br5vfuWI9//r2y+v2hDb1hnAJODwW4/nTs4R8btZ2tTHgwINJZ/OMTCfYWCH/ougN+xgvCbuojVIJjtpxbmByeFzC0e/O9tXtXDDYznceO8VMMsP3Hz/FjTsH6TS94f/n6o385E9exlWbu7n954bg42CDHkx7yaTUUuZSOYI2732ws61iiKyjzUvA6+bTt+zk9EySf7GNoL5n/6j1YLWQyaTHbIUaXSGvriLTrAxUiKyeBL+iVI+sES0yMBPKqay1ITgJkS0mt92wnb+/ZfE03urF73GzvjvIYdODOW91BCGEFSKrJhtyajpBXlI1RAZGJVl9ORinIbJ8zfCYnTddupb9p6J86p7niKdzvP3qDUWvb+0P8+/vuIJvvOdK3nzZ2qLhafXQ0eZlNpWt2KgaT2UJ2x5kBjqqezAAO9d1cvOuQb7466OWOOZ3Hz/F5t4QQZ97YR7MZIyekI9IwEtnm6/sML9WQhsYjSPCCzEwIW/5PpgGQmRSwpnZJD63q24PaKFsWxVxpBvWTLaalWQHz8yyfbUR9htoD5DO5cuOplZYPTAOPJjKIbLyHkwsnaupJGCMS3b+875x1yAel+Dre06wc22HldQu5cVbevn0LTutEut6UUahkpGM23IwYHowZYz5TDJTVDzx//3W+UgJt//seYan4uw9OskbLllDyO+x1AEa4fhE3KoC7AxqD0azQlAeTCMVVF1BX4mGVh6XwEqaO0V185+aSiy599IqbOkL8/zZWabjGc439bZU2XS1PIzq/q6WgwHTg4mlijbQmWQWISBcRiBTeTW1ho4lM7m6Hgh6w35eYc7IedtVG2qc3TjtJRp3pcRsORgwwr3pbH6eBthMolgMdF13kLdfvYE7Hx3m0z85CMDNl6wh7Pcwl2pcP8yeR+sMel/YSX4hxPVCiINCiCEhxG1lXvcLIb5pvr5HCLHR9tpHzeMHhRCvMY8FhBB7hRBPCiEOCCE+Zjv/P4QQR4UQT5j/zl0sYwWiPJhGKqi6Q76iscnpbB6fx1VXiS4UJPtHppNLnn9pFbaYlWQA5682entU42e1PMzxiThtXrc1I6cSPSEfmZxkJlEwGLPJDGGfp2zuSXk1tWbCpDL5ujwYgPe9fAvXbe/n9TsXVgFYjVIR1VLiJTmY7jJ9XVLKohCZ4v2v3ErI7+H7T4ywe2M367qDhPxu4g2GyFLZHCPRhBXm7AoaIbK8Qx26c0HTDIwQwg18DrgB2AG8VQixo+S0dwFTUsqtwO3Ap8xrdwC3AhcA1wOfN++XAq6VUu4EdgHXCyGust3vz6SUu8x/TzTra3shspAQWWfQRzSRscIoqWy+ofCWSjKPTCcIO6ggW4nYGzPPVyEy08DYe2FmzD4ZxYlJQ0W5llHvLdPNP5PIzpPqVzidCZPM5OrKwQBcvrGbL77zirqvq4dqBiadzZPO5Ys8GCV9ZA9NJTI5Mjk5z8B0hXz84Su2AvCGS9cAEPJ5Gk7yD08lkBKrUKMz6CMvK3/vc3m5oIKCxaCZHsxuYEhKecRUX74DuKnknJuAr5gf3wlcJ4y/gJuAO6SUKSnlUWAI2C0NVCuz1/zXuuZ7BbGxN0jI564oGFmN7qC3SPAyZXow9WLFyx0qKa9Etpiil30RvyXD0xP243GJom7+v7zrADf84wNWwt4eu69Godmy8IReTuhS4XQmjJGDaT2vs5qBSZhhMHsORn3P7R65urbUwAC86yWb+Ls3X8ybLjVUBBaSgymV+uk03286UT739o29J3j5397nWGm7GTTTwKwBTto+HzaPlT1HSpkFokBPtWuFEG4hxBPAWeBnUso9tvM+KYR4SghxuxCisayfpixru4Ic+Pj1bF9dv4HpKtEjS2VzdSf4gaIk6gs1B9PR5qUv4rcS/ABul2BVe6EXRkrJA8+PM5PM8umfPk8+LzkxWb0HRtFj9jjZmy3LzYJRKM+2tgdTf4hsKeiwVLrnr38uPb8c3vpdjjkzMD6Pi1suX2c9UIX8HktAs16OmYUa602x0q7QfCFZOycn4+ZwsnPnxTTzJ17OFy81pZXOqXitlDInpdwFrAV2CyEuNF//KLAduALoBj5SdlFCvFcIsU8IsW9sbH63rWbxUXpk6o/SCJE1YGBsf8BOpPpXKn/1hov40KvPKzpm7+Y/dHaO8bkU67uD3PHICe47eJZUNl+zggwKBsZeqjybypQtUQbbTJhUDQ8mk6s4LvlcUs2DiVv9VtVzMMo4lesTKiXsdzcctjo+ESfoc1uNzqovqFKpsnqfhZRFL5RmGphhYJ3t87XMH7VsnSOE8AAdwKSTa6WU08AvMXI0SClHzRBaCvgyRohuHlLKL0gpL5dSXt7XV1ncULN4WGEF84/SSPLXv9lE/B5UCmExR0IvN169YxWXrO8qOmY3MA8OjQPwL2+7jO6gjw/f+RRQuwcGjA1UCBgvCpFV9mCczoRpJAezFAS8LrxuUdbAxNQ0S1uIrL3NixDFm3o1D6aUkM/TcJL/xGScDT0hK49mhcgqeDDxFW5gHgG2CSE2CSF8GEn7u0rOuQt4h/nxm4F7pVEfeRdwq1lltgnYBuwVQvQJIToBhBBtwKuA58zPB8z/BcZI56eb+LVp6qA0MdqoB+NyCSv3UmnDe6EyYIbIpJQ8ODTB+u4gOwbb+dPXnG8pODsJkXncLrqCPiaKkvyZigKZTscmJzP5usVNlwIhREW5GLVBB21JfrdL0NnmdZyDKcXIweQaqvw6NZVgjW1qpxUZqOjB5Ir+Pxc07Sdu5lTeD/wEeBb4lpTygBDi40KIG83Tvgj0CCGGgA8Bt5nXHgC+BTwD/Bj4IyllDhgA7hNCPIVhwH4mpfyRea+vCyH2A/uBXuATzfraNPVRmhhNZXINJfmhECZ7IXsw5VjdESCRyTEZS7PnyATXbDWENN9y+TouGGzH4xKs6XImp9ITKjRbSimrejABrxuf21W7TLlFk/xQWS7G8mBKfte6Qr4iBeb6DIzxPYhn6t/0x+dSRWXmBW+qUpPoufdgmvpXKqW8G7i75Nif2z5OArdUuPaTwCdLjj0FXFLh/GsXul5Nc2jzGoKXT56c5ve/uo89Ryetqpp66WjzMjyVeEHnYMqh1IR//uwZZlNZrt5iSKe4XYLP/+6lPDs6g9ehhlxP2Gcl+ZOZPNm8rJiDAWd6ZMlMviVzMFBZ8FJt0MGSQWbdwWJtPXVtte+RQv3exlLZuh6Ssrk8k/E0fTahWbdL0B7wVszBKMNyLkuV9V+pppsTMx8AABaBSURBVOkIIegK+rjn6dNE/B4++KrzeNdLNzV0LxWqeaGWKVdCNcB+51FDEv7FttnzG3pCbHCQf1H0hP08OzID2HXIKn+/nRiYRKY+qZilpKPNW1ZmR1V7lT7MdAZ9DE8VxhbPJIwybrcDZQplVOZSWVbVscbJeBopobekUbarSjd/pST/2Zkkz4zOcMXG7qY/qOm/Us2S8O6XbmIqnubdL9lslXo2gqrUeaGWKVdCNVvuPTbJ9tURq5+lEXpDBQ/mWXMsQLXwTy3By0wuTy4vWzZE1tHm5eh4bN7xWJkcDEB3yMv+U/Yqsso5qlJUwUC9YavxWeP9Sn+uHUFfxTJlJWdT+l4PH5ngA3c8wc8/9PK6Jqo2gv4r1SwJ737p5kW5j9rodIismD5zGFZewtVb6htkVkpv2M9MMsszIzN84I7H2dgT5GXbKldc1vJgCsPGWteDKV9FpgxM+RyMlBIhRFmZmEoUQmT15WCUwS81MF3B8t4XFDyY2RIDU20E9mLTmj9xjaYC6klRJ/mL8bpdVgL4mi2NSdcrlDLx2764B49L8H9+70o6gtVzMNXELq1hYy3qwbQHjCR/aWVXPJ2jzeueF/rqDvpI5wqCl/UZGON7ULcHYxmYYu+/s628orKU0nqP0veaqTJ+YbHRBkazrFBVZLpMeT6rO9pwCdi9eWEjBVSzZSqT48vv3F1TYqZWiMzyYFo4yZ+Xhc59RSyVLSuq2lXSbNmQB1Nnd71lYEpyMJWmWqayeZS9LPWWZpNZPC6xJB6lNjCaZYXqqdEGZj6XrOvklef3O84HVGLHQDsbeoL889su46K1HTXPrxUiUwPmWrEPBmzd/CWeQOksGEWp9FE9Bsae5K+H8bk0Po9rXnFLV9DHbDJLNpcvOm6/f+l7zSYzZolz45NZnaL/SjXLitdfPIjP7bLKcjUF/vLGCxblPuu6g9z/Z690fH4k4GUunSWfl2Ul/Vs+RKb0yEq8sFgqOy/BD0aSH0o8mCohRDv2MuV6GJ9N0Rf2zzMK6oErmsgUDV2L27yW0veq1te02LTmI4VGU4GukI9bd68/18vQ2GgPeJC2ENNHv/sU395X0KotJPlb08BU0iOLp3Nli0m6LA2wDMlMjlQ27zhhHjS/B/V214/NpeblX6BgYEoryap7MNrAaDSaZYJdLubQmVn+c+9J7jt41nrd8mAaVG9oNgVF5fmbdDkPxp6DUV6P0xCZyyUI+uofOjY+ly5beq4EL6Mlkv0qx+MS5TyYDJElmqfUmj9xjUazbLDPhPn2o8NAsXxJq3swqrdqvgdTvtu+vc2LSxg5GGWUKg1kK0cjM2HG51JlDUyX8mBi88N7YFQElkvyaw9Go9EsC9RmNRXL8N3HDCWBIgOTbW0DUylEFkuVT/K7XcKq3qpHh0wR9nvqCpHl85LJWJreyPwQmRWuK7N2gFXt/gohMu3BaDSaZYDarH741IglyGjfrAtJ/tbcbsJ+Q+alnAdTrkwZDM9hKpZpyMCE/O66kvxT8TS5vCzrwXRYSuUlITLz/qsigXkGZqbKhNLFpjV/4hqNZtmgNqvvPjZMb9jHay9cXbThtXqITAhBe8Azb6plrEKZMhiew2QsbV1Tl4HxeeZt+tF4hkxJqbFCzeYpZ2AipnEs7YVRIbj+dj+xVBZjCorhDc2lskvSxQ/awGg0mgWiejOSmTxvvHQtvWE/sXTO2jBbvdES5svFZHJ50tk8oTJJfjDlYmwhsnpzMHFbDiaTy/PyT9/HdX9/P997fJhciaJAJZkYMIxjuW5+5cH0RwJk85JU1vhZzKWzSLk0XfygDYxGo1kg9s3qlsvWWmEbtfmqza1VGy1hvoFRMjDBCpJE3QvIwYT8nqLE+8Rcmul4hql4mg9+80le+5lfcdAUGYWCgekrk4MBo1R5noFJ5/C6hTWLSRkc1RCrQ2QajWZZEPC68LgEu9Z1sm1VxNps1aaXzOQQgoammC4V7fMMjLERV/VgzBxM0Od2PGsHIOx3F4XIzs4ao64/fctOPvvWSxiZTvCv9x+2Xh+brezBgBGumy4tU05lCfo888Q1laRPPR7XQtCd/BqNZkEIIfjj67ZxxUZDA620NyOZyeH3uJZEmqRR2tu8nJpKWJ+rJ/5Kqt1dQS/pXJ6R6URd3gsYORh7kl8ZkFXtAXat6+R7jw3zzOiM9fr4XBqvW1R8n86gl5HpZNGxOXOgWdjvtj4H7cFoNJplyB9ft80aE9BZUvabzORbNsGvKA2RFYaNVfZgAI5NxOs3MH4P8XTOUm8+axqYflPIcsdgO4fOzlm5q/G5FD2h+TIx1lrKCF7GUzlCfvc8cc3ZJVRSBm1gNBrNItMZnB8ia+UEPxgGZiaZsaqtKs2CUXSbXtrxiVjd4SZLst98j9IQ2AWDHeTykkNn5gCzybJC/gWgOzTfwMTSxSEyNU5BezAajWZZMy8Hk823bA+MoqPNSyYnSZhegxKLDFUqUzY9mHg6V7d6tdr0VSHB2dkkXUEvPjNHtWOgHYADI1Ggche/fS3JTL6oMq0QIitWb57RBkaj0SxnIgEvQhS6y5OZ3LIIkUEhrGd5MFUaLUuvdUrppj82m7KGxQGs7w4S9nusPMxEBR0yRXfJfBooEyJLFYfIFjrSwSnawGg0mkXF7RK0B7xE47Yk/zIzMMq7qOTBqPJf+7VOUfdUm/7Z2RT9kYD1ussleNFAhAMjM0gpaxsYNZ/Gpkc2l8oS8nkI+4qN2Wwyi9ctlqyiTxsYjUaz6HQGC0nzVCbfskrKCvVEr4aOqc2/kgfTHjAEL6EBA1PDgwEjD/Ps6AzT8QzpXL6sVL9ChesmYinrmCFz47GNaDYM5kwiY3qYS1PR19o/dY1GsyzpaPMWQmTZZRgiq5GDcbmEJTTZ0VZfPsO+6UspTQ+m2MDsGGgnns7x6PEpgHkGyE53yYRNde+g343H7cLvcdmqyJZOSRm0gdFoNE2gwyZfYuRgWnursWbCmEnweDpLwOvCXWZCp0JVyzmdZqmw50VmklnS2fw8A7Jj0Ej0P3BoDKjcZAkFAzNphsjS2TzpXN4Kj0UCHluIbOmELkEbGI1G0wQ6g75l1wcDxUn+St6LQm3s9SbMw7belDGzi7/UwGxbFcbjEtz/fG0D0x4wBC8nzRCZpUJgvo8hTVPwYJYqwQ/awGg0mibQaWtcXA59MJGAByFsSX4zxFSNQoiscQ9GNVmWGhi/x83W/jDHJ+IAVXMwQghT3dlY+5ylQmCsP+TzFPXBaA9Go9Esa4wQWZp8Xi6LEJnLJYj4PcwkMgydneXhIxN0hyp7DVDwYOo1MEGvkm/JWU2W9ioyxQWDHYBRlaeMWSV6Qj6mzDLlggqBYUiMAWf2EJn2YDQazTKmM+glLw15eKPRsrU9GDD0yH5zZII3fP4hMjnJx268oOr5nQ16MC6XIOgzho6NVfBgoJCH6Q75cFXJBQF0hbxWH0xsXojMrZP8Go1m5aA23alYmnQ23/J9MGCs+bnTs6zpbOMH77+GXes6q56/fXWE3rDPMjT1oPIiZ2dT+D2usgPALjANTLX8i6I75GMyrjwYpQRtz8EY2mdz6aUblwxaTVmj0TQBtemqHEOrh8gAXnZeH5v7wvz1Gy+yEvHVuGnXIDfuHKzpXZQj7PcQS+dImRVk5fpSXjSgDExtA9ZdNkTmtt5rLpW1ho0t1TRL0AZGo9E0AeXBnI4aVVKtnuQH+Mj12+s6XwhBo/2KIb8RIktlc/N6YBQdbV62r46wpS9c835qAFo+Lyt4MNklF7oEbWA0Gk0TUD0iZ2ZMA7MMQmRLSchneBXT8TSbekMVz/v2+662RDCr0RXykZdGFdz8HIwxHmDaDKHpJL9Go1nWdJZ6MMsgRLaU2HMw5SrIFJGAF78D789qtoynrRCZCvNFzP+VsddJfo1Gs6xRM1LOmDmYNu3BFBHye5iOZ5iOZ6rKwDil0M2fJpbK4hIFo648mdGoMjDag9FoNMuYgNdNwOviTFSHyMoR9rsZiRojmivlYOqhyybZr1QIVOGASvYrb3Ipk/zawGg0mqbQ2ebjtBmW8esQWREhnwdzeOaieDA9YSXZb3gwIVsVXHilejBCiOuFEAeFEENCiNvKvO4XQnzTfH2PEGKj7bWPmscPCiFeYx4LCCH2CiGeFEIcEEJ8zHb+JvMeh8x71l+crtFoFo3OoNcyMNqDKcZuAKrlYJyiPJiJWNpSUi59r9PRFZSDEUK4gc8BNwA7gLcKIXaUnPYuYEpKuRW4HfiUee0O4FbgAuB64PPm/VLAtVLKncAu4HohxFXmvT4F3C6l3AZMmffWaDTniI42L+lsHlgeZcpLSchmABbDgwl43QR9bsODSWeL+ngKHkwCn9u1pMa+mR7MbmBISnlESpkG7gBuKjnnJuAr5sd3AtcJI3B4E3CHlDIlpTwKDAG7pcGceb7X/CfNa64174F5z5ub9YVpNJra2CVUdBVZMcqrEKIQ3looqps/lsoS9M33YEajySX1XqC5BmYNcNL2+bB5rOw5UsosEAV6ql0rhHALIZ4AzgI/k1LuMa+ZNu9R6b00Gs0S0hm0GxjtwdhRXkV30IfXvTjbcHfIx2QszVwqV+TBKG8pns6tKANTrsdVOjyn4rVSypyUchewFtgthLjQ4XsZbyjEe4UQ+4QQ+8bGxiouXqPRLAy7Rpc2MMWoLvvFCI8puoKGXIwal6ywG5ulTPBDcw3MMLDO9vlaYKTSOUIID9ABTDq5Vko5DfwSI0fzf9u7/yCtqjqO4+/P7rK07ArbKhoBKhSjUBNg6FhWY+ofak7whw6UFjk1/UOTNjX5Y/oxOflHM/2eHNMRFIvxR4TJOE6/kKH8QwXBTMAmox9SpjgqBY4h+O2Pe+7y7LIr7vKcvXifz2uG2ece7r3POXN2n+9zftxzngd60z2Ge6/yupsjYkFELJg8efLIS2Vmb4i7yIZXBoBmBpiBXWQHg0rXuHbK5dLq1ILZCMxKs7s6KQbt1w46Zy2wNL2+GHggIiKlL0mzzGYAs4BHJE2W1AsgqQs4D3gyXbM+3YN0z3szls3MDmNAgPEg/wBlt1UzZpCV+ro7eWFPMYusp2ESgaT+gDbWASbbu0XEfkmfA34FtAMrImKrpOuATRGxFlgO/ETSUxQtlyXp2q2S7ga2AfuBZRFxQNIUYGWaUdYG3B0R96W3vAq4U9I3gS3p3mZWkXIMprO9bVQrDtdZrhbM3n0DNxsr9YzvSHvBjG0XWdZwFhH3A/cPSvtaw+tXgEuGufZ64PpBaY8D84c5fwfFzDUzOwr0dhVjMH7I8lBl627KpOa1YBp3vezuHPjRXgaciXUKMGbWusoWjAf4D3Vcz3hu/dTpnDGjr2n3LNcjg0NbMLXrIjOz1lZ+S/cA/9A+fOrxTb3fwAAzMKiXYzJ1GuQ3sxY2qWzBeIB/TPR1H+z+OqSLrLOaLjIHGDPL4pjxHbS3yV1kY6Sv++CEgaEG+cEtGDOrCUlM6hrnLrIxMqlrXP8WzoO7yA6OwbgFY2Y10ds1zi2YMdLepv6dRD3Ib2a1d96cEwYMPltefd2dvPjyqwOWh4GDgcUBxsxq49oLZ1edhZbS193JX3btHbCaMhQPdHa0iWO7m/dg5xvhAGNmVhPlw5YTBs0iWzRvKnOn9fbP7BsrHoMxM6uJY3s66RrXTvugpXk6O9o45W3HjHl+3IIxM6uJxaefyOwpE6vORj8HGDOzmpg3vZd503urzkY/d5GZmVkWDjBmZpaFA4yZmWXhAGNmZlk4wJiZWRYOMGZmloUDjJmZZeEAY2ZmWSgiqs5DZSTtAv4+ysuPA55vYnbeLFqx3K1YZmjNcrvMb8xJETH5cCe1dIA5EpI2RcSCqvMx1lqx3K1YZmjNcrvMzeUuMjMzy8IBxszMsnCAGb2bq85ARVqx3K1YZmjNcrvMTeQxGDMzy8ItGDMzy8IBZhQknS/pT5KeknR11fnJQdJ0SeslbZe0VdIVKb1P0m8k/Tn9fGvVeW02Se2Stki6Lx3PkPRwKvNdkjqrzmOzSeqVtFrSk6nO31f3upb0hfS7/YSkOyS9pY51LWmFpOckPdGQNmTdqvDD9Nn2uKTTjuS9HWBGSFI7cANwATAH+JikOdXmKov9wBcjYjZwJrAslfNqYF1EzALWpeO6uQLY3nD8LeB7qcwvAp+uJFd5/QD4ZUScCsylKH9t61rSVODzwIKIeDfQDiyhnnV9G3D+oLTh6vYCYFb691ngxiN5YweYkTsDeCoidkTEPuBOYGHFeWq6iHgmIjan1/+l+MCZSlHWlem0lcCianKYh6RpwEeAW9KxgHOA1emUOpZ5IvAhYDlAROyLiJeoeV1T7OjbJakDmAA8Qw3rOiJ+B7wwKHm4ul0I3B6Fh4BeSVNG+94OMCM3FXi64XhnSqstSScD84GHgRMi4hkoghBwfHU5y+L7wJeB19LxscBLEbE/HdexvmcCu4BbU9fgLZK6qXFdR8Q/gW8D/6AILLuBR6l/XZeGq9umfr45wIychkir7VQ8ST3Az4ErI+I/VecnJ0kXAc9FxKONyUOcWrf67gBOA26MiPnAXmrUHTaUNOawEJgBvB3opugeGqxudX04Tf19d4AZuZ3A9IbjacC/KspLVpLGUQSXVRGxJiU/WzaZ08/nqspfBmcBH5X0N4quz3MoWjS9qRsF6lnfO4GdEfFwOl5NEXDqXNfnAX+NiF0R8SqwBng/9a/r0nB129TPNweYkdsIzEqzTTopBgbXVpynpktjD8uB7RHx3Yb/WgssTa+XAveOdd5yiYhrImJaRJxMUa8PRMSlwHrg4nRarcoMEBH/Bp6WdEpKOhfYRo3rmqJr7ExJE9LvelnmWtd1g+Hqdi3wyTSb7Exgd9mVNhp+0HIUJF1I8c22HVgREddXnKWmk/QB4PfAHzk4HnEtxTjM3cCJFH+kl0TE4AHENz1JZwNfioiLJM2kaNH0AVuAyyLif1Xmr9kkzaOY2NAJ7AAup/gCWtu6lvQNYDHFjMktwGcoxhtqVdeS7gDOplg1+Vng68AvGKJuU7D9EcWss5eByyNi06jf2wHGzMxycBeZmZll4QBjZmZZOMCYmVkWDjBmZpaFA4yZmWXhAGP2JiXp7HLFZ7OjkQOMmZll4QBjlpmkyyQ9IukxSTel/Wb2SPqOpM2S1kmanM6dJ+mhtBfHPQ37dLxT0m8l/SFd8450+56GfVxWpQflzI4KDjBmGUmaTfG0+FkRMQ84AFxKsbji5og4DdhA8XQ1wO3AVRHxHopVFMr0VcANETGXYs2scvmO+cCVFHsTzaRYT83sqNBx+FPM7AicC7wX2JgaF10UCwu+BtyVzvkpsEbSJKA3Ijak9JXAzyQdA0yNiHsAIuIVgHS/RyJiZzp+DDgZeDB/scwOzwHGLC8BKyPimgGJ0lcHnfd6aza9XrdX4zpZB/DftB1F3EVmltc64GJJx0P/XugnUfztlav2fhx4MCJ2Ay9K+mBK/wSwIe3Ds1PSonSP8ZImjGkpzEbB33bMMoqIbZK+AvxaUhvwKrCMYlOvd0l6lGI3xcXpkqXAj1MAKVc1hiLY3CTpunSPS8awGGaj4tWUzSogaU9E9FSdD7Oc3EVmZmZZuAVjZmZZuAVjZmZZOMCYmVkWDjBmZpaFA4yZmWXhAGNmZlk4wJiZWRb/B7mwyRi2xyROAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0a0b4f9240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#training function\n",
    "def train(model, num_epoch, num_iter, rec_interval, disp_interval):\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-6)\n",
    "    loss_values = []\n",
    "    avg_loss_values = []\n",
    "    rec_step = 0\n",
    "    print('Starting the training ...')\n",
    "    for eph in range(num_epoch):\n",
    "        print('epoch {} starting ...'.format(eph))\n",
    "        avg_loss = 0\n",
    "        n_samples = 0\n",
    "        for i in range(num_iter):\n",
    "            model.hidden = (model.hidden[0].detach(), model.hidden[1].detach())\n",
    "            model.zero_grad()\n",
    "            X,Y = next(ACTd)\n",
    "            n_samples += len(X)\n",
    "            X = autograd.Variable(torch.from_numpy(X).float().cuda())\n",
    "            X = X.view(len(X), 1, -1)\n",
    "            Y = autograd.Variable(torch.LongTensor(np.array([Y])).cuda())\n",
    "\n",
    "            y_hat = model(X)\n",
    "#             print(eph, i, y_hat)\n",
    "            loss = F.cross_entropy(y_hat, Y)\n",
    "            avg_loss += loss.data[0]\n",
    "            \n",
    "            if i % disp_interval == 0:\n",
    "                print('epoch: %d iterations: %d loss :%g' % (eph, i, loss.data[0]))\n",
    "            if rec_step%rec_interval==0:\n",
    "                loss_values.append(loss.data[0])\n",
    "            \n",
    "            loss.backward()     \n",
    "            optimizer.step()\n",
    "            rec_step += 1\n",
    "            \n",
    "        avg_loss /= n_samples\n",
    "        avg_loss_values.append(avg_loss)\n",
    "        #evaluating model accuracy\n",
    "        acc = evaluate_accuracy(model, test_split)\n",
    "        print('epoch: {} <====train track===> avg_loss: {}, accuracy: {}% \\n'.format(eph, avg_loss, acc))\n",
    "    return loss_values, avg_loss_values\n",
    "\n",
    "\n",
    "loss_vals, avg_loss_vals = train(model0, 100, 1000, 2, 100) #ran 4 times with 3e-5,1e-5, 1e-5, 1e-6\n",
    "plt.figure()\n",
    "plt.plot(loss_vals)\n",
    "plt.figure()\n",
    "plt.plot(avg_loss_vals)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('avg loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "def save_model(model_name, path, model):\n",
    "    p = path+'/'+model_name\n",
    "    print('saving at {}'.format(p))\n",
    "    torch.save(model.state_dict(), p)\n",
    "    print('saved at {}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving at ./checkpoints/LSTMClassifierX1_c7.pth\n",
      "saved at ./checkpoints/LSTMClassifierX1_c7.pth\n"
     ]
    }
   ],
   "source": [
    "save_model('LSTMClassifierX1_c7.pth', './checkpoints', model0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtest = LSTMClassifier(75, 512, 7, 1, 2, 3).cuda()\n",
    "mtest.load_state_dict(torch.load('./checkpoints/LSTMClassifierX1_c7.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "-1.1194e-02  2.1612e-02 -2.4612e-02  ...  -1.8261e-02 -3.4351e-02  2.7542e-02\n",
       " 2.1613e-02 -2.3839e-02  2.5151e-02  ...  -3.4706e-02 -1.7615e-02  1.7344e-02\n",
       "-2.4547e-02 -3.1302e-02 -4.1630e-02  ...   4.4822e-03  9.9910e-03 -2.3484e-02\n",
       "                ...                   â‹±                   ...                \n",
       "-4.9803e-03  4.4092e-02 -1.6954e-02  ...   2.3005e-02 -4.6648e-02  2.8076e-02\n",
       "-2.4357e-02  3.0116e-02  2.1181e-02  ...   1.3496e-02 -3.4021e-02 -5.4094e-03\n",
       "-1.2135e-02  4.3892e-02  3.9074e-02  ...  -5.0744e-03  5.1172e-02 -5.0351e-03\n",
       "[torch.cuda.FloatTensor of size 2048x75 (GPU 0)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtest.lstm.weight_ih_l0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
