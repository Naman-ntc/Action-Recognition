{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7aa8032850>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random, numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the datasets, i.e loading frames for few actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.382997</td>\n",
       "      <td>-0.419442</td>\n",
       "      <td>3.449989</td>\n",
       "      <td>-0.366909</td>\n",
       "      <td>-0.092619</td>\n",
       "      <td>3.443680</td>\n",
       "      <td>-0.353380</td>\n",
       "      <td>0.229542</td>\n",
       "      <td>3.427116</td>\n",
       "      <td>-0.391862</td>\n",
       "      <td>...</td>\n",
       "      <td>3.636719</td>\n",
       "      <td>-0.435790</td>\n",
       "      <td>-0.536338</td>\n",
       "      <td>3.280097</td>\n",
       "      <td>-0.364369</td>\n",
       "      <td>-0.491436</td>\n",
       "      <td>3.269750</td>\n",
       "      <td>1</td>\n",
       "      <td>72057594037944340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.383146</td>\n",
       "      <td>-0.419292</td>\n",
       "      <td>3.450006</td>\n",
       "      <td>-0.367569</td>\n",
       "      <td>-0.092003</td>\n",
       "      <td>3.443895</td>\n",
       "      <td>-0.353885</td>\n",
       "      <td>0.230300</td>\n",
       "      <td>3.427162</td>\n",
       "      <td>-0.391820</td>\n",
       "      <td>...</td>\n",
       "      <td>3.633053</td>\n",
       "      <td>-0.436031</td>\n",
       "      <td>-0.536649</td>\n",
       "      <td>3.281972</td>\n",
       "      <td>-0.358806</td>\n",
       "      <td>-0.471054</td>\n",
       "      <td>3.269975</td>\n",
       "      <td>1</td>\n",
       "      <td>72057594037944340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.385776</td>\n",
       "      <td>-0.421191</td>\n",
       "      <td>3.449611</td>\n",
       "      <td>-0.369506</td>\n",
       "      <td>-0.092775</td>\n",
       "      <td>3.443796</td>\n",
       "      <td>-0.354571</td>\n",
       "      <td>0.230189</td>\n",
       "      <td>3.426965</td>\n",
       "      <td>-0.403822</td>\n",
       "      <td>...</td>\n",
       "      <td>3.632370</td>\n",
       "      <td>-0.436489</td>\n",
       "      <td>-0.536484</td>\n",
       "      <td>3.286322</td>\n",
       "      <td>-0.358079</td>\n",
       "      <td>-0.470344</td>\n",
       "      <td>3.270202</td>\n",
       "      <td>1</td>\n",
       "      <td>72057594037944340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.385807</td>\n",
       "      <td>-0.421205</td>\n",
       "      <td>3.449582</td>\n",
       "      <td>-0.369576</td>\n",
       "      <td>-0.092714</td>\n",
       "      <td>3.443878</td>\n",
       "      <td>-0.354524</td>\n",
       "      <td>0.230369</td>\n",
       "      <td>3.427140</td>\n",
       "      <td>-0.403580</td>\n",
       "      <td>...</td>\n",
       "      <td>3.499778</td>\n",
       "      <td>-0.441701</td>\n",
       "      <td>-0.533234</td>\n",
       "      <td>3.278971</td>\n",
       "      <td>-0.360298</td>\n",
       "      <td>-0.476572</td>\n",
       "      <td>3.268953</td>\n",
       "      <td>1</td>\n",
       "      <td>72057594037944340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.357840</td>\n",
       "      <td>-0.420304</td>\n",
       "      <td>3.438846</td>\n",
       "      <td>-0.364956</td>\n",
       "      <td>-0.092426</td>\n",
       "      <td>3.442334</td>\n",
       "      <td>-0.354907</td>\n",
       "      <td>0.230391</td>\n",
       "      <td>3.427352</td>\n",
       "      <td>-0.405945</td>\n",
       "      <td>...</td>\n",
       "      <td>3.400878</td>\n",
       "      <td>-0.430001</td>\n",
       "      <td>-0.536492</td>\n",
       "      <td>3.278641</td>\n",
       "      <td>-0.358697</td>\n",
       "      <td>-0.471415</td>\n",
       "      <td>3.270685</td>\n",
       "      <td>1</td>\n",
       "      <td>72057594037944340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.382997 -0.419442  3.449989 -0.366909 -0.092619  3.443680 -0.353380   \n",
       "1 -0.383146 -0.419292  3.450006 -0.367569 -0.092003  3.443895 -0.353885   \n",
       "2 -0.385776 -0.421191  3.449611 -0.369506 -0.092775  3.443796 -0.354571   \n",
       "3 -0.385807 -0.421205  3.449582 -0.369576 -0.092714  3.443878 -0.354524   \n",
       "4 -0.357840 -0.420304  3.438846 -0.364956 -0.092426  3.442334 -0.354907   \n",
       "\n",
       "          7         8         9    ...           68        69        70  \\\n",
       "0  0.229542  3.427116 -0.391862    ...     3.636719 -0.435790 -0.536338   \n",
       "1  0.230300  3.427162 -0.391820    ...     3.633053 -0.436031 -0.536649   \n",
       "2  0.230189  3.426965 -0.403822    ...     3.632370 -0.436489 -0.536484   \n",
       "3  0.230369  3.427140 -0.403580    ...     3.499778 -0.441701 -0.533234   \n",
       "4  0.230391  3.427352 -0.405945    ...     3.400878 -0.430001 -0.536492   \n",
       "\n",
       "         71        72        73        74  label                 id  video_id  \n",
       "0  3.280097 -0.364369 -0.491436  3.269750      1  72057594037944340         0  \n",
       "1  3.281972 -0.358806 -0.471054  3.269975      1  72057594037944340         0  \n",
       "2  3.286322 -0.358079 -0.470344  3.270202      1  72057594037944340         0  \n",
       "3  3.278971 -0.360298 -0.476572  3.268953      1  72057594037944340         0  \n",
       "4  3.278641 -0.358697 -0.471415  3.270685      1  72057594037944340         0  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading and prepping data\n",
    "#initially only one action\n",
    "dframe = pd.read_csv('./csv_data/action_1.csv')\n",
    "dframe2 = pd.read_csv('./csv_data/action_2.csv')\n",
    "dframe3 = pd.read_csv('./csv_data/action_3.csv')\n",
    "dframe4 = pd.read_csv('./csv_data/action_4.csv')\n",
    "dframe5 = pd.read_csv('./csv_data/action_5.csv')\n",
    "dframe6 = pd.read_csv('./csv_data/action_6.csv')\n",
    "dframe7 = pd.read_csv('./csv_data/action_7.csv')\n",
    "\n",
    "#to look at data\n",
    "dframe.iloc[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some utility functions to split the datasets and loading the datasets in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ (array([[ 0.       ,  0.       ,  0.       , ...,  0.0186279, -0.0719937,\n",
       "        -0.180239 ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.0243399, -0.0517625,\n",
       "        -0.180031 ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.0276977, -0.0491529,\n",
       "        -0.179409 ],\n",
       "       ..., \n",
       "       [ 0.       ,  0.       ,  0.       , ..., -0.0563203, -0.0113986,\n",
       "        -0.174284 ],\n",
       "       [ 0.       ,  0.       ,  0.       , ..., -0.0561737, -0.0162162,\n",
       "        -0.171437 ],\n",
       "       [ 0.       ,  0.       ,  0.       , ..., -0.0559275, -0.0062211,\n",
       "        -0.172233 ]]), 0),\n",
       "       (array([[ 0.        ,  0.        ,  0.        , ...,  0.01849598,\n",
       "         0.0721854 , -0.16189   ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.10443107,\n",
       "         0.05172237, -0.133567  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.08676113,\n",
       "         0.05116256, -0.145192  ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.10355284,\n",
       "         0.52594266, -0.431462  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.09376174,\n",
       "         0.53507   , -0.434229  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.08766792,\n",
       "         0.541823  , -0.422078  ]]), 0),\n",
       "       (array([[ 0.        ,  0.        ,  0.        , ...,  0.10299776,\n",
       "        -0.001346  , -0.226027  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.10025404,\n",
       "        -0.0125163 , -0.230909  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.09391637,\n",
       "        -0.0240066 , -0.245108  ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.0862923 ,\n",
       "         0.0006496 , -0.251447  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.09493048,\n",
       "        -0.0351646 , -0.236531  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.11232334,\n",
       "        -0.0043682 , -0.221527  ]]), 0)], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making test and train split\n",
    "#the recentering has been done so that the pelvic joint is always at the origin\n",
    "#labels are to be zero indexed\n",
    "def train_test_split(dframe_list):\n",
    "    train_split = np.empty(0, dtype=object)\n",
    "    test_split = np.empty(0, dtype=object)\n",
    "    for dframe in dframe_list:\n",
    "        label = dframe.iloc[0,75]-1\n",
    "#         print(label)\n",
    "        num_samples = len(dframe.iloc[:,:])\n",
    "        video_ids = np.unique(dframe.iloc[:,-1].values)\n",
    "        train_video_ids = video_ids[:-15]\n",
    "        test_video_ids = video_ids[-15:]\n",
    "        train_split1 = np.empty(len(train_video_ids), dtype=object)\n",
    "        test_split1 = np.empty(len(test_video_ids), dtype=object)\n",
    "        for idx,i in enumerate(train_video_ids):\n",
    "            train_split1[idx] = dframe.loc[dframe['video_id'] == i].values[:,0:75]\n",
    "            for fidx, f in enumerate(train_split1[idx]):\n",
    "                f = np.reshape(f, (25,3))\n",
    "                f = f-f[0,:]\n",
    "                f = np.reshape(f, (1,75))\n",
    "                train_split1[idx][fidx] = f\n",
    "#             mean_vec = np.mean(train_split1[idx], axis=0)\n",
    "#             std_vec = np.std(train_split1[idx], axis=0)\n",
    "            train_split1[idx] = (train_split1[idx], label)\n",
    "\n",
    "        for idx,i in enumerate(test_video_ids):\n",
    "            test_split1[idx] = dframe.loc[dframe['video_id'] == i].values[:,0:75]\n",
    "            for fidx, f in enumerate(test_split1[idx]):\n",
    "                f = np.reshape(f, (25,3))\n",
    "                f = f-f[0,:]\n",
    "                f = np.reshape(f, (1,75))\n",
    "                test_split1[idx][fidx] = f\n",
    "#             mean_vec = np.mean(test_split1[idx], axis=0)\n",
    "#             std_vec = np.std(test_split1[idx], axis=0)\n",
    "            test_split1[idx] = (test_split1[idx], label)\n",
    "        train_split = np.concatenate((train_split, train_split1))\n",
    "        test_split = np.concatenate((test_split, test_split1))\n",
    "    return train_split, test_split\n",
    "\n",
    "train_split, test_split = train_test_split([dframe, dframe2, dframe3, dframe4, dframe5, dframe6, dframe7])\n",
    "\n",
    "# #looking at split\n",
    "train_split[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.        ,  0.        ,  0.        , ...,  0.16425381,\n",
       "         -0.10658009, -0.050106  ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.17513742,\n",
       "         -0.13062437, -0.038991  ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.16749166,\n",
       "         -0.12804461, -0.04732   ],\n",
       "        ..., \n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.28743288,\n",
       "         -0.14391777, -0.034796  ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.29094027,\n",
       "         -0.14585398, -0.043128  ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.29262793,\n",
       "         -0.13398748, -0.04623   ]]), 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEQ_LEN = None\n",
    "def Data_gen( train_split, SEQ_LEN):\n",
    "    while(True):\n",
    "        X = train_split\n",
    "        databatch = random.sample(list(X), 1)[0]\n",
    "#         print(databatch)\n",
    "        databatch, label = databatch[0], databatch[1]\n",
    "        if SEQ_LEN is not None:\n",
    "            if len(databatch) > SEQ_LEN:\n",
    "                databatch = databatch[0:SEQ_LEN]\n",
    "            elif len(databatch) < SEQ_LEN:\n",
    "                databatch = np.concatenate((databatch, np.zeros((SEQ_LEN - len(databatch), 75))))\n",
    "            else:\n",
    "                pass\n",
    "            yield databatch,label\n",
    "        else:\n",
    "            yield databatch,label\n",
    "\n",
    "ACTd = Data_gen(train_split, SEQ_LEN)\n",
    "\n",
    "#to look at batch created by Actd\n",
    "next(ACTd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Classifier model defination and intialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action LSTM\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, joints_dim, hidden_dim, label_size, batch_size, num_layers, kernel_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        joints_dim2d = joints_dim - 25\n",
    "        \n",
    "        self.lstm3 = nn.LSTM(joints_dim, hidden_dim, num_layers=self.num_layers)\n",
    "        \n",
    "        self.lstm2_1 = nn.LSTM(joints_dim2d, hidden_dim, num_layers=self.num_layers)\n",
    "        self.lstm2_2 = nn.LSTM(joints_dim2d, hidden_dim, num_layers=self.num_layers)\n",
    "        self.lstm2_3 = nn.LSTM(joints_dim2d, hidden_dim, num_layers=self.num_layers)\n",
    "        \n",
    "        self.conv1_1 = nn.Conv1d(4, 2, kernel_size, stride=1, padding=1) #for kernel size=3\n",
    "        self.conv1_2 = nn.Conv1d(2, 1, kernel_size, stride=1, padding=1) #for kernel size=3\n",
    "        \n",
    "        self.hidden3 = self.init_hidden3()\n",
    "        self.hidden2_1 = self.init_hidden2_1()\n",
    "        self.hidden2_2 = self.init_hidden2_2()\n",
    "        self.hidden2_3 = self.init_hidden2_3()\n",
    "        \n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "    \n",
    "    def init_hidden3(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()),\n",
    "                autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()))\n",
    "    def init_hidden2_1(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()),\n",
    "                autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()))\n",
    "    def init_hidden2_2(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()),\n",
    "                autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()))\n",
    "    def init_hidden2_3(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()),\n",
    "                autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()))\n",
    "    \n",
    "    \n",
    "    def forward(self, joints3d_vec):\n",
    "        x3 = joints3d_vec\n",
    "        x2 = x3.view(-1, 25, 3)\n",
    "        x2_1 = x2[:,:,1:3].contiguous().view(-1, 1, 50)\n",
    "        x2_2 = x2[:,:,0:2].contiguous().view(-1, 1, 50)\n",
    "        x2_3 = x2[:,:,[0,2]].contiguous().view(-1, 1, 50)\n",
    "#         print('x2_3 : ',x2_3.size())\n",
    "        lstm_out3, self.hidden3 = self.lstm3(x3, self.hidden3)\n",
    "        lstm_out2_1, self.hidden2_1 = self.lstm2_1(x2_1, self.hidden2_1)\n",
    "        lstm_out2_2, self.hidden2_2 = self.lstm2_2(x2_2, self.hidden2_2)\n",
    "        lstm_out2_3, self.hidden2_3 = self.lstm2_3(x2_3, self.hidden2_3)\n",
    "#         print('lstm_out[-1] : ', lstm_out[-1].size())\n",
    "        t3 = lstm_out3[-1]\n",
    "#         print('t3 : ', t3.size())\n",
    "        t2_1 = lstm_out2_1[-1]\n",
    "        t2_2 = lstm_out2_2[-1]\n",
    "        t2_3 = lstm_out2_3[-1]\n",
    "#         print('t2_3 : ', t2_3.size())\n",
    "        \n",
    "        t = autograd.Variable(torch.zeros(self.batch_size, 4, self.hidden_dim).cuda())\n",
    "        t[:,0,:] = t3\n",
    "        t[:,1,:] = t2_1\n",
    "        t[:,2,:] = t2_2\n",
    "        t[:,3,:] = t2_3\n",
    "#         print('t : ', t.size())\n",
    "        \n",
    "        y3 = self.conv1_1(t)\n",
    "#         print('y3 : ', y3.size())\n",
    "        y3 = self.conv1_2(y3)\n",
    "#         print('y3 : ', y3.size())\n",
    "        y3 = y3.contiguous().view(-1, self.hidden_dim)\n",
    "#         print('y3 : ', y3.size())\n",
    "        \n",
    "        y  = self.hidden2label(y3)\n",
    "        log_probs = F.softmax(y, dim=1)\n",
    "        return log_probs\n",
    "#instanstiating a model\n",
    "model0 = LSTMClassifier(75, 512, 7, 1, 2, 3)\n",
    "#to do stuff in CUDA\n",
    "model0 = model0.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.1440  0.1463  0.1520  0.0988  0.1719  0.1455  0.1415\n",
       "[torch.cuda.FloatTensor of size 1x7 (GPU 0)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt = autograd.Variable(torch.rand(23, 1, 75).cuda())\n",
    "model0(Xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, test_split):\n",
    "    pred_labels = np.empty(len(test_split))\n",
    "    orig_labels = np.array([t[1] for t in test_split])\n",
    "    for i in range(len(test_split)):\n",
    "        d_in = autograd.Variable(torch.from_numpy(test_split[i][0]).float().cuda())\n",
    "        d_in = d_in.view(d_in.size()[0], 1, -1)\n",
    "        y_pred = model(d_in)\n",
    "        pred_labels[i] = y_pred.data.cpu().max(1)[1].numpy()[0];\n",
    "    n_samples = len(pred_labels)\n",
    "    res=(orig_labels==pred_labels)\n",
    "    correct_count = (res==True).sum()\n",
    "    return (correct_count*100/n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training ...\n",
      "epoch 0 starting ...\n",
      "epoch: 0 iterations: 0 loss :1.93597\n",
      "epoch: 0 iterations: 100 loss :1.93202\n",
      "epoch: 0 iterations: 200 loss :1.94823\n",
      "epoch: 0 iterations: 300 loss :1.94279\n",
      "epoch: 0 iterations: 400 loss :1.94347\n",
      "epoch: 0 iterations: 500 loss :1.93467\n",
      "epoch: 0 iterations: 600 loss :1.95211\n",
      "epoch: 0 iterations: 700 loss :1.94033\n",
      "epoch: 0 iterations: 800 loss :1.95146\n",
      "epoch: 0 iterations: 900 loss :1.94029\n",
      "epoch: 0 <====train track===> avg_loss: 0.021895749097395112, accuracy: 14.285714285714286% \n",
      "\n",
      "epoch 1 starting ...\n",
      "epoch: 1 iterations: 0 loss :1.98569\n",
      "epoch: 1 iterations: 100 loss :1.9876\n",
      "epoch: 1 iterations: 200 loss :2.00073\n",
      "epoch: 1 iterations: 300 loss :1.98807\n",
      "epoch: 1 iterations: 400 loss :1.81125\n",
      "epoch: 1 iterations: 500 loss :1.79868\n",
      "epoch: 1 iterations: 600 loss :1.80799\n",
      "epoch: 1 iterations: 700 loss :1.74476\n",
      "epoch: 1 iterations: 800 loss :1.75382\n",
      "epoch: 1 iterations: 900 loss :1.94814\n",
      "epoch: 1 <====train track===> avg_loss: 0.021480201303349784, accuracy: 29.523809523809526% \n",
      "\n",
      "epoch 2 starting ...\n",
      "epoch: 2 iterations: 0 loss :1.7729\n",
      "epoch: 2 iterations: 100 loss :1.67273\n",
      "epoch: 2 iterations: 200 loss :1.78843\n",
      "epoch: 2 iterations: 300 loss :1.82482\n",
      "epoch: 2 iterations: 400 loss :1.96098\n",
      "epoch: 2 iterations: 500 loss :1.95298\n",
      "epoch: 2 iterations: 600 loss :1.87424\n",
      "epoch: 2 iterations: 700 loss :1.38585\n",
      "epoch: 2 iterations: 800 loss :2.06522\n",
      "epoch: 2 iterations: 900 loss :2.01103\n",
      "epoch: 2 <====train track===> avg_loss: 0.020588956982680016, accuracy: 30.476190476190474% \n",
      "\n",
      "epoch 3 starting ...\n",
      "epoch: 3 iterations: 0 loss :1.8525\n",
      "epoch: 3 iterations: 100 loss :1.58273\n",
      "epoch: 3 iterations: 200 loss :1.99432\n",
      "epoch: 3 iterations: 300 loss :1.93178\n",
      "epoch: 3 iterations: 400 loss :2.05183\n",
      "epoch: 3 iterations: 500 loss :1.90957\n",
      "epoch: 3 iterations: 600 loss :1.96411\n",
      "epoch: 3 iterations: 700 loss :2.04401\n",
      "epoch: 3 iterations: 800 loss :2.02246\n",
      "epoch: 3 iterations: 900 loss :1.4583\n",
      "epoch: 3 <====train track===> avg_loss: 0.020291537823301432, accuracy: 40.0% \n",
      "\n",
      "epoch 4 starting ...\n",
      "epoch: 4 iterations: 0 loss :1.41139\n",
      "epoch: 4 iterations: 100 loss :2.01202\n",
      "epoch: 4 iterations: 200 loss :1.8945\n",
      "epoch: 4 iterations: 300 loss :1.84424\n",
      "epoch: 4 iterations: 400 loss :1.79754\n",
      "epoch: 4 iterations: 500 loss :1.89455\n",
      "epoch: 4 iterations: 600 loss :1.98354\n",
      "epoch: 4 iterations: 700 loss :2.1226\n",
      "epoch: 4 iterations: 800 loss :1.88597\n",
      "epoch: 4 iterations: 900 loss :1.90492\n",
      "epoch: 4 <====train track===> avg_loss: 0.02023586666857136, accuracy: 37.142857142857146% \n",
      "\n",
      "epoch 5 starting ...\n",
      "epoch: 5 iterations: 0 loss :2.04829\n",
      "epoch: 5 iterations: 100 loss :1.42226\n",
      "epoch: 5 iterations: 200 loss :1.81482\n",
      "epoch: 5 iterations: 300 loss :1.81135\n",
      "epoch: 5 iterations: 400 loss :2.0711\n",
      "epoch: 5 iterations: 500 loss :2.03445\n",
      "epoch: 5 iterations: 600 loss :1.84964\n",
      "epoch: 5 iterations: 700 loss :2.07904\n",
      "epoch: 5 iterations: 800 loss :1.91608\n",
      "epoch: 5 iterations: 900 loss :1.45494\n",
      "epoch: 5 <====train track===> avg_loss: 0.020376174255204646, accuracy: 34.285714285714285% \n",
      "\n",
      "epoch 6 starting ...\n",
      "epoch: 6 iterations: 0 loss :1.95658\n",
      "epoch: 6 iterations: 100 loss :1.42013\n",
      "epoch: 6 iterations: 200 loss :1.73109\n",
      "epoch: 6 iterations: 300 loss :1.31608\n",
      "epoch: 6 iterations: 400 loss :1.64585\n",
      "epoch: 6 iterations: 500 loss :1.89144\n",
      "epoch: 6 iterations: 600 loss :2.00774\n",
      "epoch: 6 iterations: 700 loss :1.69454\n",
      "epoch: 6 iterations: 800 loss :1.84822\n",
      "epoch: 6 iterations: 900 loss :1.80589\n",
      "epoch: 6 <====train track===> avg_loss: 0.020152874440329264, accuracy: 44.76190476190476% \n",
      "\n",
      "epoch 7 starting ...\n",
      "epoch: 7 iterations: 0 loss :1.91619\n",
      "epoch: 7 iterations: 100 loss :1.70491\n",
      "epoch: 7 iterations: 200 loss :1.64367\n",
      "epoch: 7 iterations: 300 loss :1.94112\n",
      "epoch: 7 iterations: 400 loss :1.977\n",
      "epoch: 7 iterations: 500 loss :2.13771\n",
      "epoch: 7 iterations: 600 loss :1.77586\n",
      "epoch: 7 iterations: 700 loss :2.03926\n",
      "epoch: 7 iterations: 800 loss :1.59233\n",
      "epoch: 7 iterations: 900 loss :1.64096\n",
      "epoch: 7 <====train track===> avg_loss: 0.020427001812949947, accuracy: 40.95238095238095% \n",
      "\n",
      "epoch 8 starting ...\n",
      "epoch: 8 iterations: 0 loss :1.25414\n",
      "epoch: 8 iterations: 100 loss :1.49526\n",
      "epoch: 8 iterations: 200 loss :2.02406\n",
      "epoch: 8 iterations: 300 loss :2.01362\n",
      "epoch: 8 iterations: 400 loss :2.05605\n",
      "epoch: 8 iterations: 500 loss :1.85618\n",
      "epoch: 8 iterations: 600 loss :2.02224\n",
      "epoch: 8 iterations: 700 loss :1.87804\n",
      "epoch: 8 iterations: 800 loss :2.02303\n",
      "epoch: 8 iterations: 900 loss :1.90806\n",
      "epoch: 8 <====train track===> avg_loss: 0.019817639516387212, accuracy: 52.38095238095238% \n",
      "\n",
      "epoch 9 starting ...\n",
      "epoch: 9 iterations: 0 loss :1.88813\n",
      "epoch: 9 iterations: 100 loss :1.85119\n",
      "epoch: 9 iterations: 200 loss :1.50334\n",
      "epoch: 9 iterations: 300 loss :1.55419\n",
      "epoch: 9 iterations: 400 loss :1.24037\n",
      "epoch: 9 iterations: 500 loss :1.75948\n",
      "epoch: 9 iterations: 600 loss :1.97747\n",
      "epoch: 9 iterations: 700 loss :1.33174\n",
      "epoch: 9 iterations: 800 loss :1.35543\n",
      "epoch: 9 iterations: 900 loss :2.0246\n",
      "epoch: 9 <====train track===> avg_loss: 0.01962377559971254, accuracy: 48.57142857142857% \n",
      "\n",
      "epoch 10 starting ...\n",
      "epoch: 10 iterations: 0 loss :2.08416\n",
      "epoch: 10 iterations: 100 loss :1.19774\n",
      "epoch: 10 iterations: 200 loss :2.06237\n",
      "epoch: 10 iterations: 300 loss :2.01195\n",
      "epoch: 10 iterations: 400 loss :1.26399\n",
      "epoch: 10 iterations: 500 loss :2.06598\n",
      "epoch: 10 iterations: 600 loss :1.60084\n",
      "epoch: 10 iterations: 700 loss :1.65306\n",
      "epoch: 10 iterations: 800 loss :2.06934\n",
      "epoch: 10 iterations: 900 loss :2.09407\n",
      "epoch: 10 <====train track===> avg_loss: 0.019422958486136857, accuracy: 55.23809523809524% \n",
      "\n",
      "epoch 11 starting ...\n",
      "epoch: 11 iterations: 0 loss :1.81644\n",
      "epoch: 11 iterations: 100 loss :2.06271\n",
      "epoch: 11 iterations: 200 loss :1.52005\n",
      "epoch: 11 iterations: 300 loss :2.01376\n",
      "epoch: 11 iterations: 400 loss :1.60613\n",
      "epoch: 11 iterations: 500 loss :1.82289\n",
      "epoch: 11 iterations: 600 loss :2.06439\n",
      "epoch: 11 iterations: 700 loss :1.89386\n",
      "epoch: 11 iterations: 800 loss :1.78363\n",
      "epoch: 11 iterations: 900 loss :1.99026\n",
      "epoch: 11 <====train track===> avg_loss: 0.019036392318561128, accuracy: 48.57142857142857% \n",
      "\n",
      "epoch 12 starting ...\n",
      "epoch: 12 iterations: 0 loss :1.62114\n",
      "epoch: 12 iterations: 100 loss :1.84677\n",
      "epoch: 12 iterations: 200 loss :1.54041\n",
      "epoch: 12 iterations: 300 loss :1.20417\n",
      "epoch: 12 iterations: 400 loss :1.55762\n",
      "epoch: 12 iterations: 500 loss :1.97561\n",
      "epoch: 12 iterations: 600 loss :1.70257\n",
      "epoch: 12 iterations: 700 loss :2.05534\n",
      "epoch: 12 iterations: 800 loss :1.67102\n",
      "epoch: 12 iterations: 900 loss :1.90751\n",
      "epoch: 12 <====train track===> avg_loss: 0.01942434766442323, accuracy: 52.38095238095238% \n",
      "\n",
      "epoch 13 starting ...\n",
      "epoch: 13 iterations: 0 loss :1.3415\n",
      "epoch: 13 iterations: 100 loss :2.08043\n",
      "epoch: 13 iterations: 200 loss :2.0339\n",
      "epoch: 13 iterations: 300 loss :1.18959\n",
      "epoch: 13 iterations: 400 loss :2.03081\n",
      "epoch: 13 iterations: 500 loss :2.1036\n",
      "epoch: 13 iterations: 600 loss :1.16855\n",
      "epoch: 13 iterations: 700 loss :2.04381\n",
      "epoch: 13 iterations: 800 loss :2.06475\n",
      "epoch: 13 iterations: 900 loss :2.02203\n",
      "epoch: 13 <====train track===> avg_loss: 0.019191320128459793, accuracy: 55.23809523809524% \n",
      "\n",
      "epoch 14 starting ...\n",
      "epoch: 14 iterations: 0 loss :1.23513\n",
      "epoch: 14 iterations: 100 loss :1.23712\n",
      "epoch: 14 iterations: 200 loss :1.89719\n",
      "epoch: 14 iterations: 300 loss :2.10498\n",
      "epoch: 14 iterations: 400 loss :2.08109\n",
      "epoch: 14 iterations: 500 loss :1.20924\n",
      "epoch: 14 iterations: 600 loss :1.66874\n",
      "epoch: 14 iterations: 700 loss :1.20746\n",
      "epoch: 14 iterations: 800 loss :1.21322\n",
      "epoch: 14 iterations: 900 loss :1.93294\n",
      "epoch: 14 <====train track===> avg_loss: 0.019132141216915602, accuracy: 43.80952380952381% \n",
      "\n",
      "epoch 15 starting ...\n",
      "epoch: 15 iterations: 0 loss :2.11585\n",
      "epoch: 15 iterations: 100 loss :1.4032\n",
      "epoch: 15 iterations: 200 loss :1.41738\n",
      "epoch: 15 iterations: 300 loss :2.00395\n",
      "epoch: 15 iterations: 400 loss :1.40016\n",
      "epoch: 15 iterations: 500 loss :2.06035\n",
      "epoch: 15 iterations: 600 loss :2.03943\n",
      "epoch: 15 iterations: 700 loss :1.4871\n",
      "epoch: 15 iterations: 800 loss :1.25851\n",
      "epoch: 15 iterations: 900 loss :1.33291\n",
      "epoch: 15 <====train track===> avg_loss: 0.01923657228025651, accuracy: 51.42857142857143% \n",
      "\n",
      "epoch 16 starting ...\n",
      "epoch: 16 iterations: 0 loss :1.31134\n",
      "epoch: 16 iterations: 100 loss :1.69023\n",
      "epoch: 16 iterations: 200 loss :1.4625\n",
      "epoch: 16 iterations: 300 loss :1.2544\n",
      "epoch: 16 iterations: 400 loss :1.44587\n",
      "epoch: 16 iterations: 500 loss :1.32822\n",
      "epoch: 16 iterations: 600 loss :1.99819\n",
      "epoch: 16 iterations: 700 loss :2.01472\n",
      "epoch: 16 iterations: 800 loss :2.12527\n",
      "epoch: 16 iterations: 900 loss :2.04717\n",
      "epoch: 16 <====train track===> avg_loss: 0.019031745868383257, accuracy: 48.57142857142857% \n",
      "\n",
      "epoch 17 starting ...\n",
      "epoch: 17 iterations: 0 loss :1.17253\n",
      "epoch: 17 iterations: 100 loss :1.76926\n",
      "epoch: 17 iterations: 200 loss :1.92253\n",
      "epoch: 17 iterations: 300 loss :1.64065\n",
      "epoch: 17 iterations: 400 loss :1.25325\n",
      "epoch: 17 iterations: 500 loss :2.1166\n",
      "epoch: 17 iterations: 600 loss :2.14085\n",
      "epoch: 17 iterations: 700 loss :1.42057\n",
      "epoch: 17 iterations: 800 loss :1.30659\n",
      "epoch: 17 iterations: 900 loss :2.13892\n",
      "epoch: 17 <====train track===> avg_loss: 0.019206662323318233, accuracy: 53.333333333333336% \n",
      "\n",
      "epoch 18 starting ...\n",
      "epoch: 18 iterations: 0 loss :2.12269\n",
      "epoch: 18 iterations: 100 loss :2.10037\n",
      "epoch: 18 iterations: 200 loss :2.09369\n",
      "epoch: 18 iterations: 300 loss :1.52399\n",
      "epoch: 18 iterations: 400 loss :2.07933\n",
      "epoch: 18 iterations: 500 loss :2.09382\n",
      "epoch: 18 iterations: 600 loss :2.1041\n",
      "epoch: 18 iterations: 700 loss :1.17415\n",
      "epoch: 18 iterations: 800 loss :2.06949\n",
      "epoch: 18 iterations: 900 loss :1.17493\n",
      "epoch: 18 <====train track===> avg_loss: 0.018917667292449036, accuracy: 50.476190476190474% \n",
      "\n",
      "epoch 19 starting ...\n",
      "epoch: 19 iterations: 0 loss :1.37733\n",
      "epoch: 19 iterations: 100 loss :2.14262\n",
      "epoch: 19 iterations: 200 loss :2.12488\n",
      "epoch: 19 iterations: 300 loss :1.69169\n",
      "epoch: 19 iterations: 400 loss :1.51948\n",
      "epoch: 19 iterations: 500 loss :2.0797\n",
      "epoch: 19 iterations: 600 loss :1.16765\n",
      "epoch: 19 iterations: 700 loss :1.69942\n",
      "epoch: 19 iterations: 800 loss :2.10923\n",
      "epoch: 19 iterations: 900 loss :1.25725\n",
      "epoch: 19 <====train track===> avg_loss: 0.01885316884068067, accuracy: 52.38095238095238% \n",
      "\n",
      "epoch 20 starting ...\n",
      "epoch: 20 iterations: 0 loss :1.7264\n",
      "epoch: 20 iterations: 100 loss :1.33558\n",
      "epoch: 20 iterations: 200 loss :1.43559\n",
      "epoch: 20 iterations: 300 loss :1.27687\n",
      "epoch: 20 iterations: 400 loss :1.28524\n",
      "epoch: 20 iterations: 500 loss :1.90223\n",
      "epoch: 20 iterations: 600 loss :2.03963\n",
      "epoch: 20 iterations: 700 loss :2.13026\n",
      "epoch: 20 iterations: 800 loss :1.51393\n",
      "epoch: 20 iterations: 900 loss :2.04439\n",
      "epoch: 20 <====train track===> avg_loss: 0.01845243462175378, accuracy: 56.19047619047619% \n",
      "\n",
      "epoch 21 starting ...\n",
      "epoch: 21 iterations: 0 loss :1.71952\n",
      "epoch: 21 iterations: 100 loss :2.0719\n",
      "epoch: 21 iterations: 200 loss :2.11951\n",
      "epoch: 21 iterations: 300 loss :1.66425\n",
      "epoch: 21 iterations: 400 loss :2.0047\n",
      "epoch: 21 iterations: 500 loss :1.16669\n",
      "epoch: 21 iterations: 600 loss :1.5209\n",
      "epoch: 21 iterations: 700 loss :1.95047\n",
      "epoch: 21 iterations: 800 loss :1.39981\n",
      "epoch: 21 iterations: 900 loss :2.11988\n",
      "epoch: 21 <====train track===> avg_loss: 0.018787204610896193, accuracy: 60.95238095238095% \n",
      "\n",
      "epoch 22 starting ...\n",
      "epoch: 22 iterations: 0 loss :2.15269\n",
      "epoch: 22 iterations: 100 loss :2.10801\n",
      "epoch: 22 iterations: 200 loss :2.00712\n",
      "epoch: 22 iterations: 300 loss :2.10267\n",
      "epoch: 22 iterations: 400 loss :1.91274\n",
      "epoch: 22 iterations: 500 loss :1.75013\n",
      "epoch: 22 iterations: 600 loss :1.25754\n",
      "epoch: 22 iterations: 700 loss :2.06417\n",
      "epoch: 22 iterations: 800 loss :1.26354\n",
      "epoch: 22 iterations: 900 loss :1.89822\n",
      "epoch: 22 <====train track===> avg_loss: 0.019227281217731918, accuracy: 57.142857142857146% \n",
      "\n",
      "epoch 23 starting ...\n",
      "epoch: 23 iterations: 0 loss :1.17828\n",
      "epoch: 23 iterations: 100 loss :1.32335\n",
      "epoch: 23 iterations: 200 loss :1.29661\n",
      "epoch: 23 iterations: 300 loss :1.24998\n",
      "epoch: 23 iterations: 400 loss :1.28605\n",
      "epoch: 23 iterations: 500 loss :1.66187\n",
      "epoch: 23 iterations: 600 loss :1.26081\n",
      "epoch: 23 iterations: 700 loss :1.18781\n",
      "epoch: 23 iterations: 800 loss :1.26036\n",
      "epoch: 23 iterations: 900 loss :2.00664\n",
      "epoch: 23 <====train track===> avg_loss: 0.018128880271918772, accuracy: 56.19047619047619% \n",
      "\n",
      "epoch 24 starting ...\n",
      "epoch: 24 iterations: 0 loss :1.17478\n",
      "epoch: 24 iterations: 100 loss :1.39603\n",
      "epoch: 24 iterations: 200 loss :2.14527\n",
      "epoch: 24 iterations: 300 loss :2.1183\n",
      "epoch: 24 iterations: 400 loss :1.21715\n",
      "epoch: 24 iterations: 500 loss :1.23035\n",
      "epoch: 24 iterations: 600 loss :2.04364\n",
      "epoch: 24 iterations: 700 loss :2.14547\n",
      "epoch: 24 iterations: 800 loss :1.44128\n",
      "epoch: 24 iterations: 900 loss :1.84014\n",
      "epoch: 24 <====train track===> avg_loss: 0.018873332710410286, accuracy: 57.142857142857146% \n",
      "\n",
      "epoch 25 starting ...\n",
      "epoch: 25 iterations: 0 loss :2.1201\n",
      "epoch: 25 iterations: 100 loss :2.12429\n",
      "epoch: 25 iterations: 200 loss :1.32524\n",
      "epoch: 25 iterations: 300 loss :2.11378\n",
      "epoch: 25 iterations: 400 loss :2.14165\n",
      "epoch: 25 iterations: 500 loss :2.13992\n",
      "epoch: 25 iterations: 600 loss :1.27775\n",
      "epoch: 25 iterations: 700 loss :1.17497\n",
      "epoch: 25 iterations: 800 loss :1.16717\n",
      "epoch: 25 iterations: 900 loss :1.23254\n",
      "epoch: 25 <====train track===> avg_loss: 0.018309565994200062, accuracy: 58.095238095238095% \n",
      "\n",
      "epoch 26 starting ...\n",
      "epoch: 26 iterations: 0 loss :1.88205\n",
      "epoch: 26 iterations: 100 loss :1.90702\n",
      "epoch: 26 iterations: 200 loss :2.04909\n",
      "epoch: 26 iterations: 300 loss :1.16698\n",
      "epoch: 26 iterations: 400 loss :1.55945\n",
      "epoch: 26 iterations: 500 loss :1.97553\n",
      "epoch: 26 iterations: 600 loss :1.19341\n",
      "epoch: 26 iterations: 700 loss :2.04139\n",
      "epoch: 26 iterations: 800 loss :1.16719\n",
      "epoch: 26 iterations: 900 loss :1.75392\n",
      "epoch: 26 <====train track===> avg_loss: 0.018386512818816994, accuracy: 60.95238095238095% \n",
      "\n",
      "epoch 27 starting ...\n",
      "epoch: 27 iterations: 0 loss :1.35569\n",
      "epoch: 27 iterations: 100 loss :1.25058\n",
      "epoch: 27 iterations: 200 loss :1.69721\n",
      "epoch: 27 iterations: 300 loss :1.18798\n",
      "epoch: 27 iterations: 400 loss :1.18781\n",
      "epoch: 27 iterations: 500 loss :2.10738\n",
      "epoch: 27 iterations: 600 loss :1.91843\n",
      "epoch: 27 iterations: 700 loss :1.16611\n",
      "epoch: 27 iterations: 800 loss :1.1935\n",
      "epoch: 27 iterations: 900 loss :1.18933\n",
      "epoch: 27 <====train track===> avg_loss: 0.018688124876651918, accuracy: 57.142857142857146% \n",
      "\n",
      "epoch 28 starting ...\n",
      "epoch: 28 iterations: 0 loss :1.18342\n",
      "epoch: 28 iterations: 100 loss :1.20917\n",
      "epoch: 28 iterations: 200 loss :1.62879\n",
      "epoch: 28 iterations: 300 loss :1.23742\n",
      "epoch: 28 iterations: 400 loss :1.47222\n",
      "epoch: 28 iterations: 500 loss :1.3638\n",
      "epoch: 28 iterations: 600 loss :2.05412\n",
      "epoch: 28 iterations: 700 loss :1.23073\n",
      "epoch: 28 iterations: 800 loss :1.20599\n",
      "epoch: 28 iterations: 900 loss :2.15115\n",
      "epoch: 28 <====train track===> avg_loss: 0.018792064612116088, accuracy: 58.095238095238095% \n",
      "\n",
      "epoch 29 starting ...\n",
      "epoch: 29 iterations: 0 loss :1.99666\n",
      "epoch: 29 iterations: 100 loss :1.75004\n",
      "epoch: 29 iterations: 200 loss :1.27721\n",
      "epoch: 29 iterations: 300 loss :2.14317\n",
      "epoch: 29 iterations: 400 loss :1.22199\n",
      "epoch: 29 iterations: 500 loss :1.3202\n",
      "epoch: 29 iterations: 600 loss :1.62228\n",
      "epoch: 29 iterations: 700 loss :2.12009\n",
      "epoch: 29 iterations: 800 loss :1.20893\n",
      "epoch: 29 iterations: 900 loss :1.2881\n",
      "epoch: 29 <====train track===> avg_loss: 0.018102402491653347, accuracy: 60.0% \n",
      "\n",
      "epoch 30 starting ...\n",
      "epoch: 30 iterations: 0 loss :1.17483\n",
      "epoch: 30 iterations: 100 loss :1.16582\n",
      "epoch: 30 iterations: 200 loss :2.11495\n",
      "epoch: 30 iterations: 300 loss :1.22971\n",
      "epoch: 30 iterations: 400 loss :1.20942\n",
      "epoch: 30 iterations: 500 loss :1.8608\n",
      "epoch: 30 iterations: 600 loss :1.18023\n",
      "epoch: 30 iterations: 700 loss :1.3784\n",
      "epoch: 30 iterations: 800 loss :1.99957\n",
      "epoch: 30 iterations: 900 loss :2.1152\n",
      "epoch: 30 <====train track===> avg_loss: 0.018275685538421738, accuracy: 58.095238095238095% \n",
      "\n",
      "epoch 31 starting ...\n",
      "epoch: 31 iterations: 0 loss :1.68226\n",
      "epoch: 31 iterations: 100 loss :2.10877\n",
      "epoch: 31 iterations: 200 loss :1.18774\n",
      "epoch: 31 iterations: 300 loss :1.89187\n",
      "epoch: 31 iterations: 400 loss :2.03785\n",
      "epoch: 31 iterations: 500 loss :2.1071\n",
      "epoch: 31 iterations: 600 loss :1.16896\n",
      "epoch: 31 iterations: 700 loss :1.37116\n",
      "epoch: 31 iterations: 800 loss :2.1071\n",
      "epoch: 31 iterations: 900 loss :1.22141\n",
      "epoch: 31 <====train track===> avg_loss: 0.018169810520011997, accuracy: 62.857142857142854% \n",
      "\n",
      "epoch 32 starting ...\n",
      "epoch: 32 iterations: 0 loss :1.16592\n",
      "epoch: 32 iterations: 100 loss :1.29347\n",
      "epoch: 32 iterations: 200 loss :2.07709\n",
      "epoch: 32 iterations: 300 loss :1.98445\n",
      "epoch: 32 iterations: 400 loss :1.17371\n",
      "epoch: 32 iterations: 500 loss :1.90729\n",
      "epoch: 32 iterations: 600 loss :1.26957\n",
      "epoch: 32 iterations: 700 loss :1.90355\n",
      "epoch: 32 iterations: 800 loss :1.18065\n",
      "epoch: 32 iterations: 900 loss :1.49614\n",
      "epoch: 32 <====train track===> avg_loss: 0.018049419589724076, accuracy: 60.0% \n",
      "\n",
      "epoch 33 starting ...\n",
      "epoch: 33 iterations: 0 loss :2.11509\n",
      "epoch: 33 iterations: 100 loss :1.29806\n",
      "epoch: 33 iterations: 200 loss :2.07695\n",
      "epoch: 33 iterations: 300 loss :1.18131\n",
      "epoch: 33 iterations: 400 loss :1.21809\n",
      "epoch: 33 iterations: 500 loss :2.15568\n",
      "epoch: 33 iterations: 600 loss :1.28497\n",
      "epoch: 33 iterations: 700 loss :2.15229\n",
      "epoch: 33 iterations: 800 loss :1.16577\n",
      "epoch: 33 iterations: 900 loss :2.13089\n",
      "epoch: 33 <====train track===> avg_loss: 0.017713485526702148, accuracy: 60.95238095238095% \n",
      "\n",
      "epoch 34 starting ...\n",
      "epoch: 34 iterations: 0 loss :1.27619\n",
      "epoch: 34 iterations: 100 loss :1.16546\n",
      "epoch: 34 iterations: 200 loss :1.18315\n",
      "epoch: 34 iterations: 300 loss :2.15714\n",
      "epoch: 34 iterations: 400 loss :1.19273\n",
      "epoch: 34 iterations: 500 loss :1.8936\n",
      "epoch: 34 iterations: 600 loss :1.22779\n",
      "epoch: 34 iterations: 700 loss :1.20056\n",
      "epoch: 34 iterations: 800 loss :1.17866\n",
      "epoch: 34 iterations: 900 loss :2.08067\n",
      "epoch: 34 <====train track===> avg_loss: 0.017961618455976346, accuracy: 57.142857142857146% \n",
      "\n",
      "epoch 35 starting ...\n",
      "epoch: 35 iterations: 0 loss :2.08203\n",
      "epoch: 35 iterations: 100 loss :1.94727\n",
      "epoch: 35 iterations: 200 loss :1.82043\n",
      "epoch: 35 iterations: 300 loss :1.24561\n",
      "epoch: 35 iterations: 400 loss :1.79182\n",
      "epoch: 35 iterations: 500 loss :2.14819\n",
      "epoch: 35 iterations: 600 loss :1.22304\n",
      "epoch: 35 iterations: 700 loss :1.95445\n",
      "epoch: 35 iterations: 800 loss :1.53703\n",
      "epoch: 35 iterations: 900 loss :2.16283\n",
      "epoch: 35 <====train track===> avg_loss: 0.018011911998409725, accuracy: 59.04761904761905% \n",
      "\n",
      "epoch 36 starting ...\n",
      "epoch: 36 iterations: 0 loss :2.07817\n",
      "epoch: 36 iterations: 100 loss :1.3018\n",
      "epoch: 36 iterations: 200 loss :1.16673\n",
      "epoch: 36 iterations: 300 loss :1.20057\n",
      "epoch: 36 iterations: 400 loss :1.17412\n",
      "epoch: 36 iterations: 500 loss :1.26939\n",
      "epoch: 36 iterations: 600 loss :1.20553\n",
      "epoch: 36 iterations: 700 loss :2.07329\n",
      "epoch: 36 iterations: 800 loss :2.15803\n",
      "epoch: 36 iterations: 900 loss :2.06962\n",
      "epoch: 36 <====train track===> avg_loss: 0.017630276544013515, accuracy: 59.04761904761905% \n",
      "\n",
      "epoch 37 starting ...\n",
      "epoch: 37 iterations: 0 loss :1.21091\n",
      "epoch: 37 iterations: 100 loss :1.17128\n",
      "epoch: 37 iterations: 200 loss :1.73073\n",
      "epoch: 37 iterations: 300 loss :1.16553\n",
      "epoch: 37 iterations: 400 loss :1.80725\n",
      "epoch: 37 iterations: 500 loss :1.16545\n",
      "epoch: 37 iterations: 600 loss :1.89804\n",
      "epoch: 37 iterations: 700 loss :1.97802\n",
      "epoch: 37 iterations: 800 loss :2.11398\n",
      "epoch: 37 iterations: 900 loss :1.27138\n",
      "epoch: 37 <====train track===> avg_loss: 0.01775578825221663, accuracy: 61.904761904761905% \n",
      "\n",
      "epoch 38 starting ...\n",
      "epoch: 38 iterations: 0 loss :1.20025\n",
      "epoch: 38 iterations: 100 loss :2.12413\n",
      "epoch: 38 iterations: 200 loss :1.74604\n",
      "epoch: 38 iterations: 300 loss :2.04176\n",
      "epoch: 38 iterations: 400 loss :2.11521\n",
      "epoch: 38 iterations: 500 loss :2.1006\n",
      "epoch: 38 iterations: 600 loss :1.55735\n",
      "epoch: 38 iterations: 700 loss :2.06386\n",
      "epoch: 38 iterations: 800 loss :2.04144\n",
      "epoch: 38 iterations: 900 loss :1.71693\n",
      "epoch: 38 <====train track===> avg_loss: 0.01807608320186713, accuracy: 59.04761904761905% \n",
      "\n",
      "epoch 39 starting ...\n",
      "epoch: 39 iterations: 0 loss :1.19128\n",
      "epoch: 39 iterations: 100 loss :2.07725\n",
      "epoch: 39 iterations: 200 loss :1.28066\n",
      "epoch: 39 iterations: 300 loss :1.20957\n",
      "epoch: 39 iterations: 400 loss :1.21523\n",
      "epoch: 39 iterations: 500 loss :1.31855\n",
      "epoch: 39 iterations: 600 loss :1.21276\n",
      "epoch: 39 iterations: 700 loss :1.20552\n",
      "epoch: 39 iterations: 800 loss :1.29172\n",
      "epoch: 39 iterations: 900 loss :1.16561\n",
      "epoch: 39 <====train track===> avg_loss: 0.01804496800412652, accuracy: 60.0% \n",
      "\n",
      "epoch 40 starting ...\n",
      "epoch: 40 iterations: 0 loss :1.19371\n",
      "epoch: 40 iterations: 100 loss :1.18702\n",
      "epoch: 40 iterations: 200 loss :1.2104\n",
      "epoch: 40 iterations: 300 loss :1.18489\n",
      "epoch: 40 iterations: 400 loss :1.17134\n",
      "epoch: 40 iterations: 500 loss :1.24619\n",
      "epoch: 40 iterations: 600 loss :1.82859\n",
      "epoch: 40 iterations: 700 loss :2.06357\n",
      "epoch: 40 iterations: 800 loss :1.79381\n",
      "epoch: 40 iterations: 900 loss :1.39315\n",
      "epoch: 40 <====train track===> avg_loss: 0.017764198638978202, accuracy: 60.0% \n",
      "\n",
      "epoch 41 starting ...\n",
      "epoch: 41 iterations: 0 loss :2.09274\n",
      "epoch: 41 iterations: 100 loss :1.26072\n",
      "epoch: 41 iterations: 200 loss :2.05043\n",
      "epoch: 41 iterations: 300 loss :1.27691\n",
      "epoch: 41 iterations: 400 loss :1.34326\n",
      "epoch: 41 iterations: 500 loss :2.1522\n",
      "epoch: 41 iterations: 600 loss :1.18953\n",
      "epoch: 41 iterations: 700 loss :1.36273\n",
      "epoch: 41 iterations: 800 loss :1.41684\n",
      "epoch: 41 iterations: 900 loss :2.05251\n",
      "epoch: 41 <====train track===> avg_loss: 0.017777719594838322, accuracy: 57.142857142857146% \n",
      "\n",
      "epoch 42 starting ...\n",
      "epoch: 42 iterations: 0 loss :1.19488\n",
      "epoch: 42 iterations: 100 loss :2.00059\n",
      "epoch: 42 iterations: 200 loss :2.13441\n",
      "epoch: 42 iterations: 300 loss :1.18519\n",
      "epoch: 42 iterations: 400 loss :2.10442\n",
      "epoch: 42 iterations: 500 loss :2.14707\n",
      "epoch: 42 iterations: 600 loss :2.01565\n",
      "epoch: 42 iterations: 700 loss :1.4617\n",
      "epoch: 42 iterations: 800 loss :2.05116\n",
      "epoch: 42 iterations: 900 loss :1.20102\n",
      "epoch: 42 <====train track===> avg_loss: 0.017677797670877904, accuracy: 60.0% \n",
      "\n",
      "epoch 43 starting ...\n",
      "epoch: 43 iterations: 0 loss :1.69047\n",
      "epoch: 43 iterations: 100 loss :1.41967\n",
      "epoch: 43 iterations: 200 loss :1.17594\n",
      "epoch: 43 iterations: 300 loss :1.2302\n",
      "epoch: 43 iterations: 400 loss :1.17821\n",
      "epoch: 43 iterations: 500 loss :1.66285\n",
      "epoch: 43 iterations: 600 loss :2.08248\n",
      "epoch: 43 iterations: 700 loss :1.44173\n",
      "epoch: 43 iterations: 800 loss :1.82007\n",
      "epoch: 43 iterations: 900 loss :1.2544\n",
      "epoch: 43 <====train track===> avg_loss: 0.017839082103795048, accuracy: 57.142857142857146% \n",
      "\n",
      "epoch 44 starting ...\n",
      "epoch: 44 iterations: 0 loss :2.06415\n",
      "epoch: 44 iterations: 100 loss :2.1425\n",
      "epoch: 44 iterations: 200 loss :2.11632\n",
      "epoch: 44 iterations: 300 loss :1.93081\n",
      "epoch: 44 iterations: 400 loss :2.14898\n",
      "epoch: 44 iterations: 500 loss :2.09305\n",
      "epoch: 44 iterations: 600 loss :1.2201\n",
      "epoch: 44 iterations: 700 loss :1.24025\n",
      "epoch: 44 iterations: 800 loss :2.09781\n",
      "epoch: 44 iterations: 900 loss :1.16567\n",
      "epoch: 44 <====train track===> avg_loss: 0.01739662034265225, accuracy: 60.0% \n",
      "\n",
      "epoch 45 starting ...\n",
      "epoch: 45 iterations: 0 loss :1.17765\n",
      "epoch: 45 iterations: 100 loss :1.27385\n",
      "epoch: 45 iterations: 200 loss :1.48067\n",
      "epoch: 45 iterations: 300 loss :2.11019\n",
      "epoch: 45 iterations: 400 loss :1.77458\n",
      "epoch: 45 iterations: 500 loss :1.19346\n",
      "epoch: 45 iterations: 600 loss :1.17805\n",
      "epoch: 45 iterations: 700 loss :1.49029\n",
      "epoch: 45 iterations: 800 loss :2.1131\n",
      "epoch: 45 iterations: 900 loss :2.09486\n",
      "epoch: 45 <====train track===> avg_loss: 0.017619397746398608, accuracy: 57.142857142857146% \n",
      "\n",
      "epoch 46 starting ...\n",
      "epoch: 46 iterations: 0 loss :2.11512\n",
      "epoch: 46 iterations: 100 loss :2.11888\n",
      "epoch: 46 iterations: 200 loss :2.15617\n",
      "epoch: 46 iterations: 300 loss :2.09336\n",
      "epoch: 46 iterations: 400 loss :1.74847\n",
      "epoch: 46 iterations: 500 loss :2.09904\n",
      "epoch: 46 iterations: 600 loss :1.24174\n",
      "epoch: 46 iterations: 700 loss :2.13416\n",
      "epoch: 46 iterations: 800 loss :2.0598\n",
      "epoch: 46 iterations: 900 loss :1.20292\n",
      "epoch: 46 <====train track===> avg_loss: 0.0176169201612778, accuracy: 60.0% \n",
      "\n",
      "epoch 47 starting ...\n",
      "epoch: 47 iterations: 0 loss :1.25412\n",
      "epoch: 47 iterations: 100 loss :1.9191\n",
      "epoch: 47 iterations: 200 loss :1.63883\n",
      "epoch: 47 iterations: 300 loss :2.08891\n",
      "epoch: 47 iterations: 400 loss :2.05002\n",
      "epoch: 47 iterations: 500 loss :1.93127\n",
      "epoch: 47 iterations: 600 loss :1.17715\n",
      "epoch: 47 iterations: 700 loss :1.17395\n",
      "epoch: 47 iterations: 800 loss :2.16235\n",
      "epoch: 47 iterations: 900 loss :1.96034\n",
      "epoch: 47 <====train track===> avg_loss: 0.01775337415992263, accuracy: 60.0% \n",
      "\n",
      "epoch 48 starting ...\n",
      "epoch: 48 iterations: 0 loss :2.06956\n",
      "epoch: 48 iterations: 100 loss :1.84779\n",
      "epoch: 48 iterations: 200 loss :2.12298\n",
      "epoch: 48 iterations: 300 loss :1.19108\n",
      "epoch: 48 iterations: 400 loss :2.12796\n",
      "epoch: 48 iterations: 500 loss :2.02668\n",
      "epoch: 48 iterations: 600 loss :1.18747\n",
      "epoch: 48 iterations: 700 loss :1.19902\n",
      "epoch: 48 iterations: 800 loss :2.11746\n",
      "epoch: 48 iterations: 900 loss :1.17132\n",
      "epoch: 48 <====train track===> avg_loss: 0.017844297769555473, accuracy: 49.523809523809526% \n",
      "\n",
      "epoch 49 starting ...\n",
      "epoch: 49 iterations: 0 loss :1.17021\n",
      "epoch: 49 iterations: 100 loss :1.30958\n",
      "epoch: 49 iterations: 200 loss :1.20849\n",
      "epoch: 49 iterations: 300 loss :2.12085\n",
      "epoch: 49 iterations: 400 loss :1.24728\n",
      "epoch: 49 iterations: 500 loss :2.0995\n",
      "epoch: 49 iterations: 600 loss :1.64316\n",
      "epoch: 49 iterations: 700 loss :1.27256\n",
      "epoch: 49 iterations: 800 loss :1.2441\n",
      "epoch: 49 iterations: 900 loss :1.54797\n",
      "epoch: 49 <====train track===> avg_loss: 0.018123572231721504, accuracy: 54.285714285714285% \n",
      "\n",
      "epoch 50 starting ...\n",
      "epoch: 50 iterations: 0 loss :1.19704\n",
      "epoch: 50 iterations: 100 loss :1.87924\n",
      "epoch: 50 iterations: 200 loss :2.12119\n",
      "epoch: 50 iterations: 300 loss :1.21133\n",
      "epoch: 50 iterations: 400 loss :2.07151\n",
      "epoch: 50 iterations: 500 loss :1.19023\n",
      "epoch: 50 iterations: 600 loss :1.96528\n",
      "epoch: 50 iterations: 700 loss :1.91468\n",
      "epoch: 50 iterations: 800 loss :1.90265\n",
      "epoch: 50 iterations: 900 loss :1.25773\n",
      "epoch: 50 <====train track===> avg_loss: 0.017664996378156567, accuracy: 54.285714285714285% \n",
      "\n",
      "epoch 51 starting ...\n",
      "epoch: 51 iterations: 0 loss :1.17373\n",
      "epoch: 51 iterations: 100 loss :1.18849\n",
      "epoch: 51 iterations: 200 loss :2.06259\n",
      "epoch: 51 iterations: 300 loss :1.1918\n",
      "epoch: 51 iterations: 400 loss :2.12778\n",
      "epoch: 51 iterations: 500 loss :2.10495\n",
      "epoch: 51 iterations: 600 loss :1.34563\n",
      "epoch: 51 iterations: 700 loss :1.33132\n",
      "epoch: 51 iterations: 800 loss :2.1539\n",
      "epoch: 51 iterations: 900 loss :1.17457\n",
      "epoch: 51 <====train track===> avg_loss: 0.018176629618099457, accuracy: 50.476190476190474% \n",
      "\n",
      "epoch 52 starting ...\n",
      "epoch: 52 iterations: 0 loss :2.12841\n",
      "epoch: 52 iterations: 100 loss :1.63446\n",
      "epoch: 52 iterations: 200 loss :2.02439\n",
      "epoch: 52 iterations: 300 loss :2.06895\n",
      "epoch: 52 iterations: 400 loss :2.15545\n",
      "epoch: 52 iterations: 500 loss :2.16306\n",
      "epoch: 52 iterations: 600 loss :1.47704\n",
      "epoch: 52 iterations: 700 loss :1.50884\n",
      "epoch: 52 iterations: 800 loss :2.13728\n",
      "epoch: 52 iterations: 900 loss :2.11561\n",
      "epoch: 52 <====train track===> avg_loss: 0.018270478742536546, accuracy: 57.142857142857146% \n",
      "\n",
      "epoch 53 starting ...\n",
      "epoch: 53 iterations: 0 loss :1.96605\n",
      "epoch: 53 iterations: 100 loss :2.12176\n",
      "epoch: 53 iterations: 200 loss :1.17943\n",
      "epoch: 53 iterations: 300 loss :2.15346\n",
      "epoch: 53 iterations: 400 loss :1.19787\n",
      "epoch: 53 iterations: 500 loss :2.00213\n",
      "epoch: 53 iterations: 600 loss :1.19567\n",
      "epoch: 53 iterations: 700 loss :2.11303\n",
      "epoch: 53 iterations: 800 loss :1.6806\n",
      "epoch: 53 iterations: 900 loss :1.5888\n",
      "epoch: 53 <====train track===> avg_loss: 0.01765108083157394, accuracy: 59.04761904761905% \n",
      "\n",
      "epoch 54 starting ...\n",
      "epoch: 54 iterations: 0 loss :1.9106\n",
      "epoch: 54 iterations: 100 loss :1.35466\n",
      "epoch: 54 iterations: 200 loss :2.03866\n",
      "epoch: 54 iterations: 300 loss :1.20647\n",
      "epoch: 54 iterations: 400 loss :2.00077\n",
      "epoch: 54 iterations: 500 loss :2.02482\n",
      "epoch: 54 iterations: 600 loss :1.45062\n",
      "epoch: 54 iterations: 700 loss :1.34483\n",
      "epoch: 54 iterations: 800 loss :1.21885\n",
      "epoch: 54 iterations: 900 loss :1.19554\n",
      "epoch: 54 <====train track===> avg_loss: 0.017748101882610886, accuracy: 60.95238095238095% \n",
      "\n",
      "epoch 55 starting ...\n",
      "epoch: 55 iterations: 0 loss :1.9655\n",
      "epoch: 55 iterations: 100 loss :2.11695\n",
      "epoch: 55 iterations: 200 loss :1.17292\n",
      "epoch: 55 iterations: 300 loss :1.50384\n",
      "epoch: 55 iterations: 400 loss :2.01244\n",
      "epoch: 55 iterations: 500 loss :1.28171\n",
      "epoch: 55 iterations: 600 loss :1.17616\n",
      "epoch: 55 iterations: 700 loss :1.29421\n",
      "epoch: 55 iterations: 800 loss :2.04367\n",
      "epoch: 55 iterations: 900 loss :1.69879\n",
      "epoch: 55 <====train track===> avg_loss: 0.017723031815758142, accuracy: 61.904761904761905% \n",
      "\n",
      "epoch 56 starting ...\n",
      "epoch: 56 iterations: 0 loss :1.17536\n",
      "epoch: 56 iterations: 100 loss :1.24172\n",
      "epoch: 56 iterations: 200 loss :2.15865\n",
      "epoch: 56 iterations: 300 loss :1.16935\n",
      "epoch: 56 iterations: 400 loss :1.17533\n",
      "epoch: 56 iterations: 500 loss :1.23455\n",
      "epoch: 56 iterations: 600 loss :2.11434\n",
      "epoch: 56 iterations: 700 loss :2.00327\n",
      "epoch: 56 iterations: 800 loss :1.18274\n",
      "epoch: 56 iterations: 900 loss :1.21272\n",
      "epoch: 56 <====train track===> avg_loss: 0.017718177710212776, accuracy: 61.904761904761905% \n",
      "\n",
      "epoch 57 starting ...\n",
      "epoch: 57 iterations: 0 loss :1.80418\n",
      "epoch: 57 iterations: 100 loss :1.34761\n",
      "epoch: 57 iterations: 200 loss :1.18525\n",
      "epoch: 57 iterations: 300 loss :1.97976\n",
      "epoch: 57 iterations: 400 loss :1.2315\n",
      "epoch: 57 iterations: 500 loss :2.14904\n",
      "epoch: 57 iterations: 600 loss :1.18679\n",
      "epoch: 57 iterations: 700 loss :1.33623\n",
      "epoch: 57 iterations: 800 loss :1.21082\n",
      "epoch: 57 iterations: 900 loss :1.174\n",
      "epoch: 57 <====train track===> avg_loss: 0.017809972766387832, accuracy: 57.142857142857146% \n",
      "\n",
      "epoch 58 starting ...\n",
      "epoch: 58 iterations: 0 loss :1.18783\n",
      "epoch: 58 iterations: 100 loss :1.18207\n",
      "epoch: 58 iterations: 200 loss :2.14936\n",
      "epoch: 58 iterations: 300 loss :1.19582\n",
      "epoch: 58 iterations: 400 loss :1.17263\n",
      "epoch: 58 iterations: 500 loss :1.20298\n",
      "epoch: 58 iterations: 600 loss :2.10346\n",
      "epoch: 58 iterations: 700 loss :1.46763\n",
      "epoch: 58 iterations: 800 loss :1.183\n",
      "epoch: 58 iterations: 900 loss :2.1179\n",
      "epoch: 58 <====train track===> avg_loss: 0.017719646219450567, accuracy: 59.04761904761905% \n",
      "\n",
      "epoch 59 starting ...\n",
      "epoch: 59 iterations: 0 loss :1.17094\n",
      "epoch: 59 iterations: 100 loss :2.11155\n",
      "epoch: 59 iterations: 200 loss :1.1812\n",
      "epoch: 59 iterations: 300 loss :1.19345\n",
      "epoch: 59 iterations: 400 loss :2.14906\n",
      "epoch: 59 iterations: 500 loss :2.12859\n",
      "epoch: 59 iterations: 600 loss :1.62965\n",
      "epoch: 59 iterations: 700 loss :1.85331\n",
      "epoch: 59 iterations: 800 loss :1.17159\n",
      "epoch: 59 iterations: 900 loss :1.26797\n",
      "epoch: 59 <====train track===> avg_loss: 0.017876398207213224, accuracy: 64.76190476190476% \n",
      "\n",
      "epoch 60 starting ...\n",
      "epoch: 60 iterations: 0 loss :2.15999\n",
      "epoch: 60 iterations: 100 loss :2.09128\n",
      "epoch: 60 iterations: 200 loss :1.18342\n",
      "epoch: 60 iterations: 300 loss :1.16955\n",
      "epoch: 60 iterations: 400 loss :1.48939\n",
      "epoch: 60 iterations: 500 loss :1.17129\n",
      "epoch: 60 iterations: 600 loss :1.40862\n",
      "epoch: 60 iterations: 700 loss :1.44787\n",
      "epoch: 60 iterations: 800 loss :1.19583\n",
      "epoch: 60 iterations: 900 loss :1.17832\n",
      "epoch: 60 <====train track===> avg_loss: 0.017737548810490836, accuracy: 66.66666666666667% \n",
      "\n",
      "epoch 61 starting ...\n",
      "epoch: 61 iterations: 0 loss :1.16917\n",
      "epoch: 61 iterations: 100 loss :2.0137\n",
      "epoch: 61 iterations: 200 loss :1.72177\n",
      "epoch: 61 iterations: 300 loss :1.18046\n",
      "epoch: 61 iterations: 400 loss :1.73011\n",
      "epoch: 61 iterations: 500 loss :2.16215\n",
      "epoch: 61 iterations: 600 loss :1.17166\n",
      "epoch: 61 iterations: 700 loss :1.19793\n",
      "epoch: 61 iterations: 800 loss :1.18295\n",
      "epoch: 61 iterations: 900 loss :1.1739\n",
      "epoch: 61 <====train track===> avg_loss: 0.01813512091096644, accuracy: 59.04761904761905% \n",
      "\n",
      "epoch 62 starting ...\n",
      "epoch: 62 iterations: 0 loss :2.16286\n",
      "epoch: 62 iterations: 100 loss :2.15205\n",
      "epoch: 62 iterations: 200 loss :2.14974\n",
      "epoch: 62 iterations: 300 loss :1.17972\n",
      "epoch: 62 iterations: 400 loss :1.2414\n",
      "epoch: 62 iterations: 500 loss :1.20191\n",
      "epoch: 62 iterations: 600 loss :1.31651\n",
      "epoch: 62 iterations: 700 loss :1.53215\n",
      "epoch: 62 iterations: 800 loss :1.16674\n",
      "epoch: 62 iterations: 900 loss :2.16318\n",
      "epoch: 62 <====train track===> avg_loss: 0.017761914025170782, accuracy: 54.285714285714285% \n",
      "\n",
      "epoch 63 starting ...\n",
      "epoch: 63 iterations: 0 loss :1.16888\n",
      "epoch: 63 iterations: 100 loss :1.17007\n",
      "epoch: 63 iterations: 200 loss :2.14746\n",
      "epoch: 63 iterations: 300 loss :1.17849\n",
      "epoch: 63 iterations: 400 loss :1.17364\n",
      "epoch: 63 iterations: 500 loss :2.00546\n",
      "epoch: 63 iterations: 600 loss :2.10543\n",
      "epoch: 63 iterations: 700 loss :1.28175\n",
      "epoch: 63 iterations: 800 loss :2.04057\n",
      "epoch: 63 iterations: 900 loss :2.09329\n",
      "epoch: 63 <====train track===> avg_loss: 0.017912995213090418, accuracy: 54.285714285714285% \n",
      "\n",
      "epoch 64 starting ...\n",
      "epoch: 64 iterations: 0 loss :1.18242\n",
      "epoch: 64 iterations: 100 loss :2.1599\n",
      "epoch: 64 iterations: 200 loss :1.70093\n",
      "epoch: 64 iterations: 300 loss :1.21005\n",
      "epoch: 64 iterations: 400 loss :1.81737\n",
      "epoch: 64 iterations: 500 loss :1.17764\n",
      "epoch: 64 iterations: 600 loss :2.11858\n",
      "epoch: 64 iterations: 700 loss :1.33437\n",
      "epoch: 64 iterations: 800 loss :1.17738\n",
      "epoch: 64 iterations: 900 loss :1.19945\n",
      "epoch: 64 <====train track===> avg_loss: 0.017246826234853994, accuracy: 66.66666666666667% \n",
      "\n",
      "epoch 65 starting ...\n",
      "epoch: 65 iterations: 0 loss :1.85275\n",
      "epoch: 65 iterations: 100 loss :1.63784\n",
      "epoch: 65 iterations: 200 loss :1.60555\n",
      "epoch: 65 iterations: 300 loss :2.01757\n",
      "epoch: 65 iterations: 400 loss :2.14971\n",
      "epoch: 65 iterations: 500 loss :1.98615\n",
      "epoch: 65 iterations: 600 loss :1.17132\n",
      "epoch: 65 iterations: 700 loss :1.17292\n",
      "epoch: 65 iterations: 800 loss :1.24461\n",
      "epoch: 65 iterations: 900 loss :1.21043\n",
      "epoch: 65 <====train track===> avg_loss: 0.01778137365703743, accuracy: 59.04761904761905% \n",
      "\n",
      "epoch 66 starting ...\n",
      "epoch: 66 iterations: 0 loss :1.51391\n",
      "epoch: 66 iterations: 100 loss :1.17236\n",
      "epoch: 66 iterations: 200 loss :1.76028\n",
      "epoch: 66 iterations: 300 loss :1.19157\n",
      "epoch: 66 iterations: 400 loss :2.0216\n",
      "epoch: 66 iterations: 500 loss :1.52659\n",
      "epoch: 66 iterations: 600 loss :1.96332\n",
      "epoch: 66 iterations: 700 loss :1.67019\n",
      "epoch: 66 iterations: 800 loss :2.15672\n",
      "epoch: 66 iterations: 900 loss :1.16695\n",
      "epoch: 66 <====train track===> avg_loss: 0.017524800630891655, accuracy: 61.904761904761905% \n",
      "\n",
      "epoch 67 starting ...\n",
      "epoch: 67 iterations: 0 loss :1.16583\n",
      "epoch: 67 iterations: 100 loss :2.1465\n",
      "epoch: 67 iterations: 200 loss :1.17575\n",
      "epoch: 67 iterations: 300 loss :1.17363\n",
      "epoch: 67 iterations: 400 loss :1.4865\n",
      "epoch: 67 iterations: 500 loss :1.29029\n",
      "epoch: 67 iterations: 600 loss :1.26327\n",
      "epoch: 67 iterations: 700 loss :1.18664\n",
      "epoch: 67 iterations: 800 loss :1.16905\n",
      "epoch: 67 iterations: 900 loss :1.19962\n",
      "epoch: 67 <====train track===> avg_loss: 0.017676814919260587, accuracy: 64.76190476190476% \n",
      "\n",
      "epoch 68 starting ...\n",
      "epoch: 68 iterations: 0 loss :1.55406\n",
      "epoch: 68 iterations: 100 loss :2.08073\n",
      "epoch: 68 iterations: 200 loss :1.16621\n",
      "epoch: 68 iterations: 300 loss :2.16093\n",
      "epoch: 68 iterations: 400 loss :2.14517\n",
      "epoch: 68 iterations: 500 loss :1.40073\n",
      "epoch: 68 iterations: 600 loss :2.14582\n",
      "epoch: 68 iterations: 700 loss :1.55153\n",
      "epoch: 68 iterations: 800 loss :1.21242\n",
      "epoch: 68 iterations: 900 loss :1.59136\n",
      "epoch: 68 <====train track===> avg_loss: 0.01731034778943813, accuracy: 65.71428571428571% \n",
      "\n",
      "epoch 69 starting ...\n",
      "epoch: 69 iterations: 0 loss :1.17621\n",
      "epoch: 69 iterations: 100 loss :1.20705\n",
      "epoch: 69 iterations: 200 loss :2.10885\n",
      "epoch: 69 iterations: 300 loss :1.17824\n",
      "epoch: 69 iterations: 400 loss :1.20415\n",
      "epoch: 69 iterations: 500 loss :1.16547\n",
      "epoch: 69 iterations: 600 loss :1.24269\n",
      "epoch: 69 iterations: 700 loss :2.11785\n",
      "epoch: 69 iterations: 800 loss :2.12921\n",
      "epoch: 69 iterations: 900 loss :1.19258\n",
      "epoch: 69 <====train track===> avg_loss: 0.017256555617919847, accuracy: 66.66666666666667% \n",
      "\n",
      "epoch 70 starting ...\n",
      "epoch: 70 iterations: 0 loss :2.16124\n",
      "epoch: 70 iterations: 100 loss :1.86087\n",
      "epoch: 70 iterations: 200 loss :1.38644\n",
      "epoch: 70 iterations: 300 loss :2.0681\n",
      "epoch: 70 iterations: 400 loss :1.16543\n",
      "epoch: 70 iterations: 500 loss :1.1774\n",
      "epoch: 70 iterations: 600 loss :2.1571\n",
      "epoch: 70 iterations: 700 loss :1.60706\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-39a963f116c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mloss_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#ran 4 times with 3e-5,1e-5, 1e-5, 1e-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-39a963f116c4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epoch, num_iter, rec_interval, disp_interval)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mrec_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DeepCV3.5/lib/python3.5/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training function\n",
    "def train(model, num_epoch, num_iter, rec_interval, disp_interval):\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-5)\n",
    "    loss_values = []\n",
    "    avg_loss_values = []\n",
    "    rec_step = 0\n",
    "    print('Starting the training ...')\n",
    "    for eph in range(num_epoch):\n",
    "        print('epoch {} starting ...'.format(eph))\n",
    "        avg_loss = 0\n",
    "        n_samples = 0\n",
    "        for i in range(num_iter):\n",
    "            model.hidden3 = (model.hidden3[0].detach(), model.hidden3[1].detach())\n",
    "            model.hidden2_1 = (model.hidden2_1[0].detach(), model.hidden2_1[1].detach())\n",
    "            model.hidden2_2 = (model.hidden2_2[0].detach(), model.hidden2_2[1].detach())\n",
    "            model.hidden2_3 = (model.hidden2_3[0].detach(), model.hidden2_3[1].detach())\n",
    "            model.zero_grad()\n",
    "            X,Y = next(ACTd)\n",
    "            n_samples += len(X)\n",
    "            X = autograd.Variable(torch.from_numpy(X).float().cuda())\n",
    "            X = X.view(len(X), 1, -1)\n",
    "            Y = autograd.Variable(torch.LongTensor(np.array([Y])).cuda())\n",
    "\n",
    "            y_hat = model(X)\n",
    "#             print(eph, i, y_hat, Y)\n",
    "            loss = F.cross_entropy(y_hat, Y)\n",
    "#             print(loss)\n",
    "            avg_loss += loss.data[0]\n",
    "            \n",
    "            if i % disp_interval == 0:\n",
    "                print('epoch: %d iterations: %d loss :%g' % (eph, i, loss.data[0]))\n",
    "            if rec_step%rec_interval==0:\n",
    "                loss_values.append(loss.data[0])\n",
    "            \n",
    "            loss.backward()     \n",
    "            optimizer.step()\n",
    "            rec_step += 1\n",
    "            \n",
    "        avg_loss /= n_samples\n",
    "        avg_loss_values.append(avg_loss)\n",
    "        #evaluating model accuracy\n",
    "        acc = evaluate_accuracy(model, test_split)\n",
    "        print('epoch: {} <====train track===> avg_loss: {}, accuracy: {}% \\n'.format(eph, avg_loss, acc))\n",
    "    return loss_values, avg_loss_values\n",
    "\n",
    "\n",
    "loss_vals, avg_loss_vals = train(model0, 100, 1000, 2, 100) #ran 4 times with 3e-5,1e-5, 1e-5, 1e-6\n",
    "plt.figure()\n",
    "plt.plot(loss_vals)\n",
    "plt.figure()\n",
    "plt.plot(avg_loss_vals)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('avg loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_name, path, model):\n",
    "    p = path+'/'+model_name\n",
    "    print('saving at {}'.format(p))\n",
    "    torch.save(model.state_dict(), p)\n",
    "    print('saved at {}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving at ./checkpoints/LSTMClassifierX2_c7.pth\n",
      "saved at ./checkpoints/LSTMClassifierX2_c7.pth\n"
     ]
    }
   ],
   "source": [
    "save_model('LSTMClassifierX2_c7.pth', './checkpoints', model0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
