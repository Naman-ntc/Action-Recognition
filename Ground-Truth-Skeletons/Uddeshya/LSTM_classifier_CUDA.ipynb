{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f73901c7910>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random, numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the datasets, i.e loading frames for few actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.382997</td>\n",
       "      <td>-0.419442</td>\n",
       "      <td>3.449989</td>\n",
       "      <td>-0.366909</td>\n",
       "      <td>-0.092619</td>\n",
       "      <td>3.443680</td>\n",
       "      <td>-0.353380</td>\n",
       "      <td>0.229542</td>\n",
       "      <td>3.427116</td>\n",
       "      <td>-0.391862</td>\n",
       "      <td>...</td>\n",
       "      <td>3.636719</td>\n",
       "      <td>-0.435790</td>\n",
       "      <td>-0.536338</td>\n",
       "      <td>3.280097</td>\n",
       "      <td>-0.364369</td>\n",
       "      <td>-0.491436</td>\n",
       "      <td>3.269750</td>\n",
       "      <td>1</td>\n",
       "      <td>72057594037944340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.383146</td>\n",
       "      <td>-0.419292</td>\n",
       "      <td>3.450006</td>\n",
       "      <td>-0.367569</td>\n",
       "      <td>-0.092003</td>\n",
       "      <td>3.443895</td>\n",
       "      <td>-0.353885</td>\n",
       "      <td>0.230300</td>\n",
       "      <td>3.427162</td>\n",
       "      <td>-0.391820</td>\n",
       "      <td>...</td>\n",
       "      <td>3.633053</td>\n",
       "      <td>-0.436031</td>\n",
       "      <td>-0.536649</td>\n",
       "      <td>3.281972</td>\n",
       "      <td>-0.358806</td>\n",
       "      <td>-0.471054</td>\n",
       "      <td>3.269975</td>\n",
       "      <td>1</td>\n",
       "      <td>72057594037944340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.385776</td>\n",
       "      <td>-0.421191</td>\n",
       "      <td>3.449611</td>\n",
       "      <td>-0.369506</td>\n",
       "      <td>-0.092775</td>\n",
       "      <td>3.443796</td>\n",
       "      <td>-0.354571</td>\n",
       "      <td>0.230189</td>\n",
       "      <td>3.426965</td>\n",
       "      <td>-0.403822</td>\n",
       "      <td>...</td>\n",
       "      <td>3.632370</td>\n",
       "      <td>-0.436489</td>\n",
       "      <td>-0.536484</td>\n",
       "      <td>3.286322</td>\n",
       "      <td>-0.358079</td>\n",
       "      <td>-0.470344</td>\n",
       "      <td>3.270202</td>\n",
       "      <td>1</td>\n",
       "      <td>72057594037944340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.385807</td>\n",
       "      <td>-0.421205</td>\n",
       "      <td>3.449582</td>\n",
       "      <td>-0.369576</td>\n",
       "      <td>-0.092714</td>\n",
       "      <td>3.443878</td>\n",
       "      <td>-0.354524</td>\n",
       "      <td>0.230369</td>\n",
       "      <td>3.427140</td>\n",
       "      <td>-0.403580</td>\n",
       "      <td>...</td>\n",
       "      <td>3.499778</td>\n",
       "      <td>-0.441701</td>\n",
       "      <td>-0.533234</td>\n",
       "      <td>3.278971</td>\n",
       "      <td>-0.360298</td>\n",
       "      <td>-0.476572</td>\n",
       "      <td>3.268953</td>\n",
       "      <td>1</td>\n",
       "      <td>72057594037944340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.357840</td>\n",
       "      <td>-0.420304</td>\n",
       "      <td>3.438846</td>\n",
       "      <td>-0.364956</td>\n",
       "      <td>-0.092426</td>\n",
       "      <td>3.442334</td>\n",
       "      <td>-0.354907</td>\n",
       "      <td>0.230391</td>\n",
       "      <td>3.427352</td>\n",
       "      <td>-0.405945</td>\n",
       "      <td>...</td>\n",
       "      <td>3.400878</td>\n",
       "      <td>-0.430001</td>\n",
       "      <td>-0.536492</td>\n",
       "      <td>3.278641</td>\n",
       "      <td>-0.358697</td>\n",
       "      <td>-0.471415</td>\n",
       "      <td>3.270685</td>\n",
       "      <td>1</td>\n",
       "      <td>72057594037944340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.382997 -0.419442  3.449989 -0.366909 -0.092619  3.443680 -0.353380   \n",
       "1 -0.383146 -0.419292  3.450006 -0.367569 -0.092003  3.443895 -0.353885   \n",
       "2 -0.385776 -0.421191  3.449611 -0.369506 -0.092775  3.443796 -0.354571   \n",
       "3 -0.385807 -0.421205  3.449582 -0.369576 -0.092714  3.443878 -0.354524   \n",
       "4 -0.357840 -0.420304  3.438846 -0.364956 -0.092426  3.442334 -0.354907   \n",
       "\n",
       "          7         8         9    ...           68        69        70  \\\n",
       "0  0.229542  3.427116 -0.391862    ...     3.636719 -0.435790 -0.536338   \n",
       "1  0.230300  3.427162 -0.391820    ...     3.633053 -0.436031 -0.536649   \n",
       "2  0.230189  3.426965 -0.403822    ...     3.632370 -0.436489 -0.536484   \n",
       "3  0.230369  3.427140 -0.403580    ...     3.499778 -0.441701 -0.533234   \n",
       "4  0.230391  3.427352 -0.405945    ...     3.400878 -0.430001 -0.536492   \n",
       "\n",
       "         71        72        73        74  label                 id  video_id  \n",
       "0  3.280097 -0.364369 -0.491436  3.269750      1  72057594037944340         0  \n",
       "1  3.281972 -0.358806 -0.471054  3.269975      1  72057594037944340         0  \n",
       "2  3.286322 -0.358079 -0.470344  3.270202      1  72057594037944340         0  \n",
       "3  3.278971 -0.360298 -0.476572  3.268953      1  72057594037944340         0  \n",
       "4  3.278641 -0.358697 -0.471415  3.270685      1  72057594037944340         0  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading and prepping data\n",
    "#initially only one action\n",
    "dframe = pd.read_csv('./csv_data/action_1.csv')\n",
    "dframe2 = pd.read_csv('./csv_data/action_2.csv')\n",
    "dframe3 = pd.read_csv('./csv_data/action_3.csv')\n",
    "dframe4 = pd.read_csv('./csv_data/action_4.csv')\n",
    "dframe5 = pd.read_csv('./csv_data/action_5.csv')\n",
    "dframe6 = pd.read_csv('./csv_data/action_6.csv')\n",
    "dframe7 = pd.read_csv('./csv_data/action_7.csv')\n",
    "\n",
    "#to look at data\n",
    "dframe.iloc[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some utility functions to split the datasets and loading the datasets in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ (array([[ 0.       ,  0.       ,  0.       , ...,  0.0186279, -0.0719937,\n",
       "        -0.180239 ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.0243399, -0.0517625,\n",
       "        -0.180031 ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.0276977, -0.0491529,\n",
       "        -0.179409 ],\n",
       "       ..., \n",
       "       [ 0.       ,  0.       ,  0.       , ..., -0.0563203, -0.0113986,\n",
       "        -0.174284 ],\n",
       "       [ 0.       ,  0.       ,  0.       , ..., -0.0561737, -0.0162162,\n",
       "        -0.171437 ],\n",
       "       [ 0.       ,  0.       ,  0.       , ..., -0.0559275, -0.0062211,\n",
       "        -0.172233 ]]), 0),\n",
       "       (array([[ 0.        ,  0.        ,  0.        , ...,  0.01849598,\n",
       "         0.0721854 , -0.16189   ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.10443107,\n",
       "         0.05172237, -0.133567  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.08676113,\n",
       "         0.05116256, -0.145192  ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.10355284,\n",
       "         0.52594266, -0.431462  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.09376174,\n",
       "         0.53507   , -0.434229  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.08766792,\n",
       "         0.541823  , -0.422078  ]]), 0),\n",
       "       (array([[ 0.        ,  0.        ,  0.        , ...,  0.10299776,\n",
       "        -0.001346  , -0.226027  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.10025404,\n",
       "        -0.0125163 , -0.230909  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.09391637,\n",
       "        -0.0240066 , -0.245108  ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.0862923 ,\n",
       "         0.0006496 , -0.251447  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.09493048,\n",
       "        -0.0351646 , -0.236531  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.11232334,\n",
       "        -0.0043682 , -0.221527  ]]), 0)], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making test and train split\n",
    "#the recentering has been done so that the pelvic joint is always at the origin\n",
    "#labels are to be zero indexed\n",
    "def train_test_split(dframe_list):\n",
    "    train_split = np.empty(0, dtype=object)\n",
    "    test_split = np.empty(0, dtype=object)\n",
    "    for dframe in dframe_list:\n",
    "        label = dframe.iloc[0,75]-1\n",
    "#         print(label)\n",
    "        num_samples = len(dframe.iloc[:,:])\n",
    "        video_ids = np.unique(dframe.iloc[:,-1].values)\n",
    "        train_video_ids = video_ids[:-15]\n",
    "        test_video_ids = video_ids[-15:]\n",
    "        train_split1 = np.empty(len(train_video_ids), dtype=object)\n",
    "        test_split1 = np.empty(len(test_video_ids), dtype=object)\n",
    "        for idx,i in enumerate(train_video_ids):\n",
    "            train_split1[idx] = dframe.loc[dframe['video_id'] == i].values[:,0:75]\n",
    "            for fidx, f in enumerate(train_split1[idx]):\n",
    "                f = np.reshape(f, (25,3))\n",
    "                f = f-f[0,:]\n",
    "                f = np.reshape(f, (1,75))\n",
    "                train_split1[idx][fidx] = f\n",
    "#             mean_vec = np.mean(train_split1[idx], axis=0)\n",
    "#             std_vec = np.std(train_split1[idx], axis=0)\n",
    "            train_split1[idx] = (train_split1[idx], label)\n",
    "\n",
    "        for idx,i in enumerate(test_video_ids):\n",
    "            test_split1[idx] = dframe.loc[dframe['video_id'] == i].values[:,0:75]\n",
    "            for fidx, f in enumerate(test_split1[idx]):\n",
    "                f = np.reshape(f, (25,3))\n",
    "                f = f-f[0,:]\n",
    "                f = np.reshape(f, (1,75))\n",
    "                test_split1[idx][fidx] = f\n",
    "#             mean_vec = np.mean(test_split1[idx], axis=0)\n",
    "#             std_vec = np.std(test_split1[idx], axis=0)\n",
    "            test_split1[idx] = (test_split1[idx], label)\n",
    "        train_split = np.concatenate((train_split, train_split1))\n",
    "        test_split = np.concatenate((test_split, test_split1))\n",
    "    return train_split, test_split\n",
    "\n",
    "train_split, test_split = train_test_split([dframe, dframe2, dframe3, dframe4, dframe5, dframe6, dframe7])\n",
    "\n",
    "# #looking at split\n",
    "train_split[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.        ,  0.        ,  0.        , ..., -0.11680183,\n",
       "          0.05415827, -0.309518  ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.11314544,\n",
       "          0.05986664, -0.307342  ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.08895626,\n",
       "          0.06416318, -0.323287  ],\n",
       "        ..., \n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.07136566,\n",
       "          0.11679833, -0.255354  ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.010707  ,\n",
       "          0.15939193, -0.202112  ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.05328294,\n",
       "          0.09100411, -0.25367   ]]), 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEQ_LEN = None\n",
    "def Data_gen( train_split, SEQ_LEN):\n",
    "    while(True):\n",
    "        X = train_split\n",
    "        databatch = random.sample(list(X), 1)[0]\n",
    "#         print(databatch)\n",
    "        databatch, label = databatch[0], databatch[1]\n",
    "        if SEQ_LEN is not None:\n",
    "            if len(databatch) > SEQ_LEN:\n",
    "                databatch = databatch[0:SEQ_LEN]\n",
    "            elif len(databatch) < SEQ_LEN:\n",
    "                databatch = np.concatenate((databatch, np.zeros((SEQ_LEN - len(databatch), 75))))\n",
    "            else:\n",
    "                pass\n",
    "            yield databatch,label\n",
    "        else:\n",
    "            yield databatch,label\n",
    "\n",
    "ACTd = Data_gen(train_split, SEQ_LEN)\n",
    "\n",
    "#to look at batch created by Actd\n",
    "next(ACTd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Classifier model defination and intialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action LSTM\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, joints_dim, hidden_dim, label_size, batch_size, num_layers):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(joints_dim, hidden_dim, num_layers=self.num_layers)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()),\n",
    "                autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()))\n",
    "    \n",
    "    def forward(self, joints3d_vec):\n",
    "        x = joints3d_vec\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y  = self.hidden2label(lstm_out[-1])\n",
    "        log_probs = F.log_softmax(y, dim=1)\n",
    "        return log_probs\n",
    "#instanstiating a model\n",
    "model0 = LSTMClassifier(75, 512, 7, 1, 2)\n",
    "#to do stuff in CUDA\n",
    "model0 = model0.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, test_split):\n",
    "    pred_labels = np.empty(len(test_split))\n",
    "    orig_labels = np.array([t[1] for t in test_split])\n",
    "    for i in range(len(test_split)):\n",
    "        d_in = autograd.Variable(torch.from_numpy(test_split[i][0]).float().cuda())\n",
    "        d_in = d_in.view(d_in.size()[0], 1, -1)\n",
    "        y_pred = model(d_in)\n",
    "        pred_labels[i] = y_pred.data.cpu().max(1)[1].numpy()[0];\n",
    "    n_samples = len(pred_labels)\n",
    "    res=(orig_labels==pred_labels)\n",
    "    correct_count = (res==True).sum()\n",
    "    return (correct_count*100/n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training ...\n",
      "epoch 0 starting ...\n",
      "epoch: 0 iterations: 0 loss :0.00144365\n",
      "epoch: 0 iterations: 100 loss :0.406116\n",
      "epoch: 0 iterations: 200 loss :1.5135\n",
      "epoch: 0 iterations: 300 loss :0.0420057\n",
      "epoch: 0 iterations: 400 loss :0.00206448\n",
      "epoch: 0 iterations: 500 loss :0.00557712\n",
      "epoch: 0 iterations: 600 loss :0.00609847\n",
      "epoch: 0 iterations: 700 loss :0.021281\n",
      "epoch: 0 iterations: 800 loss :0.793979\n",
      "epoch: 0 iterations: 900 loss :0.455785\n",
      "epoch: 0 <====train track===> avg_loss: 0.007982276739164434, accuracy: 79.04761904761905% \n",
      "\n",
      "epoch 1 starting ...\n",
      "epoch: 1 iterations: 0 loss :0.628457\n",
      "epoch: 1 iterations: 100 loss :0.00117793\n",
      "epoch: 1 iterations: 200 loss :0.0409406\n",
      "epoch: 1 iterations: 300 loss :0.0579557\n",
      "epoch: 1 iterations: 400 loss :2.05543\n",
      "epoch: 1 iterations: 500 loss :3.40309\n",
      "epoch: 1 iterations: 600 loss :0.225574\n",
      "epoch: 1 iterations: 700 loss :0.00146675\n",
      "epoch: 1 iterations: 800 loss :1.68586\n",
      "epoch: 1 iterations: 900 loss :0.0406249\n",
      "epoch: 1 <====train track===> avg_loss: 0.007865390043964889, accuracy: 80.0% \n",
      "\n",
      "epoch 2 starting ...\n",
      "epoch: 2 iterations: 0 loss :0.293715\n",
      "epoch: 2 iterations: 100 loss :0.0206371\n",
      "epoch: 2 iterations: 200 loss :0.615629\n",
      "epoch: 2 iterations: 300 loss :0.529977\n",
      "epoch: 2 iterations: 400 loss :0.0205852\n",
      "epoch: 2 iterations: 500 loss :0.268213\n",
      "epoch: 2 iterations: 600 loss :0.198984\n",
      "epoch: 2 iterations: 700 loss :2.00203\n",
      "epoch: 2 iterations: 800 loss :0.0785943\n",
      "epoch: 2 iterations: 900 loss :0.00279655\n",
      "epoch: 2 <====train track===> avg_loss: 0.0076131824120875165, accuracy: 78.0952380952381% \n",
      "\n",
      "epoch 3 starting ...\n",
      "epoch: 3 iterations: 0 loss :0.0079482\n",
      "epoch: 3 iterations: 100 loss :0.451758\n",
      "epoch: 3 iterations: 200 loss :0.563729\n",
      "epoch: 3 iterations: 300 loss :0.257514\n",
      "epoch: 3 iterations: 400 loss :0.00107481\n",
      "epoch: 3 iterations: 500 loss :1.23529\n",
      "epoch: 3 iterations: 600 loss :0.00246531\n",
      "epoch: 3 iterations: 700 loss :1.08194\n",
      "epoch: 3 iterations: 800 loss :0.802629\n",
      "epoch: 3 iterations: 900 loss :0.568258\n",
      "epoch: 3 <====train track===> avg_loss: 0.00820292693824606, accuracy: 80.95238095238095% \n",
      "\n",
      "epoch 4 starting ...\n",
      "epoch: 4 iterations: 0 loss :0.642854\n",
      "epoch: 4 iterations: 100 loss :0.804139\n",
      "epoch: 4 iterations: 200 loss :0.0726788\n",
      "epoch: 4 iterations: 300 loss :0.104763\n",
      "epoch: 4 iterations: 400 loss :0.180555\n",
      "epoch: 4 iterations: 500 loss :0.497079\n",
      "epoch: 4 iterations: 600 loss :0.0970922\n",
      "epoch: 4 iterations: 700 loss :0.0930345\n",
      "epoch: 4 iterations: 800 loss :0.00204449\n",
      "epoch: 4 iterations: 900 loss :0.0260261\n",
      "epoch: 4 <====train track===> avg_loss: 0.0071738323466424845, accuracy: 80.0% \n",
      "\n",
      "epoch 5 starting ...\n",
      "epoch: 5 iterations: 0 loss :0.0550377\n",
      "epoch: 5 iterations: 100 loss :0.00524122\n",
      "epoch: 5 iterations: 200 loss :0.222071\n",
      "epoch: 5 iterations: 300 loss :0.115913\n",
      "epoch: 5 iterations: 400 loss :0.00301062\n",
      "epoch: 5 iterations: 500 loss :0.0042687\n",
      "epoch: 5 iterations: 600 loss :2.57594\n",
      "epoch: 5 iterations: 700 loss :0.280622\n",
      "epoch: 5 iterations: 800 loss :0.00184568\n",
      "epoch: 5 iterations: 900 loss :0.694907\n",
      "epoch: 5 <====train track===> avg_loss: 0.007624736432123838, accuracy: 78.0952380952381% \n",
      "\n",
      "epoch 6 starting ...\n",
      "epoch: 6 iterations: 0 loss :0.00542241\n",
      "epoch: 6 iterations: 100 loss :0.0308918\n",
      "epoch: 6 iterations: 200 loss :0.23829\n",
      "epoch: 6 iterations: 300 loss :0.0258248\n",
      "epoch: 6 iterations: 400 loss :4.77164\n",
      "epoch: 6 iterations: 500 loss :0.752696\n",
      "epoch: 6 iterations: 600 loss :0.172811\n",
      "epoch: 6 iterations: 700 loss :0.0124608\n",
      "epoch: 6 iterations: 800 loss :4.86552\n",
      "epoch: 6 iterations: 900 loss :0.0775579\n",
      "epoch: 6 <====train track===> avg_loss: 0.007588909584082543, accuracy: 70.47619047619048% \n",
      "\n",
      "epoch 7 starting ...\n",
      "epoch: 7 iterations: 0 loss :1.82532\n",
      "epoch: 7 iterations: 100 loss :0.057974\n",
      "epoch: 7 iterations: 200 loss :1.40744\n",
      "epoch: 7 iterations: 300 loss :0.251939\n",
      "epoch: 7 iterations: 400 loss :0.319106\n",
      "epoch: 7 iterations: 500 loss :0.713421\n",
      "epoch: 7 iterations: 600 loss :1.73859\n",
      "epoch: 7 iterations: 700 loss :2.71061\n",
      "epoch: 7 iterations: 800 loss :0.708343\n",
      "epoch: 7 iterations: 900 loss :3.24355\n",
      "epoch: 7 <====train track===> avg_loss: 0.0071602933001435635, accuracy: 79.04761904761905% \n",
      "\n",
      "epoch 8 starting ...\n",
      "epoch: 8 iterations: 0 loss :2.26294\n",
      "epoch: 8 iterations: 100 loss :1.27137\n",
      "epoch: 8 iterations: 200 loss :0.899063\n",
      "epoch: 8 iterations: 300 loss :0.00893795\n",
      "epoch: 8 iterations: 400 loss :0.00074752\n",
      "epoch: 8 iterations: 500 loss :0.0208029\n",
      "epoch: 8 iterations: 600 loss :1.65086\n",
      "epoch: 8 iterations: 700 loss :1.17361\n",
      "epoch: 8 iterations: 800 loss :1.56366\n",
      "epoch: 8 iterations: 900 loss :0.535516\n",
      "epoch: 8 <====train track===> avg_loss: 0.007813904524904425, accuracy: 80.95238095238095% \n",
      "\n",
      "epoch 9 starting ...\n",
      "epoch: 9 iterations: 0 loss :0.310652\n",
      "epoch: 9 iterations: 100 loss :0.371717\n",
      "epoch: 9 iterations: 200 loss :0.439389\n",
      "epoch: 9 iterations: 300 loss :2.48756\n",
      "epoch: 9 iterations: 400 loss :0.173886\n",
      "epoch: 9 iterations: 500 loss :2.1728\n",
      "epoch: 9 iterations: 600 loss :0.283671\n",
      "epoch: 9 iterations: 700 loss :1.6031\n",
      "epoch: 9 iterations: 800 loss :0.0169803\n",
      "epoch: 9 iterations: 900 loss :0.193148\n",
      "epoch: 9 <====train track===> avg_loss: 0.007448714486886923, accuracy: 80.95238095238095% \n",
      "\n",
      "epoch 10 starting ...\n",
      "epoch: 10 iterations: 0 loss :0.432574\n",
      "epoch: 10 iterations: 100 loss :0.00396098\n",
      "epoch: 10 iterations: 200 loss :0.00790988\n",
      "epoch: 10 iterations: 300 loss :0.862184\n",
      "epoch: 10 iterations: 400 loss :0.0475368\n",
      "epoch: 10 iterations: 500 loss :0.802527\n",
      "epoch: 10 iterations: 600 loss :0.00559953\n",
      "epoch: 10 iterations: 700 loss :1.20605\n",
      "epoch: 10 iterations: 800 loss :0.0237067\n",
      "epoch: 10 iterations: 900 loss :0.0350236\n",
      "epoch: 10 <====train track===> avg_loss: 0.00744129128386339, accuracy: 77.14285714285714% \n",
      "\n",
      "epoch 11 starting ...\n",
      "epoch: 11 iterations: 0 loss :0.00147591\n",
      "epoch: 11 iterations: 100 loss :0.007318\n",
      "epoch: 11 iterations: 200 loss :0.352445\n",
      "epoch: 11 iterations: 300 loss :0.142693\n",
      "epoch: 11 iterations: 400 loss :0.107485\n",
      "epoch: 11 iterations: 500 loss :0.0678486\n",
      "epoch: 11 iterations: 600 loss :0.132207\n",
      "epoch: 11 iterations: 700 loss :0.0235899\n",
      "epoch: 11 iterations: 800 loss :0.329569\n",
      "epoch: 11 iterations: 900 loss :1.00055\n",
      "epoch: 11 <====train track===> avg_loss: 0.007573307490577343, accuracy: 78.0952380952381% \n",
      "\n",
      "epoch 12 starting ...\n",
      "epoch: 12 iterations: 0 loss :0.0212207\n",
      "epoch: 12 iterations: 100 loss :1.27195\n",
      "epoch: 12 iterations: 200 loss :0.00178607\n",
      "epoch: 12 iterations: 300 loss :0.150535\n",
      "epoch: 12 iterations: 400 loss :0.0271871\n",
      "epoch: 12 iterations: 500 loss :0.0995469\n",
      "epoch: 12 iterations: 600 loss :0.00624894\n",
      "epoch: 12 iterations: 700 loss :0.00377811\n",
      "epoch: 12 iterations: 800 loss :3.59387\n",
      "epoch: 12 iterations: 900 loss :0.0504362\n",
      "epoch: 12 <====train track===> avg_loss: 0.007399008083941986, accuracy: 77.14285714285714% \n",
      "\n",
      "epoch 13 starting ...\n",
      "epoch: 13 iterations: 0 loss :0.890339\n",
      "epoch: 13 iterations: 100 loss :0.00280249\n",
      "epoch: 13 iterations: 200 loss :0.00552555\n",
      "epoch: 13 iterations: 300 loss :1.5706\n",
      "epoch: 13 iterations: 400 loss :0.427224\n",
      "epoch: 13 iterations: 500 loss :0.50511\n",
      "epoch: 13 iterations: 600 loss :0.601881\n",
      "epoch: 13 iterations: 700 loss :2.00955\n",
      "epoch: 13 iterations: 800 loss :0.745285\n",
      "epoch: 13 iterations: 900 loss :1.29334\n",
      "epoch: 13 <====train track===> avg_loss: 0.007577545375361352, accuracy: 81.9047619047619% \n",
      "\n",
      "epoch 14 starting ...\n",
      "epoch: 14 iterations: 0 loss :0.0306504\n",
      "epoch: 14 iterations: 100 loss :1.86065\n",
      "epoch: 14 iterations: 200 loss :0.0125843\n",
      "epoch: 14 iterations: 300 loss :0.200851\n",
      "epoch: 14 iterations: 400 loss :1.20401\n",
      "epoch: 14 iterations: 500 loss :2.25426\n",
      "epoch: 14 iterations: 600 loss :0.0221589\n",
      "epoch: 14 iterations: 700 loss :0.612573\n",
      "epoch: 14 iterations: 800 loss :1.19051\n",
      "epoch: 14 iterations: 900 loss :0.208381\n",
      "epoch: 14 <====train track===> avg_loss: 0.007289196389368632, accuracy: 79.04761904761905% \n",
      "\n",
      "epoch 15 starting ...\n",
      "epoch: 15 iterations: 0 loss :0.000645667\n",
      "epoch: 15 iterations: 100 loss :0.483043\n",
      "epoch: 15 iterations: 200 loss :0.299418\n",
      "epoch: 15 iterations: 300 loss :0.189855\n",
      "epoch: 15 iterations: 400 loss :2.86091\n",
      "epoch: 15 iterations: 500 loss :2.94092\n",
      "epoch: 15 iterations: 600 loss :1.07947\n",
      "epoch: 15 iterations: 700 loss :0.0210273\n",
      "epoch: 15 iterations: 800 loss :2.56311\n",
      "epoch: 15 iterations: 900 loss :0.476115\n",
      "epoch: 15 <====train track===> avg_loss: 0.007055276973488662, accuracy: 78.0952380952381% \n",
      "\n",
      "epoch 16 starting ...\n",
      "epoch: 16 iterations: 0 loss :0.001549\n",
      "epoch: 16 iterations: 100 loss :0.0005332\n",
      "epoch: 16 iterations: 200 loss :2.06398\n",
      "epoch: 16 iterations: 300 loss :0.13174\n",
      "epoch: 16 iterations: 400 loss :2.42091\n",
      "epoch: 16 iterations: 500 loss :0.00137509\n",
      "epoch: 16 iterations: 600 loss :0.0028082\n",
      "epoch: 16 iterations: 700 loss :0.0201247\n",
      "epoch: 16 iterations: 800 loss :1.43501\n",
      "epoch: 16 iterations: 900 loss :0.0621524\n",
      "epoch: 16 <====train track===> avg_loss: 0.007242481737035679, accuracy: 76.19047619047619% \n",
      "\n",
      "epoch 17 starting ...\n",
      "epoch: 17 iterations: 0 loss :0.0143534\n",
      "epoch: 17 iterations: 100 loss :0.0276522\n",
      "epoch: 17 iterations: 200 loss :0.051011\n",
      "epoch: 17 iterations: 300 loss :0.0404913\n",
      "epoch: 17 iterations: 400 loss :0.20343\n",
      "epoch: 17 iterations: 500 loss :0.249459\n",
      "epoch: 17 iterations: 600 loss :0.444381\n",
      "epoch: 17 iterations: 700 loss :0.0103251\n",
      "epoch: 17 iterations: 800 loss :0.227692\n",
      "epoch: 17 iterations: 900 loss :0.202939\n",
      "epoch: 17 <====train track===> avg_loss: 0.0070915067693876135, accuracy: 80.0% \n",
      "\n",
      "epoch 18 starting ...\n",
      "epoch: 18 iterations: 0 loss :0.02397\n",
      "epoch: 18 iterations: 100 loss :0.155625\n",
      "epoch: 18 iterations: 200 loss :0.749994\n",
      "epoch: 18 iterations: 300 loss :0.847275\n",
      "epoch: 18 iterations: 400 loss :0.00116388\n",
      "epoch: 18 iterations: 500 loss :0.451783\n",
      "epoch: 18 iterations: 600 loss :0.609693\n",
      "epoch: 18 iterations: 700 loss :0.0278167\n",
      "epoch: 18 iterations: 800 loss :0.607997\n",
      "epoch: 18 iterations: 900 loss :0.421336\n",
      "epoch: 18 <====train track===> avg_loss: 0.006840008828475875, accuracy: 81.9047619047619% \n",
      "\n",
      "epoch 19 starting ...\n",
      "epoch: 19 iterations: 0 loss :0.269146\n",
      "epoch: 19 iterations: 100 loss :0.0289603\n",
      "epoch: 19 iterations: 200 loss :0.0401424\n",
      "epoch: 19 iterations: 300 loss :1.50134\n",
      "epoch: 19 iterations: 400 loss :0.012296\n",
      "epoch: 19 iterations: 500 loss :0.120432\n",
      "epoch: 19 iterations: 600 loss :0.964337\n",
      "epoch: 19 iterations: 700 loss :0.373973\n",
      "epoch: 19 iterations: 800 loss :0.897089\n",
      "epoch: 19 iterations: 900 loss :0.600531\n",
      "epoch: 19 <====train track===> avg_loss: 0.006779617087408177, accuracy: 77.14285714285714% \n",
      "\n",
      "epoch 20 starting ...\n",
      "epoch: 20 iterations: 0 loss :0.00100383\n",
      "epoch: 20 iterations: 100 loss :0.973374\n",
      "epoch: 20 iterations: 200 loss :0.341683\n",
      "epoch: 20 iterations: 300 loss :2.67909\n",
      "epoch: 20 iterations: 400 loss :4.09706\n",
      "epoch: 20 iterations: 500 loss :0.335692\n",
      "epoch: 20 iterations: 600 loss :3.18868\n",
      "epoch: 20 iterations: 700 loss :0.540518\n",
      "epoch: 20 iterations: 800 loss :0.00218141\n",
      "epoch: 20 iterations: 900 loss :0.752848\n",
      "epoch: 20 <====train track===> avg_loss: 0.007067911677640895, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 21 starting ...\n",
      "epoch: 21 iterations: 0 loss :1.33674\n",
      "epoch: 21 iterations: 100 loss :0.623354\n",
      "epoch: 21 iterations: 200 loss :0.198289\n",
      "epoch: 21 iterations: 300 loss :0.0641915\n",
      "epoch: 21 iterations: 400 loss :0.54031\n",
      "epoch: 21 iterations: 500 loss :0.261398\n",
      "epoch: 21 iterations: 600 loss :0.680811\n",
      "epoch: 21 iterations: 700 loss :0.51065\n",
      "epoch: 21 iterations: 800 loss :0.0140909\n",
      "epoch: 21 iterations: 900 loss :0.0536477\n",
      "epoch: 21 <====train track===> avg_loss: 0.006780185034673764, accuracy: 78.0952380952381% \n",
      "\n",
      "epoch 22 starting ...\n",
      "epoch: 22 iterations: 0 loss :3.58539\n",
      "epoch: 22 iterations: 100 loss :0.226019\n",
      "epoch: 22 iterations: 200 loss :0.88046\n",
      "epoch: 22 iterations: 300 loss :0.594924\n",
      "epoch: 22 iterations: 400 loss :0.0262775\n",
      "epoch: 22 iterations: 500 loss :0.60797\n",
      "epoch: 22 iterations: 600 loss :0.212472\n",
      "epoch: 22 iterations: 700 loss :0.369613\n",
      "epoch: 22 iterations: 800 loss :0.00813386\n",
      "epoch: 22 iterations: 900 loss :0.45443\n",
      "epoch: 22 <====train track===> avg_loss: 0.006793210620341697, accuracy: 80.0% \n",
      "\n",
      "epoch 23 starting ...\n",
      "epoch: 23 iterations: 0 loss :0.988401\n",
      "epoch: 23 iterations: 100 loss :0.150917\n",
      "epoch: 23 iterations: 200 loss :0.330246\n",
      "epoch: 23 iterations: 300 loss :0.0571248\n",
      "epoch: 23 iterations: 400 loss :0.0903316\n",
      "epoch: 23 iterations: 500 loss :1.23836\n",
      "epoch: 23 iterations: 600 loss :1.84803\n",
      "epoch: 23 iterations: 700 loss :2.27681\n",
      "epoch: 23 iterations: 800 loss :0.00798238\n",
      "epoch: 23 iterations: 900 loss :0.0150637\n",
      "epoch: 23 <====train track===> avg_loss: 0.006487695905733563, accuracy: 80.95238095238095% \n",
      "\n",
      "epoch 24 starting ...\n",
      "epoch: 24 iterations: 0 loss :0.0110631\n",
      "epoch: 24 iterations: 100 loss :0.162874\n",
      "epoch: 24 iterations: 200 loss :0.508832\n",
      "epoch: 24 iterations: 300 loss :0.00120924\n",
      "epoch: 24 iterations: 400 loss :0.347393\n",
      "epoch: 24 iterations: 500 loss :1.82569\n",
      "epoch: 24 iterations: 600 loss :0.146102\n",
      "epoch: 24 iterations: 700 loss :0.222813\n",
      "epoch: 24 iterations: 800 loss :0.110427\n",
      "epoch: 24 iterations: 900 loss :1.16538\n",
      "epoch: 24 <====train track===> avg_loss: 0.006906702560596965, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 25 starting ...\n",
      "epoch: 25 iterations: 0 loss :0.00470875\n",
      "epoch: 25 iterations: 100 loss :0.505033\n",
      "epoch: 25 iterations: 200 loss :0.00142568\n",
      "epoch: 25 iterations: 300 loss :0.000999785\n",
      "epoch: 25 iterations: 400 loss :0.0494739\n",
      "epoch: 25 iterations: 500 loss :0.000742636\n",
      "epoch: 25 iterations: 600 loss :0.00465203\n",
      "epoch: 25 iterations: 700 loss :0.0103649\n",
      "epoch: 25 iterations: 800 loss :0.115918\n",
      "epoch: 25 iterations: 900 loss :0.106791\n",
      "epoch: 25 <====train track===> avg_loss: 0.006343133440836399, accuracy: 79.04761904761905% \n",
      "\n",
      "epoch 26 starting ...\n",
      "epoch: 26 iterations: 0 loss :0.297213\n",
      "epoch: 26 iterations: 100 loss :0.0268879\n",
      "epoch: 26 iterations: 200 loss :0.166698\n",
      "epoch: 26 iterations: 300 loss :0.698108\n",
      "epoch: 26 iterations: 400 loss :1.78396\n",
      "epoch: 26 iterations: 500 loss :0.345181\n",
      "epoch: 26 iterations: 600 loss :0.0189249\n",
      "epoch: 26 iterations: 700 loss :1.74714\n",
      "epoch: 26 iterations: 800 loss :0.677711\n",
      "epoch: 26 iterations: 900 loss :0.0430671\n",
      "epoch: 26 <====train track===> avg_loss: 0.0064247712261781345, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 27 starting ...\n",
      "epoch: 27 iterations: 0 loss :0.233887\n",
      "epoch: 27 iterations: 100 loss :0.447562\n",
      "epoch: 27 iterations: 200 loss :0.00123353\n",
      "epoch: 27 iterations: 300 loss :0.175966\n",
      "epoch: 27 iterations: 400 loss :0.0175226\n",
      "epoch: 27 iterations: 500 loss :1.43188\n",
      "epoch: 27 iterations: 600 loss :0.10634\n",
      "epoch: 27 iterations: 700 loss :0.69208\n",
      "epoch: 27 iterations: 800 loss :0.11275\n",
      "epoch: 27 iterations: 900 loss :0.453164\n",
      "epoch: 27 <====train track===> avg_loss: 0.0067823147935291, accuracy: 81.9047619047619% \n",
      "\n",
      "epoch 28 starting ...\n",
      "epoch: 28 iterations: 0 loss :0.837278\n",
      "epoch: 28 iterations: 100 loss :0.0716791\n",
      "epoch: 28 iterations: 200 loss :0.516874\n",
      "epoch: 28 iterations: 300 loss :0.169305\n",
      "epoch: 28 iterations: 400 loss :0.00159744\n",
      "epoch: 28 iterations: 500 loss :0.526127\n",
      "epoch: 28 iterations: 600 loss :1.0702\n",
      "epoch: 28 iterations: 700 loss :2.23751\n",
      "epoch: 28 iterations: 800 loss :0.0011815\n",
      "epoch: 28 iterations: 900 loss :0.000814106\n",
      "epoch: 28 <====train track===> avg_loss: 0.0068291425774802276, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 29 starting ...\n",
      "epoch: 29 iterations: 0 loss :0.301209\n",
      "epoch: 29 iterations: 100 loss :0.0245904\n",
      "epoch: 29 iterations: 200 loss :0.000749426\n",
      "epoch: 29 iterations: 300 loss :0.870664\n",
      "epoch: 29 iterations: 400 loss :0.444251\n",
      "epoch: 29 iterations: 500 loss :0.0111033\n",
      "epoch: 29 iterations: 600 loss :1.60998\n",
      "epoch: 29 iterations: 700 loss :2.32732\n",
      "epoch: 29 iterations: 800 loss :0.0168852\n",
      "epoch: 29 iterations: 900 loss :0.372021\n",
      "epoch: 29 <====train track===> avg_loss: 0.0064902789210387404, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 30 starting ...\n",
      "epoch: 30 iterations: 0 loss :0.0128074\n",
      "epoch: 30 iterations: 100 loss :0.082795\n",
      "epoch: 30 iterations: 200 loss :2.76443\n",
      "epoch: 30 iterations: 300 loss :1.67476\n",
      "epoch: 30 iterations: 400 loss :0.0287242\n",
      "epoch: 30 iterations: 500 loss :1.6713\n",
      "epoch: 30 iterations: 600 loss :0.0963477\n",
      "epoch: 30 iterations: 700 loss :2.88536\n",
      "epoch: 30 iterations: 800 loss :0.454871\n",
      "epoch: 30 iterations: 900 loss :0.762894\n",
      "epoch: 30 <====train track===> avg_loss: 0.006797739730693701, accuracy: 81.9047619047619% \n",
      "\n",
      "epoch 31 starting ...\n",
      "epoch: 31 iterations: 0 loss :0.407322\n",
      "epoch: 31 iterations: 100 loss :0.189218\n",
      "epoch: 31 iterations: 200 loss :0.154052\n",
      "epoch: 31 iterations: 300 loss :0.0206943\n",
      "epoch: 31 iterations: 400 loss :0.0464218\n",
      "epoch: 31 iterations: 500 loss :1.97069\n",
      "epoch: 31 iterations: 600 loss :0.0998256\n",
      "epoch: 31 iterations: 700 loss :1.13698\n",
      "epoch: 31 iterations: 800 loss :0.0218569\n",
      "epoch: 31 iterations: 900 loss :1.28686\n",
      "epoch: 31 <====train track===> avg_loss: 0.006779599992428341, accuracy: 80.95238095238095% \n",
      "\n",
      "epoch 32 starting ...\n",
      "epoch: 32 iterations: 0 loss :0.256205\n",
      "epoch: 32 iterations: 100 loss :0.729148\n",
      "epoch: 32 iterations: 200 loss :0.0957024\n",
      "epoch: 32 iterations: 300 loss :0.00118995\n",
      "epoch: 32 iterations: 400 loss :2.06644\n",
      "epoch: 32 iterations: 500 loss :0.347954\n",
      "epoch: 32 iterations: 600 loss :0.0361995\n",
      "epoch: 32 iterations: 700 loss :0.000897601\n",
      "epoch: 32 iterations: 800 loss :0.628346\n",
      "epoch: 32 iterations: 900 loss :0.0405517\n",
      "epoch: 32 <====train track===> avg_loss: 0.006658493770076009, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 33 starting ...\n",
      "epoch: 33 iterations: 0 loss :0.143092\n",
      "epoch: 33 iterations: 100 loss :0.551163\n",
      "epoch: 33 iterations: 200 loss :0.418655\n",
      "epoch: 33 iterations: 300 loss :2.92342\n",
      "epoch: 33 iterations: 400 loss :0.0435973\n",
      "epoch: 33 iterations: 500 loss :0.286257\n",
      "epoch: 33 iterations: 600 loss :0.00165421\n",
      "epoch: 33 iterations: 700 loss :0.867744\n",
      "epoch: 33 iterations: 800 loss :2.87375\n",
      "epoch: 33 iterations: 900 loss :1.49799\n",
      "epoch: 33 <====train track===> avg_loss: 0.006416507870025974, accuracy: 81.9047619047619% \n",
      "\n",
      "epoch 34 starting ...\n",
      "epoch: 34 iterations: 0 loss :0.122101\n",
      "epoch: 34 iterations: 100 loss :0.00342188\n",
      "epoch: 34 iterations: 200 loss :0.763262\n",
      "epoch: 34 iterations: 300 loss :3.17256\n",
      "epoch: 34 iterations: 400 loss :0.00991232\n",
      "epoch: 34 iterations: 500 loss :0.370111\n",
      "epoch: 34 iterations: 600 loss :0.666469\n",
      "epoch: 34 iterations: 700 loss :0.26593\n",
      "epoch: 34 iterations: 800 loss :0.135598\n",
      "epoch: 34 iterations: 900 loss :3.17598\n",
      "epoch: 34 <====train track===> avg_loss: 0.006307837178339768, accuracy: 80.95238095238095% \n",
      "\n",
      "epoch 35 starting ...\n",
      "epoch: 35 iterations: 0 loss :0.145608\n",
      "epoch: 35 iterations: 100 loss :0.331413\n",
      "epoch: 35 iterations: 200 loss :1.43968\n",
      "epoch: 35 iterations: 300 loss :0.282806\n",
      "epoch: 35 iterations: 400 loss :0.463837\n",
      "epoch: 35 iterations: 500 loss :0.0990357\n",
      "epoch: 35 iterations: 600 loss :1.08953\n",
      "epoch: 35 iterations: 700 loss :0.0568723\n",
      "epoch: 35 iterations: 800 loss :0.210507\n",
      "epoch: 35 iterations: 900 loss :0.320704\n",
      "epoch: 35 <====train track===> avg_loss: 0.006585602168420709, accuracy: 81.9047619047619% \n",
      "\n",
      "epoch 36 starting ...\n",
      "epoch: 36 iterations: 0 loss :0.0237784\n",
      "epoch: 36 iterations: 100 loss :3.08865\n",
      "epoch: 36 iterations: 200 loss :0.665935\n",
      "epoch: 36 iterations: 300 loss :0.0225843\n",
      "epoch: 36 iterations: 400 loss :0.685152\n",
      "epoch: 36 iterations: 500 loss :1.24516\n",
      "epoch: 36 iterations: 600 loss :0.0111576\n",
      "epoch: 36 iterations: 700 loss :0.761618\n",
      "epoch: 36 iterations: 800 loss :2.72187\n",
      "epoch: 36 iterations: 900 loss :1.33917\n",
      "epoch: 36 <====train track===> avg_loss: 0.006557308298007943, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 37 starting ...\n",
      "epoch: 37 iterations: 0 loss :0.989756\n",
      "epoch: 37 iterations: 100 loss :0.00292053\n",
      "epoch: 37 iterations: 200 loss :0.274402\n",
      "epoch: 37 iterations: 300 loss :0.00864939\n",
      "epoch: 37 iterations: 400 loss :0.016656\n",
      "epoch: 37 iterations: 500 loss :0.0142852\n",
      "epoch: 37 iterations: 600 loss :5.59354\n",
      "epoch: 37 iterations: 700 loss :0.00478017\n",
      "epoch: 37 iterations: 800 loss :0.48754\n",
      "epoch: 37 iterations: 900 loss :0.230969\n",
      "epoch: 37 <====train track===> avg_loss: 0.006437483567695052, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 38 starting ...\n",
      "epoch: 38 iterations: 0 loss :1.46091\n",
      "epoch: 38 iterations: 100 loss :0.400027\n",
      "epoch: 38 iterations: 200 loss :0.00448495\n",
      "epoch: 38 iterations: 300 loss :0.0147424\n",
      "epoch: 38 iterations: 400 loss :1.26842\n",
      "epoch: 38 iterations: 500 loss :0.00724214\n",
      "epoch: 38 iterations: 600 loss :0.000474579\n",
      "epoch: 38 iterations: 700 loss :2.60579\n",
      "epoch: 38 iterations: 800 loss :0.149479\n",
      "epoch: 38 iterations: 900 loss :0.00182129\n",
      "epoch: 38 <====train track===> avg_loss: 0.006470214486767329, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 39 starting ...\n",
      "epoch: 39 iterations: 0 loss :0.0278757\n",
      "epoch: 39 iterations: 100 loss :0.0457471\n",
      "epoch: 39 iterations: 200 loss :0.00955638\n",
      "epoch: 39 iterations: 300 loss :1.16675\n",
      "epoch: 39 iterations: 400 loss :1.67465\n",
      "epoch: 39 iterations: 500 loss :0.292638\n",
      "epoch: 39 iterations: 600 loss :1.1793\n",
      "epoch: 39 iterations: 700 loss :0.0211976\n",
      "epoch: 39 iterations: 800 loss :0.0025338\n",
      "epoch: 39 iterations: 900 loss :0.0701897\n",
      "epoch: 39 <====train track===> avg_loss: 0.00621436922675223, accuracy: 81.9047619047619% \n",
      "\n",
      "epoch 40 starting ...\n",
      "epoch: 40 iterations: 0 loss :0.0642234\n",
      "epoch: 40 iterations: 100 loss :1.4115\n",
      "epoch: 40 iterations: 200 loss :0.00146163\n",
      "epoch: 40 iterations: 300 loss :1.5589\n",
      "epoch: 40 iterations: 400 loss :0.00147972\n",
      "epoch: 40 iterations: 500 loss :0.479304\n",
      "epoch: 40 iterations: 600 loss :0.517231\n",
      "epoch: 40 iterations: 700 loss :3.08788\n",
      "epoch: 40 iterations: 800 loss :0.014925\n",
      "epoch: 40 iterations: 900 loss :0.395711\n",
      "epoch: 40 <====train track===> avg_loss: 0.006675814949002192, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 41 starting ...\n",
      "epoch: 41 iterations: 0 loss :4.44209\n",
      "epoch: 41 iterations: 100 loss :0.000934998\n",
      "epoch: 41 iterations: 200 loss :0.622423\n",
      "epoch: 41 iterations: 300 loss :0.00479963\n",
      "epoch: 41 iterations: 400 loss :0.186601\n",
      "epoch: 41 iterations: 500 loss :0.104279\n",
      "epoch: 41 iterations: 600 loss :0.0221254\n",
      "epoch: 41 iterations: 700 loss :0.409759\n",
      "epoch: 41 iterations: 800 loss :1.09779\n",
      "epoch: 41 iterations: 900 loss :0.589539\n",
      "epoch: 41 <====train track===> avg_loss: 0.0062445807343549835, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 42 starting ...\n",
      "epoch: 42 iterations: 0 loss :1.11524\n",
      "epoch: 42 iterations: 100 loss :1.76901\n",
      "epoch: 42 iterations: 200 loss :0.00433956\n",
      "epoch: 42 iterations: 300 loss :0.0137194\n",
      "epoch: 42 iterations: 400 loss :0.0516304\n",
      "epoch: 42 iterations: 500 loss :1.89285\n",
      "epoch: 42 iterations: 600 loss :0.385296\n",
      "epoch: 42 iterations: 700 loss :0.107399\n",
      "epoch: 42 iterations: 800 loss :0.0195291\n",
      "epoch: 42 iterations: 900 loss :0.0011272\n",
      "epoch: 42 <====train track===> avg_loss: 0.006098767371489623, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 43 starting ...\n",
      "epoch: 43 iterations: 0 loss :0.0946304\n",
      "epoch: 43 iterations: 100 loss :0.0404447\n",
      "epoch: 43 iterations: 200 loss :0.00297628\n",
      "epoch: 43 iterations: 300 loss :0.206081\n",
      "epoch: 43 iterations: 400 loss :0.000472315\n",
      "epoch: 43 iterations: 500 loss :0.00038092\n",
      "epoch: 43 iterations: 600 loss :0.00148258\n",
      "epoch: 43 iterations: 700 loss :0.0195856\n",
      "epoch: 43 iterations: 800 loss :0.00139092\n",
      "epoch: 43 iterations: 900 loss :1.10763\n",
      "epoch: 43 <====train track===> avg_loss: 0.0062727457077874395, accuracy: 81.9047619047619% \n",
      "\n",
      "epoch 44 starting ...\n",
      "epoch: 44 iterations: 0 loss :0.0648213\n",
      "epoch: 44 iterations: 100 loss :0.0687232\n",
      "epoch: 44 iterations: 200 loss :0.0586362\n",
      "epoch: 44 iterations: 300 loss :0.404558\n",
      "epoch: 44 iterations: 400 loss :0.379306\n",
      "epoch: 44 iterations: 500 loss :0.496211\n",
      "epoch: 44 iterations: 600 loss :0.110635\n",
      "epoch: 44 iterations: 700 loss :0.480103\n",
      "epoch: 44 iterations: 800 loss :0.00151246\n",
      "epoch: 44 iterations: 900 loss :1.8491\n",
      "epoch: 44 <====train track===> avg_loss: 0.0057958236503723075, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 45 starting ...\n",
      "epoch: 45 iterations: 0 loss :0.575842\n",
      "epoch: 45 iterations: 100 loss :0.000804101\n",
      "epoch: 45 iterations: 200 loss :0.000967511\n",
      "epoch: 45 iterations: 300 loss :1.97794\n",
      "epoch: 45 iterations: 400 loss :0.0684815\n",
      "epoch: 45 iterations: 500 loss :1.31058\n",
      "epoch: 45 iterations: 600 loss :0.000454323\n",
      "epoch: 45 iterations: 700 loss :0.0696245\n",
      "epoch: 45 iterations: 800 loss :2.38565\n",
      "epoch: 45 iterations: 900 loss :0.858122\n",
      "epoch: 45 <====train track===> avg_loss: 0.006316407014305459, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 46 starting ...\n",
      "epoch: 46 iterations: 0 loss :0.000491498\n",
      "epoch: 46 iterations: 100 loss :0.0324728\n",
      "epoch: 46 iterations: 200 loss :0.000520213\n",
      "epoch: 46 iterations: 300 loss :2.13201\n",
      "epoch: 46 iterations: 400 loss :0.0802451\n",
      "epoch: 46 iterations: 500 loss :0.00183795\n",
      "epoch: 46 iterations: 600 loss :1.54697\n",
      "epoch: 46 iterations: 700 loss :3.36619\n",
      "epoch: 46 iterations: 800 loss :0.313016\n",
      "epoch: 46 iterations: 900 loss :0.00156542\n",
      "epoch: 46 <====train track===> avg_loss: 0.006785882059637454, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 47 starting ...\n",
      "epoch: 47 iterations: 0 loss :0.29939\n",
      "epoch: 47 iterations: 100 loss :0.0134828\n",
      "epoch: 47 iterations: 200 loss :1.27227\n",
      "epoch: 47 iterations: 300 loss :1.45325\n",
      "epoch: 47 iterations: 400 loss :0.00876744\n",
      "epoch: 47 iterations: 500 loss :1.45755\n",
      "epoch: 47 iterations: 600 loss :1.01021\n",
      "epoch: 47 iterations: 700 loss :0.624664\n",
      "epoch: 47 iterations: 800 loss :0.0367272\n",
      "epoch: 47 iterations: 900 loss :1.87991\n",
      "epoch: 47 <====train track===> avg_loss: 0.0057205501190913085, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 48 starting ...\n",
      "epoch: 48 iterations: 0 loss :0.00158137\n",
      "epoch: 48 iterations: 100 loss :0.66599\n",
      "epoch: 48 iterations: 200 loss :0.00133973\n",
      "epoch: 48 iterations: 300 loss :1.47326\n",
      "epoch: 48 iterations: 400 loss :0.00504672\n",
      "epoch: 48 iterations: 500 loss :0.0203278\n",
      "epoch: 48 iterations: 600 loss :0.00448946\n",
      "epoch: 48 iterations: 700 loss :0.107021\n",
      "epoch: 48 iterations: 800 loss :0.150563\n",
      "epoch: 48 iterations: 900 loss :0.00538281\n",
      "epoch: 48 <====train track===> avg_loss: 0.006109426086051423, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 49 starting ...\n",
      "epoch: 49 iterations: 0 loss :0.130016\n",
      "epoch: 49 iterations: 100 loss :0.00171824\n",
      "epoch: 49 iterations: 200 loss :0.492684\n",
      "epoch: 49 iterations: 300 loss :1.66135\n",
      "epoch: 49 iterations: 400 loss :0.0329311\n",
      "epoch: 49 iterations: 500 loss :0.69048\n",
      "epoch: 49 iterations: 600 loss :3.27465\n",
      "epoch: 49 iterations: 700 loss :4.3458\n",
      "epoch: 49 iterations: 800 loss :0.146154\n",
      "epoch: 49 iterations: 900 loss :0.757709\n",
      "epoch: 49 <====train track===> avg_loss: 0.00571894611377515, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 50 starting ...\n",
      "epoch: 50 iterations: 0 loss :0.000682002\n",
      "epoch: 50 iterations: 100 loss :0.136759\n",
      "epoch: 50 iterations: 200 loss :0.132322\n",
      "epoch: 50 iterations: 300 loss :0.0450268\n",
      "epoch: 50 iterations: 400 loss :0.157938\n",
      "epoch: 50 iterations: 500 loss :1.41208\n",
      "epoch: 50 iterations: 600 loss :0.172502\n",
      "epoch: 50 iterations: 700 loss :0.00800343\n",
      "epoch: 50 iterations: 800 loss :0.0197916\n",
      "epoch: 50 iterations: 900 loss :0.0263526\n",
      "epoch: 50 <====train track===> avg_loss: 0.005803989091881427, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 51 starting ...\n",
      "epoch: 51 iterations: 0 loss :0.798632\n",
      "epoch: 51 iterations: 100 loss :0.00661979\n",
      "epoch: 51 iterations: 200 loss :1.78062\n",
      "epoch: 51 iterations: 300 loss :0.334346\n",
      "epoch: 51 iterations: 400 loss :1.48575\n",
      "epoch: 51 iterations: 500 loss :0.212266\n",
      "epoch: 51 iterations: 600 loss :0.0219563\n",
      "epoch: 51 iterations: 700 loss :0.0323031\n",
      "epoch: 51 iterations: 800 loss :0.108204\n",
      "epoch: 51 iterations: 900 loss :0.014459\n",
      "epoch: 51 <====train track===> avg_loss: 0.005777541858661277, accuracy: 80.95238095238095% \n",
      "\n",
      "epoch 52 starting ...\n",
      "epoch: 52 iterations: 0 loss :0.0079521\n",
      "epoch: 52 iterations: 100 loss :0.566755\n",
      "epoch: 52 iterations: 200 loss :0.00196288\n",
      "epoch: 52 iterations: 300 loss :0.333692\n",
      "epoch: 52 iterations: 400 loss :0.0032698\n",
      "epoch: 52 iterations: 500 loss :0.015256\n",
      "epoch: 52 iterations: 600 loss :0.940097\n",
      "epoch: 52 iterations: 700 loss :1.49966\n",
      "epoch: 52 iterations: 800 loss :0.0151992\n",
      "epoch: 52 iterations: 900 loss :0.319219\n",
      "epoch: 52 <====train track===> avg_loss: 0.00601642487364881, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 53 starting ...\n",
      "epoch: 53 iterations: 0 loss :3.23917\n",
      "epoch: 53 iterations: 100 loss :0.0280933\n",
      "epoch: 53 iterations: 200 loss :0.0287636\n",
      "epoch: 53 iterations: 300 loss :0.00611624\n",
      "epoch: 53 iterations: 400 loss :0.00427749\n",
      "epoch: 53 iterations: 500 loss :0.090132\n",
      "epoch: 53 iterations: 600 loss :0.11129\n",
      "epoch: 53 iterations: 700 loss :0.247794\n",
      "epoch: 53 iterations: 800 loss :1.41889\n",
      "epoch: 53 iterations: 900 loss :0.671829\n",
      "epoch: 53 <====train track===> avg_loss: 0.006366730637304263, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 54 starting ...\n",
      "epoch: 54 iterations: 0 loss :0.601697\n",
      "epoch: 54 iterations: 100 loss :3.51397\n",
      "epoch: 54 iterations: 200 loss :1.41709\n",
      "epoch: 54 iterations: 300 loss :2.44381\n",
      "epoch: 54 iterations: 400 loss :0.260296\n",
      "epoch: 54 iterations: 500 loss :0.0181071\n",
      "epoch: 54 iterations: 600 loss :0.16895\n",
      "epoch: 54 iterations: 700 loss :5.04913\n",
      "epoch: 54 iterations: 800 loss :0.838044\n",
      "epoch: 54 iterations: 900 loss :0.0988909\n",
      "epoch: 54 <====train track===> avg_loss: 0.006007502664509682, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 55 starting ...\n",
      "epoch: 55 iterations: 0 loss :0.592921\n",
      "epoch: 55 iterations: 100 loss :0.184107\n",
      "epoch: 55 iterations: 200 loss :0.0134991\n",
      "epoch: 55 iterations: 300 loss :0.34054\n",
      "epoch: 55 iterations: 400 loss :1.49269\n",
      "epoch: 55 iterations: 500 loss :1.29512\n",
      "epoch: 55 iterations: 600 loss :2.30148\n",
      "epoch: 55 iterations: 700 loss :0.00479002\n",
      "epoch: 55 iterations: 800 loss :0.0353932\n",
      "epoch: 55 iterations: 900 loss :0.00081625\n",
      "epoch: 55 <====train track===> avg_loss: 0.005696152852460352, accuracy: 81.9047619047619% \n",
      "\n",
      "epoch 56 starting ...\n",
      "epoch: 56 iterations: 0 loss :0.0865292\n",
      "epoch: 56 iterations: 100 loss :0.00427404\n",
      "epoch: 56 iterations: 200 loss :0.446723\n",
      "epoch: 56 iterations: 300 loss :0.658735\n",
      "epoch: 56 iterations: 400 loss :0.685097\n",
      "epoch: 56 iterations: 500 loss :0.0021713\n",
      "epoch: 56 iterations: 600 loss :1.20268\n",
      "epoch: 56 iterations: 700 loss :0.00120758\n",
      "epoch: 56 iterations: 800 loss :0.636531\n",
      "epoch: 56 iterations: 900 loss :0.247578\n",
      "epoch: 56 <====train track===> avg_loss: 0.005665853277814436, accuracy: 80.95238095238095% \n",
      "\n",
      "epoch 57 starting ...\n",
      "epoch: 57 iterations: 0 loss :0.0412359\n",
      "epoch: 57 iterations: 100 loss :0.241183\n",
      "epoch: 57 iterations: 200 loss :0.757279\n",
      "epoch: 57 iterations: 300 loss :0.684524\n",
      "epoch: 57 iterations: 400 loss :1.43319\n",
      "epoch: 57 iterations: 500 loss :0.00120388\n",
      "epoch: 57 iterations: 600 loss :0.533588\n",
      "epoch: 57 iterations: 700 loss :0.40011\n",
      "epoch: 57 iterations: 800 loss :1.38677\n",
      "epoch: 57 iterations: 900 loss :0.763269\n",
      "epoch: 57 <====train track===> avg_loss: 0.005764426149496447, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 58 starting ...\n",
      "epoch: 58 iterations: 0 loss :0.533124\n",
      "epoch: 58 iterations: 100 loss :0.00697842\n",
      "epoch: 58 iterations: 200 loss :0.101074\n",
      "epoch: 58 iterations: 300 loss :0.0764497\n",
      "epoch: 58 iterations: 400 loss :2.13752\n",
      "epoch: 58 iterations: 500 loss :0.00235875\n",
      "epoch: 58 iterations: 600 loss :0.000262584\n",
      "epoch: 58 iterations: 700 loss :0.9121\n",
      "epoch: 58 iterations: 800 loss :2.04826\n",
      "epoch: 58 iterations: 900 loss :1.15789\n",
      "epoch: 58 <====train track===> avg_loss: 0.0058321041515137515, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 59 starting ...\n",
      "epoch: 59 iterations: 0 loss :0.528259\n",
      "epoch: 59 iterations: 100 loss :0.324982\n",
      "epoch: 59 iterations: 200 loss :0.000486494\n",
      "epoch: 59 iterations: 300 loss :0.11261\n",
      "epoch: 59 iterations: 400 loss :0.97966\n",
      "epoch: 59 iterations: 500 loss :0.0414731\n",
      "epoch: 59 iterations: 600 loss :0.194524\n",
      "epoch: 59 iterations: 700 loss :0.158684\n",
      "epoch: 59 iterations: 800 loss :0.30735\n",
      "epoch: 59 iterations: 900 loss :0.0676279\n",
      "epoch: 59 <====train track===> avg_loss: 0.005569836645820232, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 60 starting ...\n",
      "epoch: 60 iterations: 0 loss :0.00592393\n",
      "epoch: 60 iterations: 100 loss :0.225894\n",
      "epoch: 60 iterations: 200 loss :2.53701\n",
      "epoch: 60 iterations: 300 loss :0.338736\n",
      "epoch: 60 iterations: 400 loss :0.0593282\n",
      "epoch: 60 iterations: 500 loss :0.297174\n",
      "epoch: 60 iterations: 600 loss :1.26574\n",
      "epoch: 60 iterations: 700 loss :0.0236553\n",
      "epoch: 60 iterations: 800 loss :0.313479\n",
      "epoch: 60 iterations: 900 loss :0.117786\n",
      "epoch: 60 <====train track===> avg_loss: 0.00589235937666815, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 61 starting ...\n",
      "epoch: 61 iterations: 0 loss :0.0197342\n",
      "epoch: 61 iterations: 100 loss :0.483978\n",
      "epoch: 61 iterations: 200 loss :0.185358\n",
      "epoch: 61 iterations: 300 loss :0.0585459\n",
      "epoch: 61 iterations: 400 loss :1.27999\n",
      "epoch: 61 iterations: 500 loss :2.49955\n",
      "epoch: 61 iterations: 600 loss :0.113713\n",
      "epoch: 61 iterations: 700 loss :0.0397927\n",
      "epoch: 61 iterations: 800 loss :0.0720942\n",
      "epoch: 61 iterations: 900 loss :0.60981\n",
      "epoch: 61 <====train track===> avg_loss: 0.005990174931440131, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 62 starting ...\n",
      "epoch: 62 iterations: 0 loss :0.197898\n",
      "epoch: 62 iterations: 100 loss :0.532786\n",
      "epoch: 62 iterations: 200 loss :1.29377\n",
      "epoch: 62 iterations: 300 loss :0.0014627\n",
      "epoch: 62 iterations: 400 loss :0.108474\n",
      "epoch: 62 iterations: 500 loss :2.17146\n",
      "epoch: 62 iterations: 600 loss :0.00829383\n",
      "epoch: 62 iterations: 700 loss :0.000258174\n",
      "epoch: 62 iterations: 800 loss :1.51889\n",
      "epoch: 62 iterations: 900 loss :0.00248564\n",
      "epoch: 62 <====train track===> avg_loss: 0.005428630853692898, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 63 starting ...\n",
      "epoch: 63 iterations: 0 loss :0.0180393\n",
      "epoch: 63 iterations: 100 loss :0.1279\n",
      "epoch: 63 iterations: 200 loss :0.00642603\n",
      "epoch: 63 iterations: 300 loss :1.31588\n",
      "epoch: 63 iterations: 400 loss :0.135873\n",
      "epoch: 63 iterations: 500 loss :0.0232866\n",
      "epoch: 63 iterations: 600 loss :0.0642046\n",
      "epoch: 63 iterations: 700 loss :0.0544075\n",
      "epoch: 63 iterations: 800 loss :0.00599349\n",
      "epoch: 63 iterations: 900 loss :3.31112\n",
      "epoch: 63 <====train track===> avg_loss: 0.005266183551153787, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 64 starting ...\n",
      "epoch: 64 iterations: 0 loss :1.40039\n",
      "epoch: 64 iterations: 100 loss :0.258058\n",
      "epoch: 64 iterations: 200 loss :0.192086\n",
      "epoch: 64 iterations: 300 loss :0.43394\n",
      "epoch: 64 iterations: 400 loss :0.301388\n",
      "epoch: 64 iterations: 500 loss :0.687248\n",
      "epoch: 64 iterations: 600 loss :0.00127175\n",
      "epoch: 64 iterations: 700 loss :0.730625\n",
      "epoch: 64 iterations: 800 loss :0.881085\n",
      "epoch: 64 iterations: 900 loss :0.00350302\n",
      "epoch: 64 <====train track===> avg_loss: 0.0059412513453269795, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 65 starting ...\n",
      "epoch: 65 iterations: 0 loss :0.118118\n",
      "epoch: 65 iterations: 100 loss :0.0722147\n",
      "epoch: 65 iterations: 200 loss :0.150436\n",
      "epoch: 65 iterations: 300 loss :0.0863866\n",
      "epoch: 65 iterations: 400 loss :1.17611\n",
      "epoch: 65 iterations: 500 loss :0.00332434\n",
      "epoch: 65 iterations: 600 loss :0.0484216\n",
      "epoch: 65 iterations: 700 loss :0.00643669\n",
      "epoch: 65 iterations: 800 loss :0.000671638\n",
      "epoch: 65 iterations: 900 loss :1.07356\n",
      "epoch: 65 <====train track===> avg_loss: 0.0054131881112138526, accuracy: 87.61904761904762% \n",
      "\n",
      "epoch 66 starting ...\n",
      "epoch: 66 iterations: 0 loss :0.167286\n",
      "epoch: 66 iterations: 100 loss :0.176822\n",
      "epoch: 66 iterations: 200 loss :0.242562\n",
      "epoch: 66 iterations: 300 loss :0.000320145\n",
      "epoch: 66 iterations: 400 loss :0.0651323\n",
      "epoch: 66 iterations: 500 loss :0.137477\n",
      "epoch: 66 iterations: 600 loss :0.817735\n",
      "epoch: 66 iterations: 700 loss :0.0444912\n",
      "epoch: 66 iterations: 800 loss :0.0573379\n",
      "epoch: 66 iterations: 900 loss :0.0582333\n",
      "epoch: 66 <====train track===> avg_loss: 0.005228578058361808, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 67 starting ...\n",
      "epoch: 67 iterations: 0 loss :0.126019\n",
      "epoch: 67 iterations: 100 loss :0.468218\n",
      "epoch: 67 iterations: 200 loss :0.728643\n",
      "epoch: 67 iterations: 300 loss :0.00270358\n",
      "epoch: 67 iterations: 400 loss :0.0828522\n",
      "epoch: 67 iterations: 500 loss :0.286556\n",
      "epoch: 67 iterations: 600 loss :1.29042\n",
      "epoch: 67 iterations: 700 loss :1.85666\n",
      "epoch: 67 iterations: 800 loss :0.00221757\n",
      "epoch: 67 iterations: 900 loss :0.00182581\n",
      "epoch: 67 <====train track===> avg_loss: 0.005636689559904623, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 68 starting ...\n",
      "epoch: 68 iterations: 0 loss :0.00525059\n",
      "epoch: 68 iterations: 100 loss :1.84591\n",
      "epoch: 68 iterations: 200 loss :0.474501\n",
      "epoch: 68 iterations: 300 loss :0.0319019\n",
      "epoch: 68 iterations: 400 loss :0.162525\n",
      "epoch: 68 iterations: 500 loss :0.00523399\n",
      "epoch: 68 iterations: 600 loss :0.00123925\n",
      "epoch: 68 iterations: 700 loss :0.925959\n",
      "epoch: 68 iterations: 800 loss :0.0715994\n",
      "epoch: 68 iterations: 900 loss :0.164106\n",
      "epoch: 68 <====train track===> avg_loss: 0.0058849757626297, accuracy: 81.9047619047619% \n",
      "\n",
      "epoch 69 starting ...\n",
      "epoch: 69 iterations: 0 loss :0.000363522\n",
      "epoch: 69 iterations: 100 loss :0.00231582\n",
      "epoch: 69 iterations: 200 loss :0.00407223\n",
      "epoch: 69 iterations: 300 loss :0.0445843\n",
      "epoch: 69 iterations: 400 loss :0.292977\n",
      "epoch: 69 iterations: 500 loss :0.00311093\n",
      "epoch: 69 iterations: 600 loss :1.02378\n",
      "epoch: 69 iterations: 700 loss :0.00664027\n",
      "epoch: 69 iterations: 800 loss :0.000818633\n",
      "epoch: 69 iterations: 900 loss :0.209439\n",
      "epoch: 69 <====train track===> avg_loss: 0.005359941084660987, accuracy: 88.57142857142857% \n",
      "\n",
      "epoch 70 starting ...\n",
      "epoch: 70 iterations: 0 loss :0.124474\n",
      "epoch: 70 iterations: 100 loss :0.103863\n",
      "epoch: 70 iterations: 200 loss :0.000444909\n",
      "epoch: 70 iterations: 300 loss :0.00258897\n",
      "epoch: 70 iterations: 400 loss :0.338283\n",
      "epoch: 70 iterations: 500 loss :0.0470931\n",
      "epoch: 70 iterations: 600 loss :1.15089\n",
      "epoch: 70 iterations: 700 loss :0.00588008\n",
      "epoch: 70 iterations: 800 loss :1.60201\n",
      "epoch: 70 iterations: 900 loss :3.56824\n",
      "epoch: 70 <====train track===> avg_loss: 0.005692181361513771, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 71 starting ...\n",
      "epoch: 71 iterations: 0 loss :0.0070433\n",
      "epoch: 71 iterations: 100 loss :3.14748\n",
      "epoch: 71 iterations: 200 loss :0.0108772\n",
      "epoch: 71 iterations: 300 loss :0.00272249\n",
      "epoch: 71 iterations: 400 loss :4.30956\n",
      "epoch: 71 iterations: 500 loss :0.42389\n",
      "epoch: 71 iterations: 600 loss :0.00915236\n",
      "epoch: 71 iterations: 700 loss :0.313054\n",
      "epoch: 71 iterations: 800 loss :0.00419985\n",
      "epoch: 71 iterations: 900 loss :0.0669129\n",
      "epoch: 71 <====train track===> avg_loss: 0.005605136178939273, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 72 starting ...\n",
      "epoch: 72 iterations: 0 loss :0.00390066\n",
      "epoch: 72 iterations: 100 loss :0.914538\n",
      "epoch: 72 iterations: 200 loss :0.62977\n",
      "epoch: 72 iterations: 300 loss :0.0432312\n",
      "epoch: 72 iterations: 400 loss :0.00776806\n",
      "epoch: 72 iterations: 500 loss :0.0860726\n",
      "epoch: 72 iterations: 600 loss :0.00320089\n",
      "epoch: 72 iterations: 700 loss :0.230301\n",
      "epoch: 72 iterations: 800 loss :0.00748035\n",
      "epoch: 72 iterations: 900 loss :0.000623151\n",
      "epoch: 72 <====train track===> avg_loss: 0.005053394261580222, accuracy: 80.0% \n",
      "\n",
      "epoch 73 starting ...\n",
      "epoch: 73 iterations: 0 loss :1.10732\n",
      "epoch: 73 iterations: 100 loss :3.12748\n",
      "epoch: 73 iterations: 200 loss :0.0385493\n",
      "epoch: 73 iterations: 300 loss :0.044957\n",
      "epoch: 73 iterations: 400 loss :0.0238027\n",
      "epoch: 73 iterations: 500 loss :2.85579\n",
      "epoch: 73 iterations: 600 loss :0.0445079\n",
      "epoch: 73 iterations: 700 loss :0.131023\n",
      "epoch: 73 iterations: 800 loss :0.0492058\n",
      "epoch: 73 iterations: 900 loss :0.0553709\n",
      "epoch: 73 <====train track===> avg_loss: 0.005873175575665589, accuracy: 82.85714285714286% \n",
      "\n",
      "epoch 74 starting ...\n",
      "epoch: 74 iterations: 0 loss :2.58682\n",
      "epoch: 74 iterations: 100 loss :0.28355\n",
      "epoch: 74 iterations: 200 loss :0.124373\n",
      "epoch: 74 iterations: 300 loss :0.00204521\n",
      "epoch: 74 iterations: 400 loss :0.000217176\n",
      "epoch: 74 iterations: 500 loss :0.00124913\n",
      "epoch: 74 iterations: 600 loss :0.549577\n",
      "epoch: 74 iterations: 700 loss :3.45948\n",
      "epoch: 74 iterations: 800 loss :1.4371\n",
      "epoch: 74 iterations: 900 loss :0.0835914\n",
      "epoch: 74 <====train track===> avg_loss: 0.00572247981943716, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 75 starting ...\n",
      "epoch: 75 iterations: 0 loss :3.90826\n",
      "epoch: 75 iterations: 100 loss :0.224445\n",
      "epoch: 75 iterations: 200 loss :0.74401\n",
      "epoch: 75 iterations: 300 loss :0.0807721\n",
      "epoch: 75 iterations: 400 loss :0.00862717\n",
      "epoch: 75 iterations: 500 loss :0.0111946\n",
      "epoch: 75 iterations: 600 loss :0.142917\n",
      "epoch: 75 iterations: 700 loss :0.194319\n",
      "epoch: 75 iterations: 800 loss :0.000717263\n",
      "epoch: 75 iterations: 900 loss :2.07518\n",
      "epoch: 75 <====train track===> avg_loss: 0.005356313327716903, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 76 starting ...\n",
      "epoch: 76 iterations: 0 loss :0.0800391\n",
      "epoch: 76 iterations: 100 loss :1.55105\n",
      "epoch: 76 iterations: 200 loss :0.26705\n",
      "epoch: 76 iterations: 300 loss :0.755206\n",
      "epoch: 76 iterations: 400 loss :1.17996\n",
      "epoch: 76 iterations: 500 loss :0.00103039\n",
      "epoch: 76 iterations: 600 loss :0.00061648\n",
      "epoch: 76 iterations: 700 loss :0.0234127\n",
      "epoch: 76 iterations: 800 loss :0.656435\n",
      "epoch: 76 iterations: 900 loss :0.00422075\n",
      "epoch: 76 <====train track===> avg_loss: 0.0052801099436370945, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 77 starting ...\n",
      "epoch: 77 iterations: 0 loss :0.0684997\n",
      "epoch: 77 iterations: 100 loss :0.0519097\n",
      "epoch: 77 iterations: 200 loss :1.05699\n",
      "epoch: 77 iterations: 300 loss :0.196435\n",
      "epoch: 77 iterations: 400 loss :0.00210302\n",
      "epoch: 77 iterations: 500 loss :0.279004\n",
      "epoch: 77 iterations: 600 loss :0.00935174\n",
      "epoch: 77 iterations: 700 loss :1.18456\n",
      "epoch: 77 iterations: 800 loss :0.902493\n",
      "epoch: 77 iterations: 900 loss :0.120343\n",
      "epoch: 77 <====train track===> avg_loss: 0.005577706679477268, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 78 starting ...\n",
      "epoch: 78 iterations: 0 loss :1.53411\n",
      "epoch: 78 iterations: 100 loss :0.0348203\n",
      "epoch: 78 iterations: 200 loss :0.00139318\n",
      "epoch: 78 iterations: 300 loss :0.0133145\n",
      "epoch: 78 iterations: 400 loss :0.00487686\n",
      "epoch: 78 iterations: 500 loss :0.019567\n",
      "epoch: 78 iterations: 600 loss :0.00410761\n",
      "epoch: 78 iterations: 700 loss :0.00036245\n",
      "epoch: 78 iterations: 800 loss :0.196013\n",
      "epoch: 78 iterations: 900 loss :0.414229\n",
      "epoch: 78 <====train track===> avg_loss: 0.0053317485424625236, accuracy: 88.57142857142857% \n",
      "\n",
      "epoch 79 starting ...\n",
      "epoch: 79 iterations: 0 loss :0.0166331\n",
      "epoch: 79 iterations: 100 loss :0.928649\n",
      "epoch: 79 iterations: 200 loss :1.33792\n",
      "epoch: 79 iterations: 300 loss :1.4119\n",
      "epoch: 79 iterations: 400 loss :0.0422314\n",
      "epoch: 79 iterations: 500 loss :0.0283367\n",
      "epoch: 79 iterations: 600 loss :0.20655\n",
      "epoch: 79 iterations: 700 loss :0.37935\n",
      "epoch: 79 iterations: 800 loss :0.116641\n",
      "epoch: 79 iterations: 900 loss :0.0688689\n",
      "epoch: 79 <====train track===> avg_loss: 0.005745091238028991, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 80 starting ...\n",
      "epoch: 80 iterations: 0 loss :0.0205626\n",
      "epoch: 80 iterations: 100 loss :0.0093815\n",
      "epoch: 80 iterations: 200 loss :0.027728\n",
      "epoch: 80 iterations: 300 loss :0.0675029\n",
      "epoch: 80 iterations: 400 loss :0.00637119\n",
      "epoch: 80 iterations: 500 loss :0.0346875\n",
      "epoch: 80 iterations: 600 loss :0.000494715\n",
      "epoch: 80 iterations: 700 loss :0.000254479\n",
      "epoch: 80 iterations: 800 loss :0.0202032\n",
      "epoch: 80 iterations: 900 loss :1.36628\n",
      "epoch: 80 <====train track===> avg_loss: 0.004889100993451797, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 81 starting ...\n",
      "epoch: 81 iterations: 0 loss :0.00680806\n",
      "epoch: 81 iterations: 100 loss :0.000517711\n",
      "epoch: 81 iterations: 200 loss :0.194705\n",
      "epoch: 81 iterations: 300 loss :0.172429\n",
      "epoch: 81 iterations: 400 loss :0.0508266\n",
      "epoch: 81 iterations: 500 loss :0.0178607\n",
      "epoch: 81 iterations: 600 loss :0.0526056\n",
      "epoch: 81 iterations: 700 loss :0.352306\n",
      "epoch: 81 iterations: 800 loss :0.0602995\n",
      "epoch: 81 iterations: 900 loss :0.0470472\n",
      "epoch: 81 <====train track===> avg_loss: 0.005206178855574602, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 82 starting ...\n",
      "epoch: 82 iterations: 0 loss :0.00594206\n",
      "epoch: 82 iterations: 100 loss :0.110898\n",
      "epoch: 82 iterations: 200 loss :0.0177182\n",
      "epoch: 82 iterations: 300 loss :0.761388\n",
      "epoch: 82 iterations: 400 loss :0.00632996\n",
      "epoch: 82 iterations: 500 loss :0.217006\n",
      "epoch: 82 iterations: 600 loss :0.00277801\n",
      "epoch: 82 iterations: 700 loss :0.000725364\n",
      "epoch: 82 iterations: 800 loss :0.0492203\n",
      "epoch: 82 iterations: 900 loss :0.226368\n",
      "epoch: 82 <====train track===> avg_loss: 0.005133327339538808, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 83 starting ...\n",
      "epoch: 83 iterations: 0 loss :0.738588\n",
      "epoch: 83 iterations: 100 loss :0.000313352\n",
      "epoch: 83 iterations: 200 loss :0.00971884\n",
      "epoch: 83 iterations: 300 loss :0.0102302\n",
      "epoch: 83 iterations: 400 loss :1.21538\n",
      "epoch: 83 iterations: 500 loss :0.725611\n",
      "epoch: 83 iterations: 600 loss :0.473753\n",
      "epoch: 83 iterations: 700 loss :0.042667\n",
      "epoch: 83 iterations: 800 loss :0.323517\n",
      "epoch: 83 iterations: 900 loss :0.0122429\n",
      "epoch: 83 <====train track===> avg_loss: 0.005360819726607315, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 84 starting ...\n",
      "epoch: 84 iterations: 0 loss :0.0639708\n",
      "epoch: 84 iterations: 100 loss :0.0132246\n",
      "epoch: 84 iterations: 200 loss :0.160809\n",
      "epoch: 84 iterations: 300 loss :0.280411\n",
      "epoch: 84 iterations: 400 loss :0.0492091\n",
      "epoch: 84 iterations: 500 loss :0.0593601\n",
      "epoch: 84 iterations: 600 loss :0.00627926\n",
      "epoch: 84 iterations: 700 loss :0.142289\n",
      "epoch: 84 iterations: 800 loss :0.00268219\n",
      "epoch: 84 iterations: 900 loss :0.107316\n",
      "epoch: 84 <====train track===> avg_loss: 0.0049096164799067735, accuracy: 83.80952380952381% \n",
      "\n",
      "epoch 85 starting ...\n",
      "epoch: 85 iterations: 0 loss :0.225105\n",
      "epoch: 85 iterations: 100 loss :0.0105317\n",
      "epoch: 85 iterations: 200 loss :0.0525184\n",
      "epoch: 85 iterations: 300 loss :0.0382116\n",
      "epoch: 85 iterations: 400 loss :0.00987608\n",
      "epoch: 85 iterations: 500 loss :0.848246\n",
      "epoch: 85 iterations: 600 loss :0.0347001\n",
      "epoch: 85 iterations: 700 loss :0.00273568\n",
      "epoch: 85 iterations: 800 loss :0.028481\n",
      "epoch: 85 iterations: 900 loss :1.11933\n",
      "epoch: 85 <====train track===> avg_loss: 0.005040555943382423, accuracy: 84.76190476190476% \n",
      "\n",
      "epoch 86 starting ...\n",
      "epoch: 86 iterations: 0 loss :0.770524\n",
      "epoch: 86 iterations: 100 loss :0.00370958\n",
      "epoch: 86 iterations: 200 loss :0.664463\n",
      "epoch: 86 iterations: 300 loss :0.204093\n",
      "epoch: 86 iterations: 400 loss :0.291004\n",
      "epoch: 86 iterations: 500 loss :0.0615973\n",
      "epoch: 86 iterations: 600 loss :0.00858509\n",
      "epoch: 86 iterations: 700 loss :0.349579\n",
      "epoch: 86 iterations: 800 loss :1.30833\n",
      "epoch: 86 iterations: 900 loss :0.639678\n",
      "epoch: 86 <====train track===> avg_loss: 0.004793294946988631, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 87 starting ...\n",
      "epoch: 87 iterations: 0 loss :1.18391\n",
      "epoch: 87 iterations: 100 loss :0.013421\n",
      "epoch: 87 iterations: 200 loss :1.68365\n",
      "epoch: 87 iterations: 300 loss :0.00741196\n",
      "epoch: 87 iterations: 400 loss :0.0258338\n",
      "epoch: 87 iterations: 500 loss :0.000481966\n",
      "epoch: 87 iterations: 600 loss :4.65553\n",
      "epoch: 87 iterations: 700 loss :0.00256864\n",
      "epoch: 87 iterations: 800 loss :0.0039047\n",
      "epoch: 87 iterations: 900 loss :0.515443\n",
      "epoch: 87 <====train track===> avg_loss: 0.00494354025740816, accuracy: 87.61904761904762% \n",
      "\n",
      "epoch 88 starting ...\n",
      "epoch: 88 iterations: 0 loss :3.8367\n",
      "epoch: 88 iterations: 100 loss :0.0567919\n",
      "epoch: 88 iterations: 200 loss :0.298027\n",
      "epoch: 88 iterations: 300 loss :0.047439\n",
      "epoch: 88 iterations: 400 loss :1.72716\n",
      "epoch: 88 iterations: 500 loss :0.422409\n",
      "epoch: 88 iterations: 600 loss :0.216671\n",
      "epoch: 88 iterations: 700 loss :0.00109005\n",
      "epoch: 88 iterations: 800 loss :2.49346\n",
      "epoch: 88 iterations: 900 loss :0.0283008\n",
      "epoch: 88 <====train track===> avg_loss: 0.004929137400724738, accuracy: 89.52380952380952% \n",
      "\n",
      "epoch 89 starting ...\n",
      "epoch: 89 iterations: 0 loss :0.000890812\n",
      "epoch: 89 iterations: 100 loss :0.00548228\n",
      "epoch: 89 iterations: 200 loss :0.00595818\n",
      "epoch: 89 iterations: 300 loss :0.00597939\n",
      "epoch: 89 iterations: 400 loss :0.00610724\n",
      "epoch: 89 iterations: 500 loss :0.0898437\n",
      "epoch: 89 iterations: 600 loss :0.000937142\n",
      "epoch: 89 iterations: 700 loss :0.368571\n",
      "epoch: 89 iterations: 800 loss :0.0129562\n",
      "epoch: 89 iterations: 900 loss :0.0616357\n",
      "epoch: 89 <====train track===> avg_loss: 0.004556144351152152, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 90 starting ...\n",
      "epoch: 90 iterations: 0 loss :0.0343092\n",
      "epoch: 90 iterations: 100 loss :0.197417\n",
      "epoch: 90 iterations: 200 loss :0.00685329\n",
      "epoch: 90 iterations: 300 loss :0.00313042\n",
      "epoch: 90 iterations: 400 loss :0.0593115\n",
      "epoch: 90 iterations: 500 loss :0.131463\n",
      "epoch: 90 iterations: 600 loss :0.0798947\n",
      "epoch: 90 iterations: 700 loss :0.0602321\n",
      "epoch: 90 iterations: 800 loss :0.00847245\n",
      "epoch: 90 iterations: 900 loss :2.10594\n",
      "epoch: 90 <====train track===> avg_loss: 0.00444392732481492, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 91 starting ...\n",
      "epoch: 91 iterations: 0 loss :0.901304\n",
      "epoch: 91 iterations: 100 loss :0.200745\n",
      "epoch: 91 iterations: 200 loss :1.10267\n",
      "epoch: 91 iterations: 300 loss :0.000242561\n",
      "epoch: 91 iterations: 400 loss :0.0659395\n",
      "epoch: 91 iterations: 500 loss :0.275713\n",
      "epoch: 91 iterations: 600 loss :0.860605\n",
      "epoch: 91 iterations: 700 loss :0.000415001\n",
      "epoch: 91 iterations: 800 loss :0.00267767\n",
      "epoch: 91 iterations: 900 loss :0.049884\n",
      "epoch: 91 <====train track===> avg_loss: 0.0048516488805322566, accuracy: 87.61904761904762% \n",
      "\n",
      "epoch 92 starting ...\n",
      "epoch: 92 iterations: 0 loss :0.01348\n",
      "epoch: 92 iterations: 100 loss :0.871835\n",
      "epoch: 92 iterations: 200 loss :0.488662\n",
      "epoch: 92 iterations: 300 loss :0.132324\n",
      "epoch: 92 iterations: 400 loss :0.464549\n",
      "epoch: 92 iterations: 500 loss :0.0694118\n",
      "epoch: 92 iterations: 600 loss :0.622397\n",
      "epoch: 92 iterations: 700 loss :0.0406879\n",
      "epoch: 92 iterations: 800 loss :0.0470792\n",
      "epoch: 92 iterations: 900 loss :0.0689412\n",
      "epoch: 92 <====train track===> avg_loss: 0.004930007161211195, accuracy: 89.52380952380952% \n",
      "\n",
      "epoch 93 starting ...\n",
      "epoch: 93 iterations: 0 loss :0.503678\n",
      "epoch: 93 iterations: 100 loss :0.519443\n",
      "epoch: 93 iterations: 200 loss :0.0586824\n",
      "epoch: 93 iterations: 300 loss :0.00075836\n",
      "epoch: 93 iterations: 400 loss :0.033029\n",
      "epoch: 93 iterations: 500 loss :2.1081\n",
      "epoch: 93 iterations: 600 loss :0.122177\n",
      "epoch: 93 iterations: 700 loss :1.53549\n",
      "epoch: 93 iterations: 800 loss :0.00299731\n",
      "epoch: 93 iterations: 900 loss :0.00163826\n",
      "epoch: 93 <====train track===> avg_loss: 0.004712560840890694, accuracy: 87.61904761904762% \n",
      "\n",
      "epoch 94 starting ...\n",
      "epoch: 94 iterations: 0 loss :0.497437\n",
      "epoch: 94 iterations: 100 loss :0.00166837\n",
      "epoch: 94 iterations: 200 loss :0.40092\n",
      "epoch: 94 iterations: 300 loss :1.21943\n",
      "epoch: 94 iterations: 400 loss :0.00186305\n",
      "epoch: 94 iterations: 500 loss :0.592403\n",
      "epoch: 94 iterations: 600 loss :0.00118067\n",
      "epoch: 94 iterations: 700 loss :0.0189216\n",
      "epoch: 94 iterations: 800 loss :0.0308767\n",
      "epoch: 94 iterations: 900 loss :0.0755135\n",
      "epoch: 94 <====train track===> avg_loss: 0.0048472932563232195, accuracy: 85.71428571428571% \n",
      "\n",
      "epoch 95 starting ...\n",
      "epoch: 95 iterations: 0 loss :0.0344335\n",
      "epoch: 95 iterations: 100 loss :0.289792\n",
      "epoch: 95 iterations: 200 loss :0.00351965\n",
      "epoch: 95 iterations: 300 loss :0.00182533\n",
      "epoch: 95 iterations: 400 loss :0.00451925\n",
      "epoch: 95 iterations: 500 loss :0.288488\n",
      "epoch: 95 iterations: 600 loss :0.512337\n",
      "epoch: 95 iterations: 700 loss :0.662515\n",
      "epoch: 95 iterations: 800 loss :0.00881743\n",
      "epoch: 95 iterations: 900 loss :2.70022\n",
      "epoch: 95 <====train track===> avg_loss: 0.004424641089839076, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 96 starting ...\n",
      "epoch: 96 iterations: 0 loss :0.0267546\n",
      "epoch: 96 iterations: 100 loss :0.14796\n",
      "epoch: 96 iterations: 200 loss :0.627376\n",
      "epoch: 96 iterations: 300 loss :0.0756932\n",
      "epoch: 96 iterations: 400 loss :0.0815557\n",
      "epoch: 96 iterations: 500 loss :0.315962\n",
      "epoch: 96 iterations: 600 loss :0.00191089\n",
      "epoch: 96 iterations: 700 loss :0.0100589\n",
      "epoch: 96 iterations: 800 loss :0.0615933\n",
      "epoch: 96 iterations: 900 loss :0.0264615\n",
      "epoch: 96 <====train track===> avg_loss: 0.004455807506331748, accuracy: 90.47619047619048% \n",
      "\n",
      "epoch 97 starting ...\n",
      "epoch: 97 iterations: 0 loss :0.420801\n",
      "epoch: 97 iterations: 100 loss :0.116484\n",
      "epoch: 97 iterations: 200 loss :0.128792\n",
      "epoch: 97 iterations: 300 loss :0.00147794\n",
      "epoch: 97 iterations: 400 loss :0.000481966\n",
      "epoch: 97 iterations: 500 loss :0.00698186\n",
      "epoch: 97 iterations: 600 loss :0.0059993\n",
      "epoch: 97 iterations: 700 loss :0.0041602\n",
      "epoch: 97 iterations: 800 loss :0.0300336\n",
      "epoch: 97 iterations: 900 loss :5.6769\n",
      "epoch: 97 <====train track===> avg_loss: 0.004968105964495687, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 98 starting ...\n",
      "epoch: 98 iterations: 0 loss :0.0297648\n",
      "epoch: 98 iterations: 100 loss :0.0249792\n",
      "epoch: 98 iterations: 200 loss :0.339652\n",
      "epoch: 98 iterations: 300 loss :0.147202\n",
      "epoch: 98 iterations: 400 loss :0.0772364\n",
      "epoch: 98 iterations: 500 loss :0.156314\n",
      "epoch: 98 iterations: 600 loss :1.06532\n",
      "epoch: 98 iterations: 700 loss :0.000201086\n",
      "epoch: 98 iterations: 800 loss :1.22594\n",
      "epoch: 98 iterations: 900 loss :0.649575\n",
      "epoch: 98 <====train track===> avg_loss: 0.004979358689224418, accuracy: 86.66666666666667% \n",
      "\n",
      "epoch 99 starting ...\n",
      "epoch: 99 iterations: 0 loss :0.000298932\n",
      "epoch: 99 iterations: 100 loss :0.0782422\n",
      "epoch: 99 iterations: 200 loss :0.013623\n",
      "epoch: 99 iterations: 300 loss :0.0419571\n",
      "epoch: 99 iterations: 400 loss :1.31353\n",
      "epoch: 99 iterations: 500 loss :0.0397834\n",
      "epoch: 99 iterations: 600 loss :0.195183\n",
      "epoch: 99 iterations: 700 loss :0.413545\n",
      "epoch: 99 iterations: 800 loss :0.055793\n",
      "epoch: 99 iterations: 900 loss :3.52603\n",
      "epoch: 99 <====train track===> avg_loss: 0.005055050692417455, accuracy: 83.80952380952381% \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'avg loss')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f734f407b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f734f407470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#training function\n",
    "def train(model, num_epoch, num_iter, rec_interval, disp_interval):\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 3e-5)\n",
    "    loss_values = []\n",
    "    avg_loss_values = []\n",
    "    rec_step = 0\n",
    "    print('Starting the training ...')\n",
    "    for eph in range(num_epoch):\n",
    "        print('epoch {} starting ...'.format(eph))\n",
    "        avg_loss = 0\n",
    "        n_samples = 0\n",
    "        for i in range(num_iter):\n",
    "            model.hidden = (model.hidden[0].detach(), model.hidden[1].detach())\n",
    "            model.zero_grad()\n",
    "            X,Y = next(ACTd)\n",
    "            n_samples += len(X)\n",
    "            X = autograd.Variable(torch.from_numpy(X).float().cuda())\n",
    "            X = X.view(len(X), 1, -1)\n",
    "            Y = autograd.Variable(torch.LongTensor(np.array([Y])).cuda())\n",
    "\n",
    "            y_hat = model(X)\n",
    "#             print(eph, i, y_hat)\n",
    "            loss = F.cross_entropy(y_hat, Y)\n",
    "            avg_loss += loss.data[0]\n",
    "            \n",
    "            if i % disp_interval == 0:\n",
    "                print('epoch: %d iterations: %d loss :%g' % (eph, i, loss.data[0]))\n",
    "            if rec_step%rec_interval==0:\n",
    "                loss_values.append(loss.data[0])\n",
    "            \n",
    "            loss.backward()     \n",
    "            optimizer.step()\n",
    "            rec_step += 1\n",
    "            \n",
    "        avg_loss /= n_samples\n",
    "        avg_loss_values.append(avg_loss)\n",
    "        #evaluating model accuracy\n",
    "        acc = evaluate_accuracy(model, test_split)\n",
    "        print('epoch: {} <====train track===> avg_loss: {}, accuracy: {}% \\n'.format(eph, avg_loss, acc))\n",
    "    return loss_values, avg_loss_values\n",
    "\n",
    "\n",
    "loss_vals, avg_loss_vals = train(model0, 100, 1000, 2, 100)\n",
    "plt.figure()\n",
    "plt.plot(loss_vals)\n",
    "plt.figure()\n",
    "plt.plot(avg_loss_vals)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('avg loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
