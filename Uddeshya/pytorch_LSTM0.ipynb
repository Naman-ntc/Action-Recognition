{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f40e41638d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random, numpy as np\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       "[torch.DoubleTensor of size 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.DoubleTensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-843282b1906a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#emb experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my name is uddeshya'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/DeepCV3.5/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DeepCV3.5/lib/python3.5/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DeepCV3.5/lib/python3.5/site-packages/torch/nn/_functions/thnn/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, indices, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_input_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Embedding doesn't \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;34m\"compute the gradient w.r.t. the indices\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "#emb experiment\n",
    "emb = nn.Embedding(100, 100)\n",
    "emb('my name is uddeshya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some random semantics test data\n",
    "num_data = 100\n",
    "dataX = [autograd.Variable(torch.randn((1, 75))) for _ in range(num_data)]\n",
    "dataY = [autograd.Variable(torch.DoubleTensor([random.randint(1,11)])) for _ in range(num_data)]\n",
    "# print(dataY)\n",
    "# print(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.4422 -0.0836 -0.3623  0.0170  0.0153  0.3019  0.9204  0.0295  0.2799\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.0171  0.3507 -0.2216 -0.4838 -0.0374  0.1170  0.3147  0.2965 -0.6561\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.0643  0.7508 -0.4947 -0.1613 -0.0253  0.2302  0.1747  0.4461  0.1309\n",
      "\n",
      "Columns 27 to 35 \n",
      "   0.0815 -0.1382 -0.5025  0.0445 -0.0946 -0.1964  0.1771 -0.2011  0.1233\n",
      "\n",
      "Columns 36 to 44 \n",
      "   0.1657 -0.1282 -0.0748 -0.3011  0.1242 -0.0762  0.3110 -0.0740  0.3555\n",
      "\n",
      "Columns 45 to 53 \n",
      "  -0.4821 -0.4017  0.0443  0.3157  0.5549 -0.2614  0.0037  0.2796  0.0886\n",
      "\n",
      "Columns 54 to 62 \n",
      "  -0.3237 -0.0264 -0.0998 -0.1732 -0.2605 -0.0618  0.2819 -0.0445  0.1246\n",
      "\n",
      "Columns 63 to 71 \n",
      "   0.1765  0.1309  0.1503  0.1623 -0.2204 -0.1555 -0.4385  0.1237  0.0111\n",
      "\n",
      "Columns 72 to 74 \n",
      "  -0.1999  0.1897 -0.5549\n",
      "\n",
      "(1 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0849  0.0138 -0.5418 -0.1290 -0.1049  0.4493  0.2657  0.0803  0.2891\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.1018  0.0534  0.0810 -0.4659 -0.0364 -0.1796  0.2171 -0.0917 -0.4453\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.0649  0.3339 -0.3333  0.0057 -0.0109  0.0807 -0.1330  0.2799  0.0739\n",
      "\n",
      "Columns 27 to 35 \n",
      "  -0.1194  0.0277 -0.2469  0.0063  0.0031 -0.0458 -0.0085 -0.1754  0.2318\n",
      "\n",
      "Columns 36 to 44 \n",
      "   0.0089 -0.1572 -0.1643 -0.0931  0.1090 -0.2118  0.4199 -0.0890  0.3092\n",
      "\n",
      "Columns 45 to 53 \n",
      "  -0.3463  0.1636  0.0283  0.0550 -0.0143 -0.1429  0.0767  0.1438  0.1723\n",
      "\n",
      "Columns 54 to 62 \n",
      "  -0.4192 -0.0779  0.1689 -0.0874 -0.0352 -0.1795  0.3647  0.0790  0.0050\n",
      "\n",
      "Columns 63 to 71 \n",
      "   0.0357  0.0007  0.1362  0.2380 -0.1972 -0.1821  0.0236  0.0936 -0.0825\n",
      "\n",
      "Columns 72 to 74 \n",
      "  -0.2957  0.1815 -0.2011\n",
      "\n",
      "(2 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.2676 -0.0442 -0.2157 -0.0555 -0.2461  0.0324  0.3567  0.1445  0.2037\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.1857 -0.0242  0.0885 -0.0734 -0.0633 -0.1583  0.0221 -0.2970 -0.1068\n",
      "\n",
      "Columns 18 to 26 \n",
      "   0.0415  0.2139 -0.1924 -0.0137 -0.1007  0.1908 -0.1300  0.1560  0.0773\n",
      "\n",
      "Columns 27 to 35 \n",
      "  -0.0454  0.0611 -0.2237  0.1084  0.1138  0.0700  0.1559 -0.0272  0.1873\n",
      "\n",
      "Columns 36 to 44 \n",
      "  -0.0460 -0.1725 -0.1009 -0.1217 -0.1099 -0.2458 -0.0274  0.1185  0.0142\n",
      "\n",
      "Columns 45 to 53 \n",
      "  -0.1588 -0.0872  0.0774  0.1510  0.1162 -0.0890 -0.0548  0.2308  0.1254\n",
      "\n",
      "Columns 54 to 62 \n",
      "  -0.1152 -0.1520 -0.0098 -0.2378  0.0407 -0.1433 -0.0637 -0.0531  0.0855\n",
      "\n",
      "Columns 63 to 71 \n",
      "   0.1155  0.1575  0.0342  0.0410 -0.3492 -0.1912  0.0973  0.0142 -0.1520\n",
      "\n",
      "Columns 72 to 74 \n",
      "  -0.2367  0.2363 -0.3812\n",
      "\n",
      "(3 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.1696  0.0260 -0.0651  0.0307 -0.0114  0.0265  0.0123  0.0791  0.0365\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.0323 -0.0099 -0.2217 -0.1141 -0.2222  0.1425 -0.1779 -0.0616 -0.1082\n",
      "\n",
      "Columns 18 to 26 \n",
      "   0.0053  0.2117 -0.0457 -0.0785  0.0470  0.0992 -0.1573  0.1925  0.0457\n",
      "\n",
      "Columns 27 to 35 \n",
      "   0.0090 -0.0925 -0.1013  0.1281  0.0341 -0.0071 -0.0590 -0.1573  0.1524\n",
      "\n",
      "Columns 36 to 44 \n",
      "   0.2119 -0.1164  0.1353  0.0593 -0.1045 -0.1127 -0.0048 -0.0706  0.1030\n",
      "\n",
      "Columns 45 to 53 \n",
      "  -0.0802 -0.0502  0.2295 -0.0388  0.0173 -0.0443  0.0649  0.2383  0.1359\n",
      "\n",
      "Columns 54 to 62 \n",
      "  -0.1075 -0.3097 -0.2400 -0.0761 -0.0446  0.0335 -0.0983  0.0235 -0.0688\n",
      "\n",
      "Columns 63 to 71 \n",
      "   0.2661  0.0955 -0.1314 -0.0377  0.0638  0.2199 -0.1354 -0.0661 -0.0004\n",
      "\n",
      "Columns 72 to 74 \n",
      "   0.0363  0.3247 -0.2254\n",
      "\n",
      "(4 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.1560  0.3633 -0.1235  0.0123 -0.0412  0.1281 -0.0647  0.3896 -0.0619\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.1731 -0.0331  0.1780 -0.1031 -0.0596 -0.0415 -0.3337 -0.1254 -0.2543\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.0101  0.0226 -0.0682  0.0545  0.0668  0.1224  0.1083  0.3316 -0.1361\n",
      "\n",
      "Columns 27 to 35 \n",
      "   0.0366 -0.2847 -0.3227  0.0103 -0.0703 -0.1798 -0.0738  0.0633  0.0546\n",
      "\n",
      "Columns 36 to 44 \n",
      "   0.1607  0.0287  0.2797 -0.0879 -0.2441 -0.3059 -0.0049 -0.1914  0.0664\n",
      "\n",
      "Columns 45 to 53 \n",
      "   0.0056 -0.2058  0.1420 -0.1859 -0.0292  0.0122  0.2136  0.1092  0.0976\n",
      "\n",
      "Columns 54 to 62 \n",
      "  -0.3004 -0.0029 -0.2125  0.0563 -0.0215 -0.2048  0.0779  0.1686 -0.0440\n",
      "\n",
      "Columns 63 to 71 \n",
      "   0.0434  0.0747 -0.0749 -0.0191  0.0848  0.0406  0.0020  0.0023  0.2104\n",
      "\n",
      "Columns 72 to 74 \n",
      "   0.1354  0.5067 -0.0875\n",
      "\n",
      "(5 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0045  0.2419  0.2126  0.0621 -0.1890  0.1440  0.0478  0.1998  0.1186\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.1491 -0.0379  0.0561 -0.1023 -0.0995 -0.0901 -0.3215 -0.2270 -0.0320\n",
      "\n",
      "Columns 18 to 26 \n",
      "   0.0011  0.0823 -0.0076  0.0985  0.0689  0.0178  0.2359 -0.0227 -0.1252\n",
      "\n",
      "Columns 27 to 35 \n",
      "  -0.1506  0.1599  0.0184 -0.0685 -0.1262 -0.0068 -0.0610 -0.0958  0.0615\n",
      "\n",
      "Columns 36 to 44 \n",
      "   0.0519  0.3472 -0.0380 -0.0506  0.0361 -0.1004 -0.2090 -0.0328  0.1656\n",
      "\n",
      "Columns 45 to 53 \n",
      "   0.0921 -0.0199 -0.1634 -0.0903  0.0210  0.0363  0.1832  0.2070  0.0572\n",
      "\n",
      "Columns 54 to 62 \n",
      "   0.0322  0.0877  0.1169  0.2787 -0.0937 -0.1087  0.1435  0.1338  0.0330\n",
      "\n",
      "Columns 63 to 71 \n",
      "   0.2419  0.0744 -0.0317  0.0364 -0.0422 -0.1261  0.0669 -0.0574 -0.0459\n",
      "\n",
      "Columns 72 to 74 \n",
      "   0.1845  0.1375 -0.0701\n",
      "\n",
      "(6 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0624  0.2514  0.1904 -0.1376 -0.1139  0.0738 -0.1480 -0.0059  0.1317\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.2033  0.2292  0.0980 -0.1154 -0.0258  0.0591 -0.1664 -0.1273 -0.0215\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.1287 -0.0676  0.1328  0.0164 -0.0562 -0.0135 -0.1002  0.1382  0.0864\n",
      "\n",
      "Columns 27 to 35 \n",
      "  -0.3092  0.1275 -0.1561 -0.0729  0.0969 -0.2198 -0.0255 -0.0003  0.1463\n",
      "\n",
      "Columns 36 to 44 \n",
      "   0.0355  0.0769 -0.1831 -0.1741  0.1010  0.0943 -0.1926 -0.0258 -0.0861\n",
      "\n",
      "Columns 45 to 53 \n",
      "   0.0614  0.0201 -0.2857  0.0074 -0.0810 -0.1250 -0.1317  0.1659 -0.0317\n",
      "\n",
      "Columns 54 to 62 \n",
      "  -0.0475  0.0329 -0.0728  0.0803  0.0823 -0.1092  0.0271  0.0749  0.1135\n",
      "\n",
      "Columns 63 to 71 \n",
      "   0.0494  0.0811  0.1055  0.1427  0.0089  0.0309 -0.1167 -0.0779  0.0376\n",
      "\n",
      "Columns 72 to 74 \n",
      "  -0.0371  0.1621  0.0442\n",
      "\n",
      "(7 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0659  0.1941  0.0066 -0.1847  0.0284 -0.0390 -0.0314  0.0891  0.0351\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.2325  0.0945  0.0623 -0.0914  0.0095  0.0321 -0.0941 -0.0947 -0.1936\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.0273  0.0331 -0.0325  0.1123  0.0278  0.0768 -0.1224  0.1595  0.0268\n",
      "\n",
      "Columns 27 to 35 \n",
      "  -0.1766  0.0390 -0.0586 -0.0722 -0.0378 -0.1392  0.0109  0.0658 -0.1053\n",
      "\n",
      "Columns 36 to 44 \n",
      "   0.0426  0.0043 -0.0729 -0.1572  0.0235  0.1610 -0.0731 -0.0504  0.1112\n",
      "\n",
      "Columns 45 to 53 \n",
      "   0.1230  0.1442 -0.1935 -0.0053  0.0053  0.0722 -0.1048  0.2361  0.2992\n",
      "\n",
      "Columns 54 to 62 \n",
      "  -0.1593 -0.0009 -0.1655 -0.0038  0.1057 -0.1995  0.0223  0.0167  0.1317\n",
      "\n",
      "Columns 63 to 71 \n",
      "   0.0031  0.1042 -0.1369 -0.1202 -0.0375 -0.2112 -0.0971 -0.1017  0.0055\n",
      "\n",
      "Columns 72 to 74 \n",
      "   0.0267  0.0230 -0.0686\n",
      "\n",
      "(8 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.3092  0.1738  0.0687  0.0558  0.0080 -0.1590 -0.2914  0.1692  0.1305\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.3883  0.0514  0.1454 -0.3922 -0.0147  0.0987 -0.3296 -0.0747 -0.0091\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.1829  0.1080  0.0866  0.1172  0.1487  0.1736 -0.0872  0.0296  0.0440\n",
      "\n",
      "Columns 27 to 35 \n",
      "  -0.3750  0.1027 -0.3959  0.1785  0.2099 -0.1050  0.0751  0.1344  0.2222\n",
      "\n",
      "Columns 36 to 44 \n",
      "  -0.1071 -0.1314  0.0818 -0.3621 -0.0418 -0.0134 -0.0511 -0.2588 -0.0688\n",
      "\n",
      "Columns 45 to 53 \n",
      "  -0.0390 -0.1435  0.0491  0.1022 -0.0414 -0.0411 -0.2472  0.0435  0.2175\n",
      "\n",
      "Columns 54 to 62 \n",
      "  -0.1533 -0.1138 -0.1712 -0.0296  0.0229 -0.1023 -0.1008  0.1249  0.0786\n",
      "\n",
      "Columns 63 to 71 \n",
      "   0.1648  0.2828 -0.2036  0.0113 -0.1364 -0.0864 -0.3731 -0.1289  0.0914\n",
      "\n",
      "Columns 72 to 74 \n",
      "  -0.0698 -0.1141  0.0152\n",
      "\n",
      "(9 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.1273  0.0049 -0.0746 -0.0149 -0.0980 -0.0596  0.0416 -0.0351  0.1384\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.2281 -0.0225  0.0486 -0.1031  0.0471 -0.0149 -0.0737 -0.1343 -0.1042\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.0299 -0.0725  0.0202  0.1934 -0.0529  0.2842 -0.0436  0.2063  0.1844\n",
      "\n",
      "Columns 27 to 35 \n",
      "  -0.1814  0.0916 -0.0708  0.0535  0.2033 -0.1053 -0.1535  0.0553  0.1433\n",
      "\n",
      "Columns 36 to 44 \n",
      "  -0.0128 -0.0894 -0.0493 -0.0850 -0.0623  0.0817 -0.0132 -0.2885 -0.0877\n",
      "\n",
      "Columns 45 to 53 \n",
      "   0.0919  0.0039 -0.0597  0.0912 -0.0879 -0.1140 -0.2233  0.0628  0.1418\n",
      "\n",
      "Columns 54 to 62 \n",
      "   0.0846  0.0225  0.0869 -0.0254  0.1358 -0.1428 -0.1323 -0.0887 -0.0026\n",
      "\n",
      "Columns 63 to 71 \n",
      "   0.1622  0.1692  0.0289  0.0054 -0.1563  0.0752 -0.1881 -0.0871  0.1966\n",
      "\n",
      "Columns 72 to 74 \n",
      "  -0.1943 -0.0044  0.1728\n",
      "[torch.FloatTensor of size 10x1x75]\n",
      "\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.1273  0.0049 -0.0746 -0.0149 -0.0980 -0.0596  0.0416 -0.0351  0.1384\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.2281 -0.0225  0.0486 -0.1031  0.0471 -0.0149 -0.0737 -0.1343 -0.1042\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.0299 -0.0725  0.0202  0.1934 -0.0529  0.2842 -0.0436  0.2063  0.1844\n",
      "\n",
      "Columns 27 to 35 \n",
      "  -0.1814  0.0916 -0.0708  0.0535  0.2033 -0.1053 -0.1535  0.0553  0.1433\n",
      "\n",
      "Columns 36 to 44 \n",
      "  -0.0128 -0.0894 -0.0493 -0.0850 -0.0623  0.0817 -0.0132 -0.2885 -0.0877\n",
      "\n",
      "Columns 45 to 53 \n",
      "   0.0919  0.0039 -0.0597  0.0912 -0.0879 -0.1140 -0.2233  0.0628  0.1418\n",
      "\n",
      "Columns 54 to 62 \n",
      "   0.0846  0.0225  0.0869 -0.0254  0.1358 -0.1428 -0.1323 -0.0887 -0.0026\n",
      "\n",
      "Columns 63 to 71 \n",
      "   0.1622  0.1692  0.0289  0.0054 -0.1563  0.0752 -0.1881 -0.0871  0.1966\n",
      "\n",
      "Columns 72 to 74 \n",
      "  -0.1943 -0.0044  0.1728\n",
      "[torch.FloatTensor of size 1x1x75]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.3668  0.0129 -0.1360 -0.0409 -0.2303 -0.1231  0.0632 -0.0576  0.2970\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.3829 -0.0671  0.0750 -0.2187  0.1650 -0.0418 -0.1239 -0.2532 -0.3447\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.0685 -0.1596  0.0344  0.5255 -0.1555  0.3571 -0.0986  0.3654  0.3215\n",
      "\n",
      "Columns 27 to 35 \n",
      "  -0.2539  0.5284 -0.4383  0.1178  0.3971 -0.4251 -0.2735  0.2383  0.3868\n",
      "\n",
      "Columns 36 to 44 \n",
      "  -0.0244 -0.2179 -0.0852 -0.1228 -0.1173  0.1278 -0.0282 -0.5530 -0.2474\n",
      "\n",
      "Columns 45 to 53 \n",
      "   0.2162  0.0174 -0.0857  0.1721 -0.1294 -0.2419 -0.4582  0.1410  0.2200\n",
      "\n",
      "Columns 54 to 62 \n",
      "   0.1410  0.0719  0.2145 -0.0543  0.3066 -0.2102 -0.1869 -0.1313 -0.0036\n",
      "\n",
      "Columns 63 to 71 \n",
      "   0.3034  0.4222  0.0454  0.0104 -0.2766  0.1230 -0.6501 -0.1920  0.4278\n",
      "\n",
      "Columns 72 to 74 \n",
      "  -0.3244 -0.0072  0.3959\n",
      "[torch.FloatTensor of size 1x1x75]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "inp_dim=75\n",
    "hidden_dim=75\n",
    "lstm = nn.LSTM(inp_dim, hidden_dim) # Input dim is 3, output dim is 3\n",
    "inputs = [autograd.Variable(torch.randn((1, inp_dim)))\n",
    "          for _ in range(10)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, hidden_dim)),\n",
    "          autograd.Variable(torch.randn((1, 1, hidden_dim))))\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, hidden_dim)),\n",
    "          autograd.Variable(torch.randn((1, 1, hidden_dim))))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action LSTM\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, joints_dim, hidden_dim, label_size, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm = nn.LSTM(joints_dim, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, seq_of_vec):\n",
    "#         embeds = self.word_embeddings(sentence)\n",
    "#         x = embeds.view(len(sentence), self.batch_size , -1)\n",
    "        x = torch.cat(seq_of_vec).view(len(seq_of_vec), self.batch_size, -1)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y  = self.hidden2label(lstm_out[-1])\n",
    "        log_probs = F.log_softmax(y)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = LSTMClassifier(75, 75, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.cat(dataX)\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/udion/anaconda3/envs/DeepCV3.5/lib/python3.5/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-2.3174 -2.3126 -2.3362 -2.2584 -2.3638 -2.2537 -2.2875 -2.4089 -2.2500 -2.2505\n",
       "[torch.FloatTensor of size 1x10]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0.forward(dataX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
